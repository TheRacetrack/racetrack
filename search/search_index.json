{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Racetrack \u00b6 Racetrack is a framework for deploying, managing, and serving application workloads. Features \u00b6 Racetrack uses the notion of conventions to allow domain experts to produce operationally sound and effective workloads without understanding the underlying infrastructure. It makes extensive use of plugins. Thus, the type of code which it can accept is broad and covers languages such as Python 3 or Go, and actual applications such as Sphinx, Drupal or even Quake 3. These \"jobs\", in Racetrack lingo, can be deployed to different \"infrastructure targets\". We currently support Kubernetes and Docker. This is also pluggable and can be extended. Code deployed via Racetrack gets free Swagger pages and Prometheus metrics endpoints. What distinguishes Racetrack is: You only supply your function's logic . No need to write repetitive API code, setting up webservers, creating dockerfiles, kubernetes YAMLs, and so on. Racetrack takes care of that for you. Language agnostic . Deploy code written in Python 3, Go, Rust, or anything else encapsulated in a Dockerfile. Infrastructure independent . Deploy to either a Kubernetes cluster or a Docker environment in a single transparent step. Example usage \u00b6 Given a Python file like this: class Meow : def perform ( self , a , b ): \"\"\"Add numbers\"\"\" return a + b And a YAML file like this: name : adder owner_email : meowmeow@example.com jobtype : python3:latest git : remote : https://www.github.com/path/to-repo.git python : entrypoint_path : 'adder.py' entrypoint_class : 'Meow' You can racetrack deploy And the result is a micro-service with an API: which can be monitored on a dashboard: or called with a curl command: Getting started \u00b6 Depending on your role, you may be insterested in different documents: Racetrack User: Quickstart User Manual User Guide - Deploying a Job Available plugins Installing plugins Job Manifest File Schema Glossary Racetrack Admin: Local Kubernetes Setup Installation to standalone host Installation to Kubernetes Racetrack Developer: Developing plugins Job type plugins Developer manual See the Documentation Pages for more details. Currently supported \u00b6 What can we deploy? \u00b6 Python 3 Go Rust Any language wrapped in a Dockerfile HUGO framework Drupal Sphinx Quake III Where can we deploy to? \u00b6 local Kubernetes local Docker remote Docker Daemon remote Kubernetes About \u00b6 This software is copyright Erhvervsstyrelsen. It is published under the Apache 2.0 License . See Copyright Notices . This project is a collaboration between The Danish Business Authority and deepsense.ai .","title":"Introduction"},{"location":"#racetrack","text":"Racetrack is a framework for deploying, managing, and serving application workloads.","title":"Racetrack"},{"location":"#features","text":"Racetrack uses the notion of conventions to allow domain experts to produce operationally sound and effective workloads without understanding the underlying infrastructure. It makes extensive use of plugins. Thus, the type of code which it can accept is broad and covers languages such as Python 3 or Go, and actual applications such as Sphinx, Drupal or even Quake 3. These \"jobs\", in Racetrack lingo, can be deployed to different \"infrastructure targets\". We currently support Kubernetes and Docker. This is also pluggable and can be extended. Code deployed via Racetrack gets free Swagger pages and Prometheus metrics endpoints. What distinguishes Racetrack is: You only supply your function's logic . No need to write repetitive API code, setting up webservers, creating dockerfiles, kubernetes YAMLs, and so on. Racetrack takes care of that for you. Language agnostic . Deploy code written in Python 3, Go, Rust, or anything else encapsulated in a Dockerfile. Infrastructure independent . Deploy to either a Kubernetes cluster or a Docker environment in a single transparent step.","title":"Features"},{"location":"#example-usage","text":"Given a Python file like this: class Meow : def perform ( self , a , b ): \"\"\"Add numbers\"\"\" return a + b And a YAML file like this: name : adder owner_email : meowmeow@example.com jobtype : python3:latest git : remote : https://www.github.com/path/to-repo.git python : entrypoint_path : 'adder.py' entrypoint_class : 'Meow' You can racetrack deploy And the result is a micro-service with an API: which can be monitored on a dashboard: or called with a curl command:","title":"Example usage"},{"location":"#getting-started","text":"Depending on your role, you may be insterested in different documents: Racetrack User: Quickstart User Manual User Guide - Deploying a Job Available plugins Installing plugins Job Manifest File Schema Glossary Racetrack Admin: Local Kubernetes Setup Installation to standalone host Installation to Kubernetes Racetrack Developer: Developing plugins Job type plugins Developer manual See the Documentation Pages for more details.","title":"Getting started"},{"location":"#currently-supported","text":"","title":"Currently supported"},{"location":"#what-can-we-deploy","text":"Python 3 Go Rust Any language wrapped in a Dockerfile HUGO framework Drupal Sphinx Quake III","title":"What can we deploy?"},{"location":"#where-can-we-deploy-to","text":"local Kubernetes local Docker remote Docker Daemon remote Kubernetes","title":"Where can we deploy to?"},{"location":"#about","text":"This software is copyright Erhvervsstyrelsen. It is published under the Apache 2.0 License . See Copyright Notices . This project is a collaboration between The Danish Business Authority and deepsense.ai .","title":"About"},{"location":"docs/CHANGELOG/","text":"Changelog \u00b6 All user-facing , notable changes will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 [2.36.0] - 2025-09-26 \u00b6 Added \u00b6 Websocket traffic is supported for Racetrack jobs. ( #592 ) [2.35.1] - 2025-03-20 \u00b6 Added \u00b6 Names of the foreign objects in the Records table are displayed next to their IDs. ( #570 ) [2.35.0] - 2025-02-05 \u00b6 Added \u00b6 More types of Pub errors are presented on the Grafana Dashboard, including panic errors. ( #546 ) A new tab in the Dashboard called \"Records\" lets you manage all the data models stored in Racetrack's database. It's designed to replace the Django Admin panel and incorporate its features into Racetrack's built-inadministration interface. This way, you can manage database entities in one central place without needing to log in separately. ( #543 ) Changed \u00b6 Upgraded third-party dependencies. Use svc.cluster.local suffix for Kubernetes DNS names. Take into account negation patterns in .gitignore ( #556 ) [2.34.2] - 2024-11-29 \u00b6 Fixed \u00b6 Fixed nil pointer dereference in case of caller data missing in the cached Pub response. ( #544 ) [2.34.1] - 2024-11-12 \u00b6 Fixed \u00b6 Fixed infinite redirects to Sign in page on Dashboard. ( #541 ) [2.34.0] - 2024-11-12 \u00b6 Added \u00b6 Internal calls of Pub service are cached for a short configurable time of 1-10 minutes. This improves performance and robustness in case of issues with Lifecycle, network or a database. ( #213 ) Changed \u00b6 Upgraded third-party dependencies. Enhanced the proxying of requests from Dashboard to Lifecycle in order to display detailed errors on the Dashboard ( #540 ) Fixed \u00b6 Fixed Duplicate Prepared Statement in psycopg connector ( #536 ) [2.33.0] - 2024-10-01 \u00b6 Changed \u00b6 Most database operations are now handled by the new database access layer, making direct SQL queries. It has a configurable connection pool that reduces the number of open connections. ( #480 ) Maintenance mode now also prohibits making calls to the Jobs. If turned on, it responds with \"503 Service Unavailable\". ( #517 ) Job's secrets are mounted when building the image. This hides the secret environment variables, so they can't be accessed after the build. ( #474 ) Removed \u00b6 Support for OpenTelemetry exporters has been dropped. Fixed \u00b6 Fixed deployment error about missing warnings. ( #523 ) Added informative error about crash during Job initialization. ( #469 ) Resolved compatibility issues with the Racetrack client for Python 3.8 related to unsupported typing operands. ( #527 ) [2.32.1] - 2024-08-26 \u00b6 Added \u00b6 sslmode and sslrootcert is supported in database connector for PostgreSQL ( #509 ) Deprecated fields in the manifest trigger a warning. ( #482 ) Changed \u00b6 \"404 Not Found\" responses are treated as errors in Async Job calls. ( #508 ) Attempting to install multiple versions of the same infrastructure plugin causes an error. ( #489 ) [2.32.0] - 2024-08-19 \u00b6 Added \u00b6 A new tab \"External Consumers\" has been added to the Dashboard. It allows to browse, read auth token and create a new ESC. The tab is only available for administrator users. ( #484 ) \"External Consumers\" can have multiple auth tokens to facilitate the process of rotating tokens. Tokens can be created and revoked from the Dashboard. Dashboard keeps track of the date when each token was in-use last time. ( #487 ) Changed \u00b6 Reconciliation loop is now turned off by default. It can be enabled in configuration, if needed. Instead, reconciliation can be triggered manually through the Dashboard, at Administration tab. ( #497 ) Jobs are no longer deleted on unsuccessful, retried deployment. ( #491 ) Pub's metrics has been revised to detect overloaded jobs easier. ( #493 ) Fixed \u00b6 Download button works on Kubernetes clusters. ( #490 ) [2.31.0] - 2024-07-09 \u00b6 Added \u00b6 Lifecycle now provides more metrics to monitor its health. In particular, it tracks the number of established database connections and the duration of fetching Job data from the database. To improve logging, Lifecycle now displays more details about cancelled requests. ( #472 ) Support deployment warnings, such as #452 . Internal racetrack components can now send \"log\" warnings for a deployment. ( #473 ) Changed \u00b6 The Pub-Lifecycle client now has a timeout of 30 seconds. The HTTP thread pool of Lifecycle has been increased to 60 threads. This setting is configurable through the config file. Fixed \u00b6 Fix portfolio UI sorting bug. ( #479 ) [2.30.0] - 2024-06-10 \u00b6 Added \u00b6 Submitted manifest YAML is compared with the one in the job's repository. In case of incosistencies, the deployment is aborted. This only happens if the optional setting verify_manifest_consistency is turned on in the Image Builder. ( #452 ) Infrastructure plugins can now provide its own statistics for every job. These key-value statistics are displayed in Job details on a Dashboard. ( #77 ) Plugins added to racetrack can now be downloaded via the CLI or the admin interface. ( #451 ) Changed \u00b6 The job_types method's format for plugins has been changed to support providing Dockerfiles directly by the Jobs. See Developing plugins guide . Previous format is still supported for backward compatibility. ( #470 ) [2.29.3] - 2024-05-21 \u00b6 Added \u00b6 Racetrack is compatible with Python 3.12 ( #463 ) Changed \u00b6 \"Connection refused\" error is now retriable in async job calls (will be retried automatically). ( #459 ) Fixed \u00b6 Long deployments (over 20 minutes) no longer get stuck in the final stage. ( #448 ) [2.29.2] - 2024-04-30 \u00b6 Fixed \u00b6 Fixed overwriting STARTING job status in a sync loop. ( #457 ) [2.29.1] - 2024-04-26 \u00b6 Fixed \u00b6 Fixed typing error in racetrack CLI. ( #455 ) [2.29.0] - 2024-04-25 \u00b6 Added \u00b6 Image builder keeps track of the duration of each phase of image building in its metrics, including: preparing job type builder , fetching the source code , building image and pushing image phases. ( #453 ) Changed \u00b6 When building a Job, Racetrack only fetches a single commit from the selected branch in the repository containing the Job's source code. The image builder also performs \"clone\" and \"checkout\" in a single step. Downloading fewer data makes the build process faster. ( #446 ) .git folder is excluded from the job image when copying the source code. Fixed \u00b6 Fixed race condition bug in Async job calls. ( #449 ) [2.28.0] - 2024-04-16 \u00b6 Added \u00b6 Status of an async job call can be checked at endpoint: /pub/async/task/{ID}/status . See Asynchronous calls to jobs guide for more details. ( #434 ) Maximum number of concurrent requests can be limited by max_concurrency field in a manifest: jobtype_extra : max_concurrency : 1 By default, concurrent requests are unlimited. Setting max_concurrency to 1 will make the job process requests one by one. Overdue requests will be queued and processed in order. Having such concurrency limits may cause some requests to wait in a queue. If an average throughput is higher than the job can handle, the queue will grow indefinitely. To prevent that, you can also set jobtype_extra.max_concurrency_queue to limit the queue size. When the queue is full, the job will return 429 Too Many Requests status code. See the Python job type reference - Docker command can be overwritten when running job locally. Check out racetrack run-local --help . ( #431 ) - New flag in Racetrack CLI to avoid using cache when building the job's image: racetrack deploy --no-cache ( #430 ) - Manifest input can be piped into the Racetrack CLI to deploy a job. ( #240 ) Changed \u00b6 Asynchronous job calls are now resilient to restarts by automatically retrying the requests, The restart could be either due to an upgrade in Racetrack services or a job crash (like an Out of Memory kill). Regardless of the reason, the job call will keep retrying until it hits the maximum number of attempts. Async job calls are stored in a database for a short period. ( #424 ) If an error occurs while deploying a job to an infrastructure, the unsuccessful deployment is rolled back to clean up the mess. The timeout for intializing a job in the cluster (waiting until it's alive) has been increased to 15 minutes. ( #436 ) A new status STARTING has been introduced for a Job that is still initialising. This way, the ERROR state is reserved solely for the unexpected problems that arise after a job has been successfully initialised. It is possible to distinguish whether a job has ever run successfully. Thanks to this, version aliases, such as latest or wildcards, won't redirect to starting Jobs that are not yet ready. ( #437 ) Fixed \u00b6 FastAPI dependency has been upgraded to solve memory leaks. ( #442 ) Fixed nullable extra_vars in racetrack CLI. ( #440 ) [2.27.0] - 2024-03-04 \u00b6 Added \u00b6 Job type plugins can validate their part of the manifest. See validate_job_manifest hook in Developing plugins Structured logging can be enabled by setting LOG_STRUCTURED environment variable to true . This will cause Racetrack services to produce logs in a JSON format. The default logging formatter can also be changed by jobs. For more details, refer to the documentation of the specifc job type plugin. Take a look at the python-logging-format sample to see how to configure your own logging formatter in jobs. Introduced asynchronous calls to jobs. This feature lets you avoid timeouts on very long requests by switching to a two-step process: request a task and then check on the result. See Asynchronous calls to jobs guide for more details. Changed \u00b6 pydantic package has been upgraded to version 2 to prevent from conflicts with other up-to-date packages. pydantic is one of the dependencies of the racetrack-client package. [2.26.0] - 2024-01-24 \u00b6 Changed \u00b6 Job type plugins can now build job images from a single Dockerfile. Base Dockerfile has been merged with a job template. It gives more flexibility by allowing to parameterize all build commands of a base image with a user-defined configuration. See Developing plugins and Job type plugins documents. Old style is still supported to keep backwards compatibility. ( #403 ) [2.25.0] - 2024-01-16 \u00b6 Added \u00b6 Administration tab in Dashboard shows number of jobs using the particular job type or infrastructure. This helps to indicate which job types are actually in use and which can be deleted. It also shows which job types and infrastructures are provided by which plugins in a tree view. ( #392 ) Activity tab on Dashboard tracks more kinds of events: Plugin installed Plugin uninstalled Deployment attempt failed Job moved (to other infrastructure) ( #382 ) Changed \u00b6 Dashboard's \"Audit Log\" tab is now called \"Activity\". Fixed \u00b6 Dashboard's auto-update feature works with multiple Lifecycle replicas. Live updates can now be enabled on the jobs list. It is turned off by default. ( #317 ) [2.24.0] - 2023-12-14 \u00b6 Added \u00b6 racetrack plugin bundle command has --out-filename parameter that allows you to overwrite the filename of the output ZIP file. ( #391 ) Changed \u00b6 Infrastructure targets can now mount secret vars to a job regardless of environment variables. Secret variables are passed as a separate argument, they're no longer mixed with regular env vars. Infrastructure plugins should be updated accordingly due to interface change. Specifically, deploy_job function of JobDeployer class now has a new argument runtime_secret_vars: Dict[str, str] with secret env vars for a job. See Supported hooks . ( #394 ) [2.23.0] - 2023-12-04 \u00b6 Added \u00b6 A command racetrack get remote -q (with flag -q or --quiet ) prints only the current address of Lifecycle (without other logs), which makes it usable for scripts. Likewise, a command racetrack get pub -q (with flag -q or --quiet ) prints the current address of Pub service. \"Quiet\" mode is automatically applied when not in a TTY. Racetrack can be turned into a Maintenance mode. During that time, users are unable to make changes. Therefore, deploying, deleting and moving jobs is disabled. See how to enable maintenance mode ( #370 ) A job can be deployed through the Dashboard. To submit a job manifest, go to the \"Deployments\" tab and then \"Deploy a new job\" . Note that secret variables are not supported in this manner; instead, use the command line client. ( #306 ) A new tab called \"Deployments\" displays a list of recent deployment attempts. After selecting one, you may view deployment data such as status, failure cause, duration, and so on. In case of trouble with deployment, you can share this link with a support team. Fixed \u00b6 Fixed checking job condition in case of a missing Content-Type header of job's live endpoint. #376 Word wrapping applied to job logs in Dashboard. #372 SIGTERM signal triggers graceful shutdown. #379 [2.22.0] - 2023-11-07 \u00b6 Added \u00b6 Manifest values can be overriden with key-value pairs coming from a command line. It doesn't modify actual file, but its one-time, in-memory version before submitting it. Racetrack client has --extra-vars KEY=VALUE parameter (or -e in short) that overwrites values found in YAML manifest. KEY is the name of field and it can contain dots to refer to a nested field, for example git.branch=master . VALUE can be any YAML or JSON object. Extra vars parameters can be used multiple times in one command. Example: racetrack deploy -e secret_runtime_env_file=.env.local -e git.branch=$(git rev-parse --abbrev-ref HEAD) It makes CLI commands more script-friendly, so you can overwrite manifest without tracking changes in job.yaml file. Tip: Use racetrack validate command beforehand to make sure your final manifest is what you expected. ( #340 ) You can install Racetrack to a standalone host (e.g. EC2 host or fresh VM instance) using the installer script that runs it on the Docker Engine infrastructure. ( #308 ) Command racetrack get auth-token prints out current auth token. It can be used in CLI scripts: curl -H \"X-Racetrack-Auth: $(racetrack get auth-token)\" ( #357 ) Command racetrack login --username <username> allows you to log in with your username and password (entered into the standard input) and saves the auth token without having to visit the Dashboard page. Fixed \u00b6 Manifest is validated after updating it on Dashboard. Changing primary keys (name or value) is forbidden. ( #293 ) Fixed reviving a dead job after deleting it. ( #366 ) [2.21.0] - 2023-10-16 \u00b6 Added \u00b6 Pub can now be turned into a remote gateway mode. It allows to distribute services between clusters as 2 (or more) separate infrastructures: main hub - hosting core Racetrack services: Lifecycle, image-builder, Dashboard, main Pub remote jobs - infrastructure hosting jobs and remote Pub gateway that protects jobs from unauthorized access. It can cooperate with the following infrastructure plugins: remote Kubernetes plugin remote docker plugin New administrative endpoint for cleaning up plugins mess. ( #331 ) Changed \u00b6 Streaming live logs of a job can now work with multiple infrastructures. Logs streamer interface has been redesigned so infrastructure plugins has to be updated accordingly. Prometheus, Grafana and PostgreSQL have been upgraded. Fixed \u00b6 Database connection is checked in background to avoid blocking metrics endpoint. It reduced number of open database connections. ( #318 ) Faulty plugins can be deleted. [2.20.0] - 2023-09-27 \u00b6 Added \u00b6 Lifecycle has more metrics and Grafana panels for monitoring: number of requests, websocket clients, server errors, open connections and active threads. Improved error handling and logging in Lifecycle. Fixed \u00b6 Fixed metrics endpoint, which was making Lifecycle unresponsive to liveness probes in case of database connection errors. ( #314 ) Fixed TypeError in racetrack client on Python 3.8 ( #320 ) Removed \u00b6 Auto-Update feature has been disabled temporarily due to generating too many requests on multiple replicas. [2.19.0] - 2023-09-04 \u00b6 Added \u00b6 Plugin's manifest can declare its category, which makes the label to be displayed on the plugins list. Category is a kind of the plugin and can be either \"job-type\", \"infrastructure\" or \"core\". If you don't want the plugin to be loaded by Racetrack's Lifecycle or Image-builder (due to missing imports), you can enable it selectively for particular components. Declare components field in a plugin-manifest.yaml and add 'lifecycle' or 'image-builder' to the list. See Developing plugins . ( #291 ) Plugins can implement run_action method for calling a supplementary action with an endpoint. It can be used for debugging purposes or to extend plugin's functionality that is not covered by a plugin interface. Changed \u00b6 Installation to non-local cluster is done by utility script that automatically generates resources and unique passwords for your setup. ( #298 ) Fixed \u00b6 Fixed racetrack run-local command. ( #312 ) Improved finalizing database connections. ( #295 ) [2.18.0] - 2023-08-04 \u00b6 Added \u00b6 Job types can read the manifest file from the job's directory. It might be useful to configure the home page of a job ( see example ). Racetrack-client has new flag --replace available when uploading a plugin: racetrack plugin install <file.zip> --replace . It will delete the existing versions (with the same name) on uploading a new one. Accordingly, there is a checkbox on the Dashboard's Administration tab for replacing a plugin on upload. By default, uploading a new plugin doesn't replace the older versions. ( #270 ) Changed \u00b6 When a job is built from local files (with --build-context=local ), files matching .gitignore are excluded. ( #281 ) [2.17.0] - 2023-07-17 \u00b6 Added \u00b6 When a job is running a deprecated job type version (due to being no longer available), there is a notice displayed on a dashboard - \"info\" icon in a jobs tree and a \"Notice\" field after selecting a job. ( #269 ) Fixed \u00b6 Missing schema.json is now distributed in PyPI packages. ( #283 ) Corrected aggregated metrics on Grafana Dashboards. ( #273 ) Fixed reloading local modules by plugins. ( #275 ) [2.16.1] - 2023-06-19 \u00b6 Changed \u00b6 Dashboard displays original YAML for the job's manifest. ( #262 ) Dashboard now renders a table on the portfolio tab with QTable rather than TableFilter, which occasionally seemed to have rendering issues. Plus, it's more consistent with the current UI styling. ( #258 ) Fixed \u00b6 Dashboard redirects to login page when session expires. ( #266 ) [2.16.0] - 2023-06-05 \u00b6 Added \u00b6 Jobs tree on the Dashboard is refreshed in real time as soon as someone else's change is detected. You can turn it off on the \"Jobs\" page by clicking \"3 dots menu\", \"Auto-update\" toggle. ( #239 ) Changed \u00b6 Chain calls to the jobs should be made by importing a dedicated function from a library provided by the job type plugin. This will keep the chain call function always up-to-date with the Racetrack version. See the example . ( #20 ) [2.15.0] - 2023-05-26 \u00b6 Changed \u00b6 The name of the caller is recorded in the internal logs, giving the ability to track down who made the request based on its ID. Job types can also retrieve that information by extracting it from an HTTP header. See the python-job-type docs and a job for example. ( #246 ) racetrack-client shows more details in case of an HTTP error. ( #245 ) golang , python , and wrapper_properties fields are deprecated in Manifest schema, use jobtype_extra instead. This is backwards compatible. ( #231 ) Editing the manifest online triggers a redeployment of the job, keeping manifest up-to-date with running job. ( #250 ) [2.14.0] - 2023-05-18 \u00b6 Added \u00b6 Dashboard links to job-related Grafana dashobards. issue #206 Manifest of a job can be edited online on the Dashboard after selecting a job. Keep in mind that your online changes can be overwritten by the next deployment, if you didn't reflect these temporary changes in git after all. issue #217 Changed \u00b6 Minor UI improvements and styling for Dashboard, including: Sorting jobs by status (starting from faulty ones) Jobs ordering is persisted in local storage Spinner indicators during loading the data Status indicator (green/red/yellow) and number of jobs on a jobs tree Panel takes up the whole space available - no white bars on the sides Jobs tree has its own scrollbar \"10 seconds ago\" labels are refreshed over time. Fixed \u00b6 Tree of jobs is refreshed after deleting a job. issue #211 [2.13.0] - 2023-05-10 \u00b6 Added \u00b6 \"Jobs\" tab now shows the tree of jobs grouped by the family name. Jobs tree can be filtered by name, version, owner, job type version or infrastructure target. issue #212 Changed \u00b6 Dashboard UI has been revamped and turned into Single Page Application using modern front-end frameworks, which made it more smooth and responsive. issue #212 [2.12.1] - 2023-04-21 \u00b6 Changed \u00b6 Racetrack client returns non-zero exit code in case of error. issue #224 Dashboard is more responsive in case of a database malfunction (shows error rather than hanging indefinitely), since Postgres is no longer its hard dependency. [2.12.0] - 2023-03-29 \u00b6 Added \u00b6 New column in the list command of the Racetrack client displays a job type. Try it out by running racetrack list -c job_type . issue #207 New command racetrack call NAME ENDPOINT PAYLOAD [--version VERSION] [--remote REMOTE] [--curl] allows you to call an endpoint of a job. Provide the name of the job and the payload of the request in JSON or YAML format, for example racetrack call adder /api/v1/perform '{\"numbers\": [40,2]}' . Use --curl flag, if you want to generate a curl query instead of calling the job. Check out racetrack call --help for more details. issue #146 Name of the job can be autocompleted by hitting Tab while typing a CLI command. Remember to run racetrack --install-completion beforehand. Under the hood, it fetches the available jobs from the current remote. Changed \u00b6 The list command of the Racetrack client drops the fancy formatting and INFO / DEBUG logs when being piped into another command (ie. not connected to a terminal/tty device). Try racetrack list | cat . issue #207 Fixed \u00b6 Performance of PUB component has been improved by reducing number of requests made to the Lifecycle and to the database. issue #155 Fixed saving plugin's config. issue #218 Racetrack's cookie can work even on non-HTTPS deployments. issue #225 [2.11.0] - 2023-03-17 \u00b6 Added \u00b6 When specifying a job type version, wildcards can be used. For instance, mllab:1.3.*-3.9-bullseye to subscribe for the latest patches or mllab:1.*.*-3.9-bullseye if you feel more adventurous. The job could be upgraded without committing to manifest then. It's the extension of the existing latest tag, but it now supports multiple job type variants. Note: The release of a new job type version has no effect on existing jobs until they are redeployed. issue #183 Changed \u00b6 Portfolio tab on Dashboard shows the exact version of a job type, even if its manifest specifies a wildcard version or \"latest\". issue #203 Fixed \u00b6 Fixed opening a job after regenerating the token. issue #200 Fixed CSRF Trusted Origins protection. issue #197 [2.10.1] - 2023-03-10 \u00b6 Added \u00b6 Grafana's \"Lifecycle\" dashboard now includes a new panel that tracks the number of jobs with the particular status. Changed \u00b6 \"Orhpaned\" jobs are no longer adopted by Racetrack. This was causing spurious \"Lost\" status after all. issue #163 , issue #134 Third-party dependencies have been upgraded. Fixed \u00b6 Regenerating a user token via the button on your profile now no longer requires a refresh. issue #188 [2.10.0] - 2023-03-03 \u00b6 Added \u00b6 Deletion pop-up contains the name of the job to confirm that you're about to delete the right one. issue #176 Button added to the profile page that lets you regenerate your authentication token. issue #139 Changed \u00b6 Racetrack client keeps the local config file ~/.racetrack/config.yaml with mode 600 (not readable by others) and warns you if this file has insecure permissions since it may contain credentials. issue #179 [2.9.1] - 2023-02-20 \u00b6 Changed \u00b6 Fatman has been renamed to Job in all database tables, columns and values. For instance, scope call_fatman is now call_job . issue #162 [2.9.0] - 2023-02-14 \u00b6 Added \u00b6 Deployment phases, shown by a racetrack client, are more granular, including image building steps. issue #145 Racetrack comes with standalone Prometheus and Grafana. It contains dedicated dashboards for monitoring the jobs and internal services. issue #144 Database connection is being monitored closely by Lifecycle and displayed on a Grafana dashboard. issue #165 Changed \u00b6 Fatman is renamed to Job . fatman.yaml is thus deprecated in favour of job.yaml file name. Although the endpoint for calling the Job has changed to /pub/job/NAME/VERSION/PATH , the old endpoint /pub/fatman/NAME/VERSION/PATH is still supported for backward compatibility. issue #136 [2.8.1] - 2023-01-27 \u00b6 Changed \u00b6 Set User-agent header in all CLI requests to circumvent silly Cloudflare's protection. [2.8.0] - 2023-01-26 \u00b6 Added \u00b6 A job can be moved from one infrastructure target to another by using a new CLI command. Check out racetrack move --help . A new command racetrack list allows to fetch the list of all deployed fatmen with the selected columns. See racetrack list --help for more details. Changed \u00b6 Syntax of Racetrack client has been rearranged. Check out racetrack --help for more details. Notable changes: Racetrack URL is now called \"remote\" and it can be explicitly set with an optional parameter --remote alias in most of the commands. \"Remote\" can be a direct URL or an alias. The current remote can be set once with racetrack set remote ALIAS_OR_URL and then you can omit --remote parameter in the next commands like racetrack list . Check your current remote with racetrack get remote . Although racetrack deploy [WORKDIR] [REMOTE] syntax is still supported, it's deprecated now and racetrack deploy [WORKDIR] [--remote REMOTE] should be used instead. Automatic completion can be activated by running racetrack --install-completion . Then, you'll see relevant prompts after hitting Tab . To show the runtime logs, use racetrack logs NAME [--version VERSION] [--remote REMOTE] . No need to pass workdir with a manifest file any longer. To show the build logs, use racetrack build-logs NAME [--version VERSION] [--remote REMOTE] . No need to pass workdir with a manifest file any longer. To delete a job, use new syntax: racetrack delete NAME --version VERSION [--remote REMOTE] - job name and version is now required. No need to pass workdir with a manifest file any longer. racetrack plugin install PLUGIN_URI [--remote REMOTE] - \"remote\" is now an optional parameter instead of a required argument. To set up an alias for Racetrack's remote URL, use racetrack set alias ALIAS RACETRACK_URL (former racetrack config alias set ). To set read-access credentials for a git repository, use racetrack set credentials REPO_URL USERNAME TOKEN_PASSWORD (former racetrack config credentials set ). To save user's Racetrack Auth Token for Racetrack server, use racetrack login USER_TOKEN [--remote REMOTE] (former racetrack login RACETRACK_URL USER_TOKEN ). racetrack logout [--remote REMOTE] (former racetrack logout RACETRACK_URL ). In case of problems with reaching the Job, PUB shows the error page (JSON with the meaningful reason), instead of a white page of death. The \"Open\" button gets deactivated on the Dashboard for jobs that are not Running . [2.7.0] - 2023-01-05 \u00b6 Added \u00b6 Every plugin can have its own configuration, stored in a YAML file. After uploading the plugin, the configuration can be edited on Dashboard's Administration tab, with a Edit Config button. Plugin can read the configuration from a file self.config_path: pathlib.Path . Racetrack instance can operate with multiple infrastructure targets. By default, there is no infrastructure target. It has to be extended by installing a plugin. The infrastructure target is displayed next to the fatman name, on Dashboard's list. There is a new, optional field infrastructure_target in a Manifest. If given, it enforces using the specific target in case of many infrastructures are in use. Otherwise, the deafult infrastructure is applied. Fatman can be composed of multiple Docker images provided by a plugin or user's manifest. [2.6.0] - 2022-12-02 \u00b6 Added \u00b6 \"Portfolio\" table in a Dashboard has a new column \"Job type version\". List of available job types can be checked with: racetrack plugin list --job-types <racetrack_url> . It is also listed in the Administration tab on Dashboard . Exact job type version can be checked at Fatman's /health endpoint. Fixed \u00b6 When uploading a faulty plugin, the errors are handled in a more reliable manner. Plugin's requirements are installed with non-root user context. Plugin directory is accessible when initializing the plugin in __init__ method. Fixed conflict between newer protobuf version and opentelemetry package. [2.5.1] - 2022-10-28 \u00b6 Added \u00b6 Users can change their password on a Dashboard -> Profile tab -> Change Password Fatman webviews can now serve ASGI applications (like FastAPI) Fixed \u00b6 Added missing __init__.py file in one of the racetrack-client modules. [2.5.0] - 2022-10-18 \u00b6 Changed \u00b6 One job type can be installed in multiple versions at the same time. Users have to pick one of these versions and specify it in the manifest of their fatman, eg. jobtype: python3:2.4.0 . Version of a job type is now required in the jobtype field of Manifest in a manner name:version . latest version can be used (resolving to the highest semantic version), though it's discouraged. Plugin bundler has been moved to racetrack-client. Now you can do racetrack plugin bundle instead of racetrack-plugin-bundler bundle . [2.4.0] - 2022-10-11 \u00b6 Added \u00b6 Plugins can be installed by means of racetrack-client command racetrack plugin install <plugin_uri> <racetrack_url> , where plugin_uri can be either: local file path (eg. python3-job-type-2.4.0.zip ), URL to a remote HTTP file (eg. https://github.com/TheRacetrack/plugin/releases/download/2.4.0/python3-job-type-2.4.0.zip ), GitHub repository name (eg. github.com/TheRacetrack/plugin-python-job-type ) - it takes the ZIP file from the latest release. GitHub repository name with version (eg. github.com/TheRacetrack/plugin-python-job-type==2.4.0 ) - it takes the ZIP file from the specific release. Plugins can be uninstalled with a new command: racetrack plugin uninstall <plugin_name> <plugin_version> <racetrack_url> . List of currently installed plugins can be checked with: racetrack plugin list <racetrack_url> . Changed \u00b6 All job types are individual plugins now. Racetrack starts without any job type by default. Base Fatman images are built inside Racetrack by image-builder so it's no longer needed to push images prior to the plugin release. [2.3.0] - 2022-09-23 \u00b6 Added \u00b6 Fatman access can be narrowed down to single endpoints. When adding Auth Resource Permission (in Admin panel), there is a new endpoint field, which narrows down the permission only to this particular Fatman's endpoint. If not set, it covers all endpoints (just as other resource filters do). For instance, ESC can have a permission with call_fatman scope to only call one endpoint. OpenTelemetry can be turned on in Racetrack. Given the OTLP collector endpoint, the traces from lifecycle, pub and fatman will be sent there. Changed \u00b6 python3 job type is now based on 3.9-slim-bullseye image instead of 3.8-slim-buster . Plugins are distributed as ZIP files. They can be installed and uninstalled in a Dashboard's Administration page. See using-plugins.md . [2.2.1] - 2022-08-25 \u00b6 Changed \u00b6 Maximum amount of memory for a fatman is reduced to 1GiB by default (if not set explicitly in a manifest). In general the maximum memory can't be more than 4 times higher than the minimum memory. Racetrack keeps an eye on this rule by automatically adjusting minimum memory amount, if needed. It is recommended to declare maximum memory amount explicitly in a manifest by defining resources.memory_max field. Fatman versions in PUB's metrics are always resolved to the exact version instead of wildcards. [2.2.0] - 2022-08-16 \u00b6 Added \u00b6 Golang job types serve interactive Swagger UI with API documentation on the home page. Changed \u00b6 Function fatman_job_types of plugins changed its signature. Now the first parameter is a Docker Registry prefix to work well with different Docker registry namespaces. All job-type plugins has to be adjusted accordingly. See plugins-job-types.md for more details. A namespace for docker images in a Docker Registry is configurable so that Racetrack can be released to any Docker Registry. A namespace for fatman workloads running on Kubernetes is now configurable by means of FATMAN_K8S_NAMESPACE environment variable, by default it's set to racetrack . Hostname subdomain of Racetrack services is now configurable by means of RACETRACK_SUBDOMAIN environment variable, by default it's set to racetrack . Golang job type has been moved to Plugin - Go Job Type . Now it's not enabled by default in Racetrack. [2.1.2] - 2022-07-29 \u00b6 Changed \u00b6 Overall requests performance has been improved by switching from Flask & twisted (WSGI) to FastAPI & Uvicorn (ASGI) web server. Fatman server should now be less laggy under heavy load. Additionaly, FastAPI comes with better request validation and newer SwaggerUI page. Fatman redeployment is needed for the changes to take effect. [2.1.1] - 2022-07-25 \u00b6 Added \u00b6 Plugins can provide its own Fatman Deployers to deploy workloads to other types of platforms. See developing-plugins.md and Docker Daemon Deployer Plugin Changed \u00b6 Fatman's metrics now contain number of requests and duration for each endpoint (including auxiliary endpoints). Fixed \u00b6 Fixed redeploying fatmen from the dashboard Making multiple attempts to deploy the same fatman at the same time is now disallowed. \"In progress\" deployments are halted when Lifecycle restarts (due to upgrade or eviction) [2.1.0] - 2022-07-08 \u00b6 Changed \u00b6 Fatman secrets (git credentials and secret vars) are kept in Racetrack, so fatmen are fully reproducible again. In other words, it is technically possible to rebuild and reprovision fatman by anyone who is allowed to do so (anyone with permissions, not just the author) to apply latest Racetrack changes. You can click \"Rebuild and provision\" to do a complete redeployment of a fatman, or just \"Reprovision\" to apply changes without rebuilding the image (eg. useful when changing replicas count). Racetrack client no longer depends on requests package in favour of using the built-in urllib module. [2.0.0] - 2022-06-30 \u00b6 Added \u00b6 Plugins can hook into the post_fatman_delete event to add their actions after deleting a fatman. See Teams Notifications Racetrack Plugin Response errors are also displayed in the logs in addition to returning message in HTTP payload. Racetrack client has new command racetrack run-local . <racetrack_url> allowing to run fatman locally based on docker image built by remote Racetrack. Docker has to be installed on your system. Changed \u00b6 Fatman permissions model is more restrictive. Now users, ESCs and fatman families can be granted fine-grained permissions. Users can have access to a subset of fatmen and they can see and manage only those that are allowed for them. Old permissions have been migrated (Fatman-to-Fatman and ESC-to-fatman access). By default, Users can now do the following: read all fatman status (browse on a dashboard) call endpoints of every fatman deploy and redeploy fatmen delete only fatmen that has been deployed by the user (starting from the newly deployed ones) Admin can grant or revoke permissions. Permisssion can cover either all fatmen, whole fatman family or a single fatman. Permission is related to one of the operation type (called \"scope\"): read_fatman - list fatman, check fatman details deploy_fatman - deploy fatman in a particular family, redeploy fatman deploy_new_family - deploy new fatman family delete_fatman - move to trash, dismantle from a cluster call_fatman - call fatman endpoints call_admin_api - not important for regular users full_access - not important for regular users Authentication tokens have been revamped and format changed. Users are required to use new tokens in order to use CLI racetrack client. Log in to Dashboard, copy the token and run racetrack login again. Old ESC tokens are still supported to keep backwards compatibility, but they should be regenerated if possible. A method for calling fatman from inside of other fatman has been changed due to new auth model. See python-chain sample . All fatmen calling another fatman are required to be redeployed (to use new tokens). Since there is one type of tokens, Auth header has been unified and renamed to X-Racetrack-Auth . It should be used no matter if it's ESC, User or a Fatman. Old headers are still supported but may be abandoned in the future: X-Racetrack-User-Auth , X-Racetrack-Esc-Auth , X-Racetrack-Fatman-Auth Racetrack services has been adjusted to the new URL format. /ikp-rt prefix has been removed from all URLs. Instead, ikp-rt part may be included in the cluster hostname, making session cookies to work hassle-free and more secure. For instance, Racetrack Dashboard address is now: https://ikp-rt.<cluster.address>/dashboard . Lifecycle address is: https://ikp-rt.<cluster.address>/lifecycle . PUB runs on: https://ikp-rt.<cluster.address>/pub . Thus, Fatman instances can be accessed at: https://ikp-rt.<cluster.address>/pub/fatman/<name>/<version>/<path> . [1.2.0] - 2022-05-31 \u00b6 Added \u00b6 fatman.yaml manifest file supports overlays. Including extends field (with a path to base manifest) will make it to merge base and overlay layers. It may come in handy when working with multiple environments (dev/test/prod) referring to the same base manifest. See overlays sample . Job types can be extended with plugins. Check out sample job-type plugin and developing-plugins.md to see a guide how to create and activate plugins. Racetrack documentation is now served at \"Docs\" tab in Dashboard. Check out \"Home\" page to get started. Plugins can extend Racetrack documentation with its own pages written in Markdown (it may be generated at runtime depending on plugin configuration). Changed \u00b6 racetrack validate PATH now evaluates the manifest outcome and prints it to verify the result of extending overlay manifest. Critical actions like deleting fatman or redeploying it open a modal window with confirmation button to prevent accidental removal. Racetrack environments are easier distinguishable between dev, test, preprod and prod due to different colors of the dashboard navbar and additional prefix [prod] in the title. [1.1.0] - 2022-04-21 \u00b6 Added \u00b6 New \"Audit Log\" tab in Dashboard shows activity events done by users, eg. \"fatman F1 of user Alice has been deleted by Bob\". It can be filtered by events related to a logged user, whole fatman family or a particular fatman. Changed \u00b6 Once a fatman name (and version) has been claimed, it can never be deployed again to prevent accidental/hostile reusing. Login page has been revamped. Fixed \u00b6 Display error cause on dashboard in case of \"Redeploy\" failure. [1.0.2] - 2022-04-06 \u00b6 Changed \u00b6 Set default fatman maximum memory to 8GiB [1.0.1] - 2022-04-06 \u00b6 Changed \u00b6 Fatman dependency graph is interactive, nodes are movable. Clicking on a node filters out all its neighbours to see which fatmen can be accessed by particular ESC or which ESCs have access to a particular fatman. Changed \u00b6 Set default fatman minimum memory to 150MiB [1.0.0] - 2022-03-29 \u00b6 Added \u00b6 Fatman manifest can declare resources to be allocated to the Fatman with resources field, including minimum/maximum memory, minimum/maximum CPU cores. See Fatman Manifest File Dashboard shows datetime when the last call was made to a fatman. Fatman redeployment is needed for the changes to take effect. Dashboard has a new \"Portfolio\" tab for browsing fatmen freely with custom criteria and showing the candidates for removal. [0.7.8] - 2022-03-18 \u00b6 [0.7.7] - 2022-03-18 \u00b6 Changed \u00b6 Racetrack client validates given token when running racetrack login and complains immediately if it's wrong Fatman returns 400 BAD REQUEST response in case of missing argument, unexpected argument or any other TypeError. Fixed \u00b6 Fixed Fatmen getting Lost. Fatmen shouldn't wrongly change its status to \"Lost\" any longer. [0.7.6] - 2022-03-04 \u00b6 [0.7.5] - 2022-03-01 \u00b6 Added \u00b6 Log messages contain Request Tracing ID, which allows to track down what was happening during the request processing by different services. PUB can read tracing ID from a specific HTTP request header. If not given, it will be generated. On IKP clusters this header is named traceparent (by default it's X-Request-Tracing-Id ). ID is also returned in the response headers, so you can use it when reporting bugs. Fatman redeployment is needed for the changes to take effect. [0.7.4] - 2022-03-01 \u00b6 Added \u00b6 PUB collects histogram of fatman response times and metrics related to endless requests and response codes. [0.7.3] - 2022-02-28 \u00b6 Changed \u00b6 GET /metrics 200 requests are hidden from access logs. Reading from stdin is prohibited at fatman initialization and it raises an error once detected, showing stack trace with the place where there was an attempt. Fatman doesn't go unresponsive any longer, awaiting input indefinitely. Fixed \u00b6 Fatman logs doesn't show misleading \"Timing out client\" errors anymore. It was related to HTTP sessions kept alive by browsers, not actual fatman request time-outs. [0.7.2] - 2022-02-14 \u00b6 Changed \u00b6 Access logs are not polluted with redundant /live , /ready & /health requests. Fixed \u00b6 Racetrack logs show non-ASCII characters [0.7.1] - 2022-02-07 \u00b6 Added \u00b6 Racetrack dashboard shows active plugins along with their versions. Changed \u00b6 Fatman manifest is checked against using HTTPS git remote only (SSH is not supported). None values metrics are allowed. They're ignored instead of logging error as there is no null in Prometheus, only samples that don't exist. Fatman access logs have shorter, concise format. Time, HTTP method, path and response code are the only ones displayed. [0.7.0] - 2022-01-31 \u00b6 Added \u00b6 When accessing a Fatman through a PUB, there can be used wildcard version format: 1.x or 1.2.x . This points to a highest stable version of the Fatman that matches the pattern. For instance, 1.x keeps track of all v1 updates, but skips next major versions 2.0.0 and above. Fatman redeployment is needed for the changes to take effect. Changed \u00b6 \"latest\" fatman version now points to the highest version disregarding versions with any label (eg. -dev ). Thus deploying 2.0.0-alpha pre-release versions doesn't affect \"latest\" alias until 2.0.0 is deployed. Fixed \u00b6 Public endpoints are properly handled with version aliases (eg. \"latest\", \"1.x\") [0.6.1] - 2022-01-24 \u00b6 Added \u00b6 Phases of deployment are tracked by showing \"building image\", \"creating cluster resources\", \"initializing Fatman entrypoint\", etc. in RT client in place of bare \"deployment in progress...\" message. Changed \u00b6 Fatman versions are validated according to SemVer format . Version should match MAJOR.MINOR.PATCH[LABEL] , eg.: 0.1.2 or 0.0.0-dev . Fixed \u00b6 Zero-value metrics are shown properly at /metrics endpoint of a Fatman. [0.6.0] - 2022-01-17 \u00b6 Added \u00b6 Racetrack functionality can be extended with plugins. Check out developing-plugins.md to see a guide how to create and activate plugins. Changed \u00b6 Whole directories are handled in static_endpoints to serve recursively all of its content. Metrics from Postgres database and PUB are exported. It gives better monitoring of Fatman requests performance. Registration form has been revamped to show errors in a more user-friendly way. Pods resource requests have been adjusted to its actual needs. Cluster resource shortages shouldn't happen so prematurely anymore. Fixed \u00b6 Show real reason of internal exceptions, eg. Neo4j auth error [0.5.0] - 2022-01-10 \u00b6 Changed \u00b6 Overwriting fatmen is disabled. It's not allowed to deploy a fatman with the same name & version as an existing one, unless it's deployed with --force flag. Duplicated env vars error is now more meaningful: \"found illegal runtime env vars, which conflict with reserved names: RK_HOST\" [0.4.1] - 2022-01-05 \u00b6 Fixed \u00b6 Fixed Manifests getting empty on Racetrack upgrade. [0.4.0] - 2022-01-03 \u00b6 Added \u00b6 Number of running instances of the Fatman can be configured with replicas field in a manifest.yaml. In case of multiple replicas, racetrack logs collects logs from all instances at once. Deployed fatmen can be deleted through racetrack client using new command: racetrack delete <workdir> <racetrack_url> [--version X.Y.Z] . [0.3.5] - 2021-12-09 \u00b6 Added \u00b6 Fatman endpoints that are supposed to be accessed publicly (without authentication) can be declared in public_endpoints field of manifest. This makes a request that needs to be approved by Racetrack admin. See python-ui-flask sample . Fixed \u00b6 Improved overall Fatman response time. [0.3.4] - 2021-12-06 \u00b6 Changed \u00b6 Fatman's /live , /ready and /health endpoints doesn't require authentication. [0.3.3] - 2021-11-22 \u00b6 Changed \u00b6 Fatman \"dashboard\" renamed to \"webview\". See Python Job Type docs: Custom Webview UI. On local dev Racetrack there's no need to commit every job change prior to deployment (it gets the working directory snapshot). Fixed \u00b6 Default admin account is no longer recreated. [0.3.1] - 2021-11-15 \u00b6 Added \u00b6 New optional field in Manifest: labels - dictionary with metadata describing fatman for humans. Labels are shown on a dashboard page besides fatman name. Example input data (shown on Swagger UI) can be defined for auxiliary endpoints by implementing docs_input_examples . See Python Job Type docs: Auxiliary endpoints section and python-auxiliary-endpoints sample . Fixed \u00b6 RT specific Dashboard cookie is removed upon logout Error returned by Fatman contains more information (including Lifecycle response) and uses correct status code (instead of 500) Fixed showing spurious success of Fatman redeployment (ensure new Fatman responds to health probes). [0.3.0] - 2021-11-08 \u00b6 Added \u00b6 racetrack logs command follows Fatman logs when called along with --follow flag. Changed \u00b6 Lifecycle API is fully authenticated, every request to Fatman needs to have either User, ESC or Fatman token. X-Racetrack-Caller header has changed to X-Racetrack-Esc-Auth [0.2.1] - 2021-10-25 \u00b6 Added \u00b6 Multiple versions of the same fatman are allowed. Obsolete versions are not removed automatically. ESC permissions are assigned to whole fatmen family by its name. Build logs are always captured and can be seen on Racetrack Dashboard by clicking \"Logs\" / \"Build logs\" or by issuing racetrack build-logs <workdir> <racetrack_url> . Fatman logs are displayed on Dashboard under option \"Logs\" / \"Runtime logs\". Changed \u00b6 Fatman base URL is changed to /ikp-rt/pub/fatman/<fatman-name>/<version>/ . Version can be e.g. 0.0.1 or latest . [0.2.0] - 2021-10-11 \u00b6 Added \u00b6 Environment vars can be configured in manifest (including build time env, runtime env and secrets). Therefore, pip dependencies can be installed from a private repository. See Python Job Type docs: Environment variables section. Swagger UI allows to set X-Racetrack-Caller header for making authorized Fatman calls. Dashboard profile shows a user token Racetrack client deploy and logs commands enforces logging with token racetrack login and logout commands racetrack config show for viewing racetrack config Changed \u00b6 docs_input_example outcome is validated at deployment stage. It's checked against JSON-serializability and its size should not exceed 1 MB. This solves unresponsive Swagger UI pages, but it may enforce to use a brief example at the expense of having representative one. [0.1.1] - 2021-10-04 \u00b6 Added \u00b6 Customized Prometheus metrics can be exported by implementing metrics method. Changed \u00b6 Dashboard allows users to register, to be able to login and view list of Fatmen. Fixed \u00b6 Generating graph on RT Dashboard Generating token for ESC in RT admin panel [0.1.0] - 2021-09-27 \u00b6 Changed \u00b6 Hide \"Open\" button for erroneous Fatman on Dashboard page. Instead, show a section containing error message and logs. Racetrack client infers full lifecycle URL, namely https protocol and /ikp-rt/lifecycle path, if not given. For instance, ikp-dev-cluster.example.com is a valid URL to deploy fatmen. Admin panel not available on Dashboard (moved to Lifecycle). Fixed \u00b6 Use DNS hostname of a cluster instead of raw IP address when redirecting to a Fatman URL (fixed in dashboard \"Open\" link as well as in racetrack deploy output). [0.0.17] - 2021-09-20 \u00b6 Added \u00b6 Fatman serves 3 version numbers at /health endpoint: git_version - arising from the git history of a job source code, fatman_version - taken from version field in manifest (this is also displayed at SwaggerUI page). deployed_by_racetrack_version - version of the Racetrack the Fatman was deployed with. Dashboard displays current Racetrack version in the footer. Static endpoints for serving local files at particular path (eg: /xrai endpoint serving xrai.yaml ) Authorization to PUB (optional right now) for accessing Fatman /perform . Requires creating ESC and distributing its token to user. Fixed \u00b6 Show localized Dashboard dates on Firefox [0.0.16] - 2021-09-13 \u00b6 Added \u00b6 build-essential package is incorporated into python3 base image. gcc and g++ packages are there out-of-the-box, so there is no need to include them manually to your system_dependencies any longer. Dashboard page shows both dates for each Fatman: \"Created at\" and \"Last updated\" (deployed at) along with its age (e.g. \"5 minutes ago\", \"5 days ago\"). Changed \u00b6 In case of python3 initialization error, full traceback coming from fatman is displayed to a user (eg: it's easier to find out where is a bad import). Deployment errors displayed by CLI client are shortened and yet more meaningful. Client's traceback is not displayed if not needed. Datetimes shown on Dashboard get a localized timezone from a browser, so it's no longer UTC time (unless you work from UK). Fatmen list on Dashboard is ordered by last deployment date. Fixed \u00b6 Fix retrieving logs right after deploying a Fatman. Increased timeout for creating a pod. It takes into account longer image pulling. [0.0.15] - 2021-09-07 \u00b6 Added \u00b6 New required owner_email field (email address of the Fatman's owner to reach out) in manifest List of Fatmen (in Dashboard) shows who deployed the fatman recently (username is taken from git credentials) Auxiliary endpoints - custom endpoint paths handled by entrypoint methods (eg. /explain endpoint) Changed \u00b6 Racetrack client shows full logs from building the image in case of failure. [0.0.14] - 2021-09-06 \u00b6 Changed \u00b6 Fatman has read-write access to local working directory. Deployment errors are more meaningful and less misleading due to revised health checks (ie. cluster errors are distinguished from job syntax errors or initialization errors). One common versioning for all Racetrack components (backend and CLI client) Fixed \u00b6 Fix \"Critical worker timeout\" WSGI error Increase memory limits for building images (up to 8Gi) Fix Lifecycle resurrecting dead jobs [0.0.5] - 2021-08-26 \u00b6 Added \u00b6 Setting aliases for Racetrack servers Changed \u00b6 Syntax for configuring private registries credentials Input payload is flattened (without \"args\" & \"kwargs\") [0.0.4] - 2021-08-20 \u00b6 Added \u00b6 Showing recent logs from Fatman [0.0.2] - 2021-08-10 \u00b6 Added \u00b6 Deploying Fatman","title":"Changelog"},{"location":"docs/CHANGELOG/#changelog","text":"All user-facing , notable changes will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"docs/CHANGELOG/#unreleased","text":"","title":"[Unreleased]"},{"location":"docs/CHANGELOG/#2360-2025-09-26","text":"","title":"[2.36.0] - 2025-09-26"},{"location":"docs/CHANGELOG/#added","text":"Websocket traffic is supported for Racetrack jobs. ( #592 )","title":"Added"},{"location":"docs/CHANGELOG/#2351-2025-03-20","text":"","title":"[2.35.1] - 2025-03-20"},{"location":"docs/CHANGELOG/#added_1","text":"Names of the foreign objects in the Records table are displayed next to their IDs. ( #570 )","title":"Added"},{"location":"docs/CHANGELOG/#2350-2025-02-05","text":"","title":"[2.35.0] - 2025-02-05"},{"location":"docs/CHANGELOG/#added_2","text":"More types of Pub errors are presented on the Grafana Dashboard, including panic errors. ( #546 ) A new tab in the Dashboard called \"Records\" lets you manage all the data models stored in Racetrack's database. It's designed to replace the Django Admin panel and incorporate its features into Racetrack's built-inadministration interface. This way, you can manage database entities in one central place without needing to log in separately. ( #543 )","title":"Added"},{"location":"docs/CHANGELOG/#changed","text":"Upgraded third-party dependencies. Use svc.cluster.local suffix for Kubernetes DNS names. Take into account negation patterns in .gitignore ( #556 )","title":"Changed"},{"location":"docs/CHANGELOG/#2342-2024-11-29","text":"","title":"[2.34.2] - 2024-11-29"},{"location":"docs/CHANGELOG/#fixed","text":"Fixed nil pointer dereference in case of caller data missing in the cached Pub response. ( #544 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2341-2024-11-12","text":"","title":"[2.34.1] - 2024-11-12"},{"location":"docs/CHANGELOG/#fixed_1","text":"Fixed infinite redirects to Sign in page on Dashboard. ( #541 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2340-2024-11-12","text":"","title":"[2.34.0] - 2024-11-12"},{"location":"docs/CHANGELOG/#added_3","text":"Internal calls of Pub service are cached for a short configurable time of 1-10 minutes. This improves performance and robustness in case of issues with Lifecycle, network or a database. ( #213 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_1","text":"Upgraded third-party dependencies. Enhanced the proxying of requests from Dashboard to Lifecycle in order to display detailed errors on the Dashboard ( #540 )","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_2","text":"Fixed Duplicate Prepared Statement in psycopg connector ( #536 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2330-2024-10-01","text":"","title":"[2.33.0] - 2024-10-01"},{"location":"docs/CHANGELOG/#changed_2","text":"Most database operations are now handled by the new database access layer, making direct SQL queries. It has a configurable connection pool that reduces the number of open connections. ( #480 ) Maintenance mode now also prohibits making calls to the Jobs. If turned on, it responds with \"503 Service Unavailable\". ( #517 ) Job's secrets are mounted when building the image. This hides the secret environment variables, so they can't be accessed after the build. ( #474 )","title":"Changed"},{"location":"docs/CHANGELOG/#removed","text":"Support for OpenTelemetry exporters has been dropped.","title":"Removed"},{"location":"docs/CHANGELOG/#fixed_3","text":"Fixed deployment error about missing warnings. ( #523 ) Added informative error about crash during Job initialization. ( #469 ) Resolved compatibility issues with the Racetrack client for Python 3.8 related to unsupported typing operands. ( #527 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2321-2024-08-26","text":"","title":"[2.32.1] - 2024-08-26"},{"location":"docs/CHANGELOG/#added_4","text":"sslmode and sslrootcert is supported in database connector for PostgreSQL ( #509 ) Deprecated fields in the manifest trigger a warning. ( #482 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_3","text":"\"404 Not Found\" responses are treated as errors in Async Job calls. ( #508 ) Attempting to install multiple versions of the same infrastructure plugin causes an error. ( #489 )","title":"Changed"},{"location":"docs/CHANGELOG/#2320-2024-08-19","text":"","title":"[2.32.0] - 2024-08-19"},{"location":"docs/CHANGELOG/#added_5","text":"A new tab \"External Consumers\" has been added to the Dashboard. It allows to browse, read auth token and create a new ESC. The tab is only available for administrator users. ( #484 ) \"External Consumers\" can have multiple auth tokens to facilitate the process of rotating tokens. Tokens can be created and revoked from the Dashboard. Dashboard keeps track of the date when each token was in-use last time. ( #487 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_4","text":"Reconciliation loop is now turned off by default. It can be enabled in configuration, if needed. Instead, reconciliation can be triggered manually through the Dashboard, at Administration tab. ( #497 ) Jobs are no longer deleted on unsuccessful, retried deployment. ( #491 ) Pub's metrics has been revised to detect overloaded jobs easier. ( #493 )","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_4","text":"Download button works on Kubernetes clusters. ( #490 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2310-2024-07-09","text":"","title":"[2.31.0] - 2024-07-09"},{"location":"docs/CHANGELOG/#added_6","text":"Lifecycle now provides more metrics to monitor its health. In particular, it tracks the number of established database connections and the duration of fetching Job data from the database. To improve logging, Lifecycle now displays more details about cancelled requests. ( #472 ) Support deployment warnings, such as #452 . Internal racetrack components can now send \"log\" warnings for a deployment. ( #473 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_5","text":"The Pub-Lifecycle client now has a timeout of 30 seconds. The HTTP thread pool of Lifecycle has been increased to 60 threads. This setting is configurable through the config file.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_5","text":"Fix portfolio UI sorting bug. ( #479 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2300-2024-06-10","text":"","title":"[2.30.0] - 2024-06-10"},{"location":"docs/CHANGELOG/#added_7","text":"Submitted manifest YAML is compared with the one in the job's repository. In case of incosistencies, the deployment is aborted. This only happens if the optional setting verify_manifest_consistency is turned on in the Image Builder. ( #452 ) Infrastructure plugins can now provide its own statistics for every job. These key-value statistics are displayed in Job details on a Dashboard. ( #77 ) Plugins added to racetrack can now be downloaded via the CLI or the admin interface. ( #451 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_6","text":"The job_types method's format for plugins has been changed to support providing Dockerfiles directly by the Jobs. See Developing plugins guide . Previous format is still supported for backward compatibility. ( #470 )","title":"Changed"},{"location":"docs/CHANGELOG/#2293-2024-05-21","text":"","title":"[2.29.3] - 2024-05-21"},{"location":"docs/CHANGELOG/#added_8","text":"Racetrack is compatible with Python 3.12 ( #463 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_7","text":"\"Connection refused\" error is now retriable in async job calls (will be retried automatically). ( #459 )","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_6","text":"Long deployments (over 20 minutes) no longer get stuck in the final stage. ( #448 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2292-2024-04-30","text":"","title":"[2.29.2] - 2024-04-30"},{"location":"docs/CHANGELOG/#fixed_7","text":"Fixed overwriting STARTING job status in a sync loop. ( #457 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2291-2024-04-26","text":"","title":"[2.29.1] - 2024-04-26"},{"location":"docs/CHANGELOG/#fixed_8","text":"Fixed typing error in racetrack CLI. ( #455 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2290-2024-04-25","text":"","title":"[2.29.0] - 2024-04-25"},{"location":"docs/CHANGELOG/#added_9","text":"Image builder keeps track of the duration of each phase of image building in its metrics, including: preparing job type builder , fetching the source code , building image and pushing image phases. ( #453 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_8","text":"When building a Job, Racetrack only fetches a single commit from the selected branch in the repository containing the Job's source code. The image builder also performs \"clone\" and \"checkout\" in a single step. Downloading fewer data makes the build process faster. ( #446 ) .git folder is excluded from the job image when copying the source code.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_9","text":"Fixed race condition bug in Async job calls. ( #449 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2280-2024-04-16","text":"","title":"[2.28.0] - 2024-04-16"},{"location":"docs/CHANGELOG/#added_10","text":"Status of an async job call can be checked at endpoint: /pub/async/task/{ID}/status . See Asynchronous calls to jobs guide for more details. ( #434 ) Maximum number of concurrent requests can be limited by max_concurrency field in a manifest: jobtype_extra : max_concurrency : 1 By default, concurrent requests are unlimited. Setting max_concurrency to 1 will make the job process requests one by one. Overdue requests will be queued and processed in order. Having such concurrency limits may cause some requests to wait in a queue. If an average throughput is higher than the job can handle, the queue will grow indefinitely. To prevent that, you can also set jobtype_extra.max_concurrency_queue to limit the queue size. When the queue is full, the job will return 429 Too Many Requests status code. See the Python job type reference - Docker command can be overwritten when running job locally. Check out racetrack run-local --help . ( #431 ) - New flag in Racetrack CLI to avoid using cache when building the job's image: racetrack deploy --no-cache ( #430 ) - Manifest input can be piped into the Racetrack CLI to deploy a job. ( #240 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_9","text":"Asynchronous job calls are now resilient to restarts by automatically retrying the requests, The restart could be either due to an upgrade in Racetrack services or a job crash (like an Out of Memory kill). Regardless of the reason, the job call will keep retrying until it hits the maximum number of attempts. Async job calls are stored in a database for a short period. ( #424 ) If an error occurs while deploying a job to an infrastructure, the unsuccessful deployment is rolled back to clean up the mess. The timeout for intializing a job in the cluster (waiting until it's alive) has been increased to 15 minutes. ( #436 ) A new status STARTING has been introduced for a Job that is still initialising. This way, the ERROR state is reserved solely for the unexpected problems that arise after a job has been successfully initialised. It is possible to distinguish whether a job has ever run successfully. Thanks to this, version aliases, such as latest or wildcards, won't redirect to starting Jobs that are not yet ready. ( #437 )","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_10","text":"FastAPI dependency has been upgraded to solve memory leaks. ( #442 ) Fixed nullable extra_vars in racetrack CLI. ( #440 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2270-2024-03-04","text":"","title":"[2.27.0] - 2024-03-04"},{"location":"docs/CHANGELOG/#added_11","text":"Job type plugins can validate their part of the manifest. See validate_job_manifest hook in Developing plugins Structured logging can be enabled by setting LOG_STRUCTURED environment variable to true . This will cause Racetrack services to produce logs in a JSON format. The default logging formatter can also be changed by jobs. For more details, refer to the documentation of the specifc job type plugin. Take a look at the python-logging-format sample to see how to configure your own logging formatter in jobs. Introduced asynchronous calls to jobs. This feature lets you avoid timeouts on very long requests by switching to a two-step process: request a task and then check on the result. See Asynchronous calls to jobs guide for more details.","title":"Added"},{"location":"docs/CHANGELOG/#changed_10","text":"pydantic package has been upgraded to version 2 to prevent from conflicts with other up-to-date packages. pydantic is one of the dependencies of the racetrack-client package.","title":"Changed"},{"location":"docs/CHANGELOG/#2260-2024-01-24","text":"","title":"[2.26.0] - 2024-01-24"},{"location":"docs/CHANGELOG/#changed_11","text":"Job type plugins can now build job images from a single Dockerfile. Base Dockerfile has been merged with a job template. It gives more flexibility by allowing to parameterize all build commands of a base image with a user-defined configuration. See Developing plugins and Job type plugins documents. Old style is still supported to keep backwards compatibility. ( #403 )","title":"Changed"},{"location":"docs/CHANGELOG/#2250-2024-01-16","text":"","title":"[2.25.0] - 2024-01-16"},{"location":"docs/CHANGELOG/#added_12","text":"Administration tab in Dashboard shows number of jobs using the particular job type or infrastructure. This helps to indicate which job types are actually in use and which can be deleted. It also shows which job types and infrastructures are provided by which plugins in a tree view. ( #392 ) Activity tab on Dashboard tracks more kinds of events: Plugin installed Plugin uninstalled Deployment attempt failed Job moved (to other infrastructure) ( #382 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_12","text":"Dashboard's \"Audit Log\" tab is now called \"Activity\".","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_11","text":"Dashboard's auto-update feature works with multiple Lifecycle replicas. Live updates can now be enabled on the jobs list. It is turned off by default. ( #317 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2240-2023-12-14","text":"","title":"[2.24.0] - 2023-12-14"},{"location":"docs/CHANGELOG/#added_13","text":"racetrack plugin bundle command has --out-filename parameter that allows you to overwrite the filename of the output ZIP file. ( #391 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_13","text":"Infrastructure targets can now mount secret vars to a job regardless of environment variables. Secret variables are passed as a separate argument, they're no longer mixed with regular env vars. Infrastructure plugins should be updated accordingly due to interface change. Specifically, deploy_job function of JobDeployer class now has a new argument runtime_secret_vars: Dict[str, str] with secret env vars for a job. See Supported hooks . ( #394 )","title":"Changed"},{"location":"docs/CHANGELOG/#2230-2023-12-04","text":"","title":"[2.23.0] - 2023-12-04"},{"location":"docs/CHANGELOG/#added_14","text":"A command racetrack get remote -q (with flag -q or --quiet ) prints only the current address of Lifecycle (without other logs), which makes it usable for scripts. Likewise, a command racetrack get pub -q (with flag -q or --quiet ) prints the current address of Pub service. \"Quiet\" mode is automatically applied when not in a TTY. Racetrack can be turned into a Maintenance mode. During that time, users are unable to make changes. Therefore, deploying, deleting and moving jobs is disabled. See how to enable maintenance mode ( #370 ) A job can be deployed through the Dashboard. To submit a job manifest, go to the \"Deployments\" tab and then \"Deploy a new job\" . Note that secret variables are not supported in this manner; instead, use the command line client. ( #306 ) A new tab called \"Deployments\" displays a list of recent deployment attempts. After selecting one, you may view deployment data such as status, failure cause, duration, and so on. In case of trouble with deployment, you can share this link with a support team.","title":"Added"},{"location":"docs/CHANGELOG/#fixed_12","text":"Fixed checking job condition in case of a missing Content-Type header of job's live endpoint. #376 Word wrapping applied to job logs in Dashboard. #372 SIGTERM signal triggers graceful shutdown. #379","title":"Fixed"},{"location":"docs/CHANGELOG/#2220-2023-11-07","text":"","title":"[2.22.0] - 2023-11-07"},{"location":"docs/CHANGELOG/#added_15","text":"Manifest values can be overriden with key-value pairs coming from a command line. It doesn't modify actual file, but its one-time, in-memory version before submitting it. Racetrack client has --extra-vars KEY=VALUE parameter (or -e in short) that overwrites values found in YAML manifest. KEY is the name of field and it can contain dots to refer to a nested field, for example git.branch=master . VALUE can be any YAML or JSON object. Extra vars parameters can be used multiple times in one command. Example: racetrack deploy -e secret_runtime_env_file=.env.local -e git.branch=$(git rev-parse --abbrev-ref HEAD) It makes CLI commands more script-friendly, so you can overwrite manifest without tracking changes in job.yaml file. Tip: Use racetrack validate command beforehand to make sure your final manifest is what you expected. ( #340 ) You can install Racetrack to a standalone host (e.g. EC2 host or fresh VM instance) using the installer script that runs it on the Docker Engine infrastructure. ( #308 ) Command racetrack get auth-token prints out current auth token. It can be used in CLI scripts: curl -H \"X-Racetrack-Auth: $(racetrack get auth-token)\" ( #357 ) Command racetrack login --username <username> allows you to log in with your username and password (entered into the standard input) and saves the auth token without having to visit the Dashboard page.","title":"Added"},{"location":"docs/CHANGELOG/#fixed_13","text":"Manifest is validated after updating it on Dashboard. Changing primary keys (name or value) is forbidden. ( #293 ) Fixed reviving a dead job after deleting it. ( #366 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2210-2023-10-16","text":"","title":"[2.21.0] - 2023-10-16"},{"location":"docs/CHANGELOG/#added_16","text":"Pub can now be turned into a remote gateway mode. It allows to distribute services between clusters as 2 (or more) separate infrastructures: main hub - hosting core Racetrack services: Lifecycle, image-builder, Dashboard, main Pub remote jobs - infrastructure hosting jobs and remote Pub gateway that protects jobs from unauthorized access. It can cooperate with the following infrastructure plugins: remote Kubernetes plugin remote docker plugin New administrative endpoint for cleaning up plugins mess. ( #331 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_14","text":"Streaming live logs of a job can now work with multiple infrastructures. Logs streamer interface has been redesigned so infrastructure plugins has to be updated accordingly. Prometheus, Grafana and PostgreSQL have been upgraded.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_14","text":"Database connection is checked in background to avoid blocking metrics endpoint. It reduced number of open database connections. ( #318 ) Faulty plugins can be deleted.","title":"Fixed"},{"location":"docs/CHANGELOG/#2200-2023-09-27","text":"","title":"[2.20.0] - 2023-09-27"},{"location":"docs/CHANGELOG/#added_17","text":"Lifecycle has more metrics and Grafana panels for monitoring: number of requests, websocket clients, server errors, open connections and active threads. Improved error handling and logging in Lifecycle.","title":"Added"},{"location":"docs/CHANGELOG/#fixed_15","text":"Fixed metrics endpoint, which was making Lifecycle unresponsive to liveness probes in case of database connection errors. ( #314 ) Fixed TypeError in racetrack client on Python 3.8 ( #320 )","title":"Fixed"},{"location":"docs/CHANGELOG/#removed_1","text":"Auto-Update feature has been disabled temporarily due to generating too many requests on multiple replicas.","title":"Removed"},{"location":"docs/CHANGELOG/#2190-2023-09-04","text":"","title":"[2.19.0] - 2023-09-04"},{"location":"docs/CHANGELOG/#added_18","text":"Plugin's manifest can declare its category, which makes the label to be displayed on the plugins list. Category is a kind of the plugin and can be either \"job-type\", \"infrastructure\" or \"core\". If you don't want the plugin to be loaded by Racetrack's Lifecycle or Image-builder (due to missing imports), you can enable it selectively for particular components. Declare components field in a plugin-manifest.yaml and add 'lifecycle' or 'image-builder' to the list. See Developing plugins . ( #291 ) Plugins can implement run_action method for calling a supplementary action with an endpoint. It can be used for debugging purposes or to extend plugin's functionality that is not covered by a plugin interface.","title":"Added"},{"location":"docs/CHANGELOG/#changed_15","text":"Installation to non-local cluster is done by utility script that automatically generates resources and unique passwords for your setup. ( #298 )","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_16","text":"Fixed racetrack run-local command. ( #312 ) Improved finalizing database connections. ( #295 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2180-2023-08-04","text":"","title":"[2.18.0] - 2023-08-04"},{"location":"docs/CHANGELOG/#added_19","text":"Job types can read the manifest file from the job's directory. It might be useful to configure the home page of a job ( see example ). Racetrack-client has new flag --replace available when uploading a plugin: racetrack plugin install <file.zip> --replace . It will delete the existing versions (with the same name) on uploading a new one. Accordingly, there is a checkbox on the Dashboard's Administration tab for replacing a plugin on upload. By default, uploading a new plugin doesn't replace the older versions. ( #270 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_16","text":"When a job is built from local files (with --build-context=local ), files matching .gitignore are excluded. ( #281 )","title":"Changed"},{"location":"docs/CHANGELOG/#2170-2023-07-17","text":"","title":"[2.17.0] - 2023-07-17"},{"location":"docs/CHANGELOG/#added_20","text":"When a job is running a deprecated job type version (due to being no longer available), there is a notice displayed on a dashboard - \"info\" icon in a jobs tree and a \"Notice\" field after selecting a job. ( #269 )","title":"Added"},{"location":"docs/CHANGELOG/#fixed_17","text":"Missing schema.json is now distributed in PyPI packages. ( #283 ) Corrected aggregated metrics on Grafana Dashboards. ( #273 ) Fixed reloading local modules by plugins. ( #275 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2161-2023-06-19","text":"","title":"[2.16.1] - 2023-06-19"},{"location":"docs/CHANGELOG/#changed_17","text":"Dashboard displays original YAML for the job's manifest. ( #262 ) Dashboard now renders a table on the portfolio tab with QTable rather than TableFilter, which occasionally seemed to have rendering issues. Plus, it's more consistent with the current UI styling. ( #258 )","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_18","text":"Dashboard redirects to login page when session expires. ( #266 )","title":"Fixed"},{"location":"docs/CHANGELOG/#2160-2023-06-05","text":"","title":"[2.16.0] - 2023-06-05"},{"location":"docs/CHANGELOG/#added_21","text":"Jobs tree on the Dashboard is refreshed in real time as soon as someone else's change is detected. You can turn it off on the \"Jobs\" page by clicking \"3 dots menu\", \"Auto-update\" toggle. ( #239 )","title":"Added"},{"location":"docs/CHANGELOG/#changed_18","text":"Chain calls to the jobs should be made by importing a dedicated function from a library provided by the job type plugin. This will keep the chain call function always up-to-date with the Racetrack version. See the example . ( #20 )","title":"Changed"},{"location":"docs/CHANGELOG/#2150-2023-05-26","text":"","title":"[2.15.0] - 2023-05-26"},{"location":"docs/CHANGELOG/#changed_19","text":"The name of the caller is recorded in the internal logs, giving the ability to track down who made the request based on its ID. Job types can also retrieve that information by extracting it from an HTTP header. See the python-job-type docs and a job for example. ( #246 ) racetrack-client shows more details in case of an HTTP error. ( #245 ) golang , python , and wrapper_properties fields are deprecated in Manifest schema, use jobtype_extra instead. This is backwards compatible. ( #231 ) Editing the manifest online triggers a redeployment of the job, keeping manifest up-to-date with running job. ( #250 )","title":"Changed"},{"location":"docs/CHANGELOG/#2140-2023-05-18","text":"","title":"[2.14.0] - 2023-05-18"},{"location":"docs/CHANGELOG/#added_22","text":"Dashboard links to job-related Grafana dashobards. issue #206 Manifest of a job can be edited online on the Dashboard after selecting a job. Keep in mind that your online changes can be overwritten by the next deployment, if you didn't reflect these temporary changes in git after all. issue #217","title":"Added"},{"location":"docs/CHANGELOG/#changed_20","text":"Minor UI improvements and styling for Dashboard, including: Sorting jobs by status (starting from faulty ones) Jobs ordering is persisted in local storage Spinner indicators during loading the data Status indicator (green/red/yellow) and number of jobs on a jobs tree Panel takes up the whole space available - no white bars on the sides Jobs tree has its own scrollbar \"10 seconds ago\" labels are refreshed over time.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_19","text":"Tree of jobs is refreshed after deleting a job. issue #211","title":"Fixed"},{"location":"docs/CHANGELOG/#2130-2023-05-10","text":"","title":"[2.13.0] - 2023-05-10"},{"location":"docs/CHANGELOG/#added_23","text":"\"Jobs\" tab now shows the tree of jobs grouped by the family name. Jobs tree can be filtered by name, version, owner, job type version or infrastructure target. issue #212","title":"Added"},{"location":"docs/CHANGELOG/#changed_21","text":"Dashboard UI has been revamped and turned into Single Page Application using modern front-end frameworks, which made it more smooth and responsive. issue #212","title":"Changed"},{"location":"docs/CHANGELOG/#2121-2023-04-21","text":"","title":"[2.12.1] - 2023-04-21"},{"location":"docs/CHANGELOG/#changed_22","text":"Racetrack client returns non-zero exit code in case of error. issue #224 Dashboard is more responsive in case of a database malfunction (shows error rather than hanging indefinitely), since Postgres is no longer its hard dependency.","title":"Changed"},{"location":"docs/CHANGELOG/#2120-2023-03-29","text":"","title":"[2.12.0] - 2023-03-29"},{"location":"docs/CHANGELOG/#added_24","text":"New column in the list command of the Racetrack client displays a job type. Try it out by running racetrack list -c job_type . issue #207 New command racetrack call NAME ENDPOINT PAYLOAD [--version VERSION] [--remote REMOTE] [--curl] allows you to call an endpoint of a job. Provide the name of the job and the payload of the request in JSON or YAML format, for example racetrack call adder /api/v1/perform '{\"numbers\": [40,2]}' . Use --curl flag, if you want to generate a curl query instead of calling the job. Check out racetrack call --help for more details. issue #146 Name of the job can be autocompleted by hitting Tab while typing a CLI command. Remember to run racetrack --install-completion beforehand. Under the hood, it fetches the available jobs from the current remote.","title":"Added"},{"location":"docs/CHANGELOG/#changed_23","text":"The list command of the Racetrack client drops the fancy formatting and INFO / DEBUG logs when being piped into another command (ie. not connected to a terminal/tty device). Try racetrack list | cat . issue #207","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_20","text":"Performance of PUB component has been improved by reducing number of requests made to the Lifecycle and to the database. issue #155 Fixed saving plugin's config. issue #218 Racetrack's cookie can work even on non-HTTPS deployments. issue #225","title":"Fixed"},{"location":"docs/CHANGELOG/#2110-2023-03-17","text":"","title":"[2.11.0] - 2023-03-17"},{"location":"docs/CHANGELOG/#added_25","text":"When specifying a job type version, wildcards can be used. For instance, mllab:1.3.*-3.9-bullseye to subscribe for the latest patches or mllab:1.*.*-3.9-bullseye if you feel more adventurous. The job could be upgraded without committing to manifest then. It's the extension of the existing latest tag, but it now supports multiple job type variants. Note: The release of a new job type version has no effect on existing jobs until they are redeployed. issue #183","title":"Added"},{"location":"docs/CHANGELOG/#changed_24","text":"Portfolio tab on Dashboard shows the exact version of a job type, even if its manifest specifies a wildcard version or \"latest\". issue #203","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_21","text":"Fixed opening a job after regenerating the token. issue #200 Fixed CSRF Trusted Origins protection. issue #197","title":"Fixed"},{"location":"docs/CHANGELOG/#2101-2023-03-10","text":"","title":"[2.10.1] - 2023-03-10"},{"location":"docs/CHANGELOG/#added_26","text":"Grafana's \"Lifecycle\" dashboard now includes a new panel that tracks the number of jobs with the particular status.","title":"Added"},{"location":"docs/CHANGELOG/#changed_25","text":"\"Orhpaned\" jobs are no longer adopted by Racetrack. This was causing spurious \"Lost\" status after all. issue #163 , issue #134 Third-party dependencies have been upgraded.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_22","text":"Regenerating a user token via the button on your profile now no longer requires a refresh. issue #188","title":"Fixed"},{"location":"docs/CHANGELOG/#2100-2023-03-03","text":"","title":"[2.10.0] - 2023-03-03"},{"location":"docs/CHANGELOG/#added_27","text":"Deletion pop-up contains the name of the job to confirm that you're about to delete the right one. issue #176 Button added to the profile page that lets you regenerate your authentication token. issue #139","title":"Added"},{"location":"docs/CHANGELOG/#changed_26","text":"Racetrack client keeps the local config file ~/.racetrack/config.yaml with mode 600 (not readable by others) and warns you if this file has insecure permissions since it may contain credentials. issue #179","title":"Changed"},{"location":"docs/CHANGELOG/#291-2023-02-20","text":"","title":"[2.9.1] - 2023-02-20"},{"location":"docs/CHANGELOG/#changed_27","text":"Fatman has been renamed to Job in all database tables, columns and values. For instance, scope call_fatman is now call_job . issue #162","title":"Changed"},{"location":"docs/CHANGELOG/#290-2023-02-14","text":"","title":"[2.9.0] - 2023-02-14"},{"location":"docs/CHANGELOG/#added_28","text":"Deployment phases, shown by a racetrack client, are more granular, including image building steps. issue #145 Racetrack comes with standalone Prometheus and Grafana. It contains dedicated dashboards for monitoring the jobs and internal services. issue #144 Database connection is being monitored closely by Lifecycle and displayed on a Grafana dashboard. issue #165","title":"Added"},{"location":"docs/CHANGELOG/#changed_28","text":"Fatman is renamed to Job . fatman.yaml is thus deprecated in favour of job.yaml file name. Although the endpoint for calling the Job has changed to /pub/job/NAME/VERSION/PATH , the old endpoint /pub/fatman/NAME/VERSION/PATH is still supported for backward compatibility. issue #136","title":"Changed"},{"location":"docs/CHANGELOG/#281-2023-01-27","text":"","title":"[2.8.1] - 2023-01-27"},{"location":"docs/CHANGELOG/#changed_29","text":"Set User-agent header in all CLI requests to circumvent silly Cloudflare's protection.","title":"Changed"},{"location":"docs/CHANGELOG/#280-2023-01-26","text":"","title":"[2.8.0] - 2023-01-26"},{"location":"docs/CHANGELOG/#added_29","text":"A job can be moved from one infrastructure target to another by using a new CLI command. Check out racetrack move --help . A new command racetrack list allows to fetch the list of all deployed fatmen with the selected columns. See racetrack list --help for more details.","title":"Added"},{"location":"docs/CHANGELOG/#changed_30","text":"Syntax of Racetrack client has been rearranged. Check out racetrack --help for more details. Notable changes: Racetrack URL is now called \"remote\" and it can be explicitly set with an optional parameter --remote alias in most of the commands. \"Remote\" can be a direct URL or an alias. The current remote can be set once with racetrack set remote ALIAS_OR_URL and then you can omit --remote parameter in the next commands like racetrack list . Check your current remote with racetrack get remote . Although racetrack deploy [WORKDIR] [REMOTE] syntax is still supported, it's deprecated now and racetrack deploy [WORKDIR] [--remote REMOTE] should be used instead. Automatic completion can be activated by running racetrack --install-completion . Then, you'll see relevant prompts after hitting Tab . To show the runtime logs, use racetrack logs NAME [--version VERSION] [--remote REMOTE] . No need to pass workdir with a manifest file any longer. To show the build logs, use racetrack build-logs NAME [--version VERSION] [--remote REMOTE] . No need to pass workdir with a manifest file any longer. To delete a job, use new syntax: racetrack delete NAME --version VERSION [--remote REMOTE] - job name and version is now required. No need to pass workdir with a manifest file any longer. racetrack plugin install PLUGIN_URI [--remote REMOTE] - \"remote\" is now an optional parameter instead of a required argument. To set up an alias for Racetrack's remote URL, use racetrack set alias ALIAS RACETRACK_URL (former racetrack config alias set ). To set read-access credentials for a git repository, use racetrack set credentials REPO_URL USERNAME TOKEN_PASSWORD (former racetrack config credentials set ). To save user's Racetrack Auth Token for Racetrack server, use racetrack login USER_TOKEN [--remote REMOTE] (former racetrack login RACETRACK_URL USER_TOKEN ). racetrack logout [--remote REMOTE] (former racetrack logout RACETRACK_URL ). In case of problems with reaching the Job, PUB shows the error page (JSON with the meaningful reason), instead of a white page of death. The \"Open\" button gets deactivated on the Dashboard for jobs that are not Running .","title":"Changed"},{"location":"docs/CHANGELOG/#270-2023-01-05","text":"","title":"[2.7.0] - 2023-01-05"},{"location":"docs/CHANGELOG/#added_30","text":"Every plugin can have its own configuration, stored in a YAML file. After uploading the plugin, the configuration can be edited on Dashboard's Administration tab, with a Edit Config button. Plugin can read the configuration from a file self.config_path: pathlib.Path . Racetrack instance can operate with multiple infrastructure targets. By default, there is no infrastructure target. It has to be extended by installing a plugin. The infrastructure target is displayed next to the fatman name, on Dashboard's list. There is a new, optional field infrastructure_target in a Manifest. If given, it enforces using the specific target in case of many infrastructures are in use. Otherwise, the deafult infrastructure is applied. Fatman can be composed of multiple Docker images provided by a plugin or user's manifest.","title":"Added"},{"location":"docs/CHANGELOG/#260-2022-12-02","text":"","title":"[2.6.0] - 2022-12-02"},{"location":"docs/CHANGELOG/#added_31","text":"\"Portfolio\" table in a Dashboard has a new column \"Job type version\". List of available job types can be checked with: racetrack plugin list --job-types <racetrack_url> . It is also listed in the Administration tab on Dashboard . Exact job type version can be checked at Fatman's /health endpoint.","title":"Added"},{"location":"docs/CHANGELOG/#fixed_23","text":"When uploading a faulty plugin, the errors are handled in a more reliable manner. Plugin's requirements are installed with non-root user context. Plugin directory is accessible when initializing the plugin in __init__ method. Fixed conflict between newer protobuf version and opentelemetry package.","title":"Fixed"},{"location":"docs/CHANGELOG/#251-2022-10-28","text":"","title":"[2.5.1] - 2022-10-28"},{"location":"docs/CHANGELOG/#added_32","text":"Users can change their password on a Dashboard -> Profile tab -> Change Password Fatman webviews can now serve ASGI applications (like FastAPI)","title":"Added"},{"location":"docs/CHANGELOG/#fixed_24","text":"Added missing __init__.py file in one of the racetrack-client modules.","title":"Fixed"},{"location":"docs/CHANGELOG/#250-2022-10-18","text":"","title":"[2.5.0] - 2022-10-18"},{"location":"docs/CHANGELOG/#changed_31","text":"One job type can be installed in multiple versions at the same time. Users have to pick one of these versions and specify it in the manifest of their fatman, eg. jobtype: python3:2.4.0 . Version of a job type is now required in the jobtype field of Manifest in a manner name:version . latest version can be used (resolving to the highest semantic version), though it's discouraged. Plugin bundler has been moved to racetrack-client. Now you can do racetrack plugin bundle instead of racetrack-plugin-bundler bundle .","title":"Changed"},{"location":"docs/CHANGELOG/#240-2022-10-11","text":"","title":"[2.4.0] - 2022-10-11"},{"location":"docs/CHANGELOG/#added_33","text":"Plugins can be installed by means of racetrack-client command racetrack plugin install <plugin_uri> <racetrack_url> , where plugin_uri can be either: local file path (eg. python3-job-type-2.4.0.zip ), URL to a remote HTTP file (eg. https://github.com/TheRacetrack/plugin/releases/download/2.4.0/python3-job-type-2.4.0.zip ), GitHub repository name (eg. github.com/TheRacetrack/plugin-python-job-type ) - it takes the ZIP file from the latest release. GitHub repository name with version (eg. github.com/TheRacetrack/plugin-python-job-type==2.4.0 ) - it takes the ZIP file from the specific release. Plugins can be uninstalled with a new command: racetrack plugin uninstall <plugin_name> <plugin_version> <racetrack_url> . List of currently installed plugins can be checked with: racetrack plugin list <racetrack_url> .","title":"Added"},{"location":"docs/CHANGELOG/#changed_32","text":"All job types are individual plugins now. Racetrack starts without any job type by default. Base Fatman images are built inside Racetrack by image-builder so it's no longer needed to push images prior to the plugin release.","title":"Changed"},{"location":"docs/CHANGELOG/#230-2022-09-23","text":"","title":"[2.3.0] - 2022-09-23"},{"location":"docs/CHANGELOG/#added_34","text":"Fatman access can be narrowed down to single endpoints. When adding Auth Resource Permission (in Admin panel), there is a new endpoint field, which narrows down the permission only to this particular Fatman's endpoint. If not set, it covers all endpoints (just as other resource filters do). For instance, ESC can have a permission with call_fatman scope to only call one endpoint. OpenTelemetry can be turned on in Racetrack. Given the OTLP collector endpoint, the traces from lifecycle, pub and fatman will be sent there.","title":"Added"},{"location":"docs/CHANGELOG/#changed_33","text":"python3 job type is now based on 3.9-slim-bullseye image instead of 3.8-slim-buster . Plugins are distributed as ZIP files. They can be installed and uninstalled in a Dashboard's Administration page. See using-plugins.md .","title":"Changed"},{"location":"docs/CHANGELOG/#221-2022-08-25","text":"","title":"[2.2.1] - 2022-08-25"},{"location":"docs/CHANGELOG/#changed_34","text":"Maximum amount of memory for a fatman is reduced to 1GiB by default (if not set explicitly in a manifest). In general the maximum memory can't be more than 4 times higher than the minimum memory. Racetrack keeps an eye on this rule by automatically adjusting minimum memory amount, if needed. It is recommended to declare maximum memory amount explicitly in a manifest by defining resources.memory_max field. Fatman versions in PUB's metrics are always resolved to the exact version instead of wildcards.","title":"Changed"},{"location":"docs/CHANGELOG/#220-2022-08-16","text":"","title":"[2.2.0] - 2022-08-16"},{"location":"docs/CHANGELOG/#added_35","text":"Golang job types serve interactive Swagger UI with API documentation on the home page.","title":"Added"},{"location":"docs/CHANGELOG/#changed_35","text":"Function fatman_job_types of plugins changed its signature. Now the first parameter is a Docker Registry prefix to work well with different Docker registry namespaces. All job-type plugins has to be adjusted accordingly. See plugins-job-types.md for more details. A namespace for docker images in a Docker Registry is configurable so that Racetrack can be released to any Docker Registry. A namespace for fatman workloads running on Kubernetes is now configurable by means of FATMAN_K8S_NAMESPACE environment variable, by default it's set to racetrack . Hostname subdomain of Racetrack services is now configurable by means of RACETRACK_SUBDOMAIN environment variable, by default it's set to racetrack . Golang job type has been moved to Plugin - Go Job Type . Now it's not enabled by default in Racetrack.","title":"Changed"},{"location":"docs/CHANGELOG/#212-2022-07-29","text":"","title":"[2.1.2] - 2022-07-29"},{"location":"docs/CHANGELOG/#changed_36","text":"Overall requests performance has been improved by switching from Flask & twisted (WSGI) to FastAPI & Uvicorn (ASGI) web server. Fatman server should now be less laggy under heavy load. Additionaly, FastAPI comes with better request validation and newer SwaggerUI page. Fatman redeployment is needed for the changes to take effect.","title":"Changed"},{"location":"docs/CHANGELOG/#211-2022-07-25","text":"","title":"[2.1.1] - 2022-07-25"},{"location":"docs/CHANGELOG/#added_36","text":"Plugins can provide its own Fatman Deployers to deploy workloads to other types of platforms. See developing-plugins.md and Docker Daemon Deployer Plugin","title":"Added"},{"location":"docs/CHANGELOG/#changed_37","text":"Fatman's metrics now contain number of requests and duration for each endpoint (including auxiliary endpoints).","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_25","text":"Fixed redeploying fatmen from the dashboard Making multiple attempts to deploy the same fatman at the same time is now disallowed. \"In progress\" deployments are halted when Lifecycle restarts (due to upgrade or eviction)","title":"Fixed"},{"location":"docs/CHANGELOG/#210-2022-07-08","text":"","title":"[2.1.0] - 2022-07-08"},{"location":"docs/CHANGELOG/#changed_38","text":"Fatman secrets (git credentials and secret vars) are kept in Racetrack, so fatmen are fully reproducible again. In other words, it is technically possible to rebuild and reprovision fatman by anyone who is allowed to do so (anyone with permissions, not just the author) to apply latest Racetrack changes. You can click \"Rebuild and provision\" to do a complete redeployment of a fatman, or just \"Reprovision\" to apply changes without rebuilding the image (eg. useful when changing replicas count). Racetrack client no longer depends on requests package in favour of using the built-in urllib module.","title":"Changed"},{"location":"docs/CHANGELOG/#200-2022-06-30","text":"","title":"[2.0.0] - 2022-06-30"},{"location":"docs/CHANGELOG/#added_37","text":"Plugins can hook into the post_fatman_delete event to add their actions after deleting a fatman. See Teams Notifications Racetrack Plugin Response errors are also displayed in the logs in addition to returning message in HTTP payload. Racetrack client has new command racetrack run-local . <racetrack_url> allowing to run fatman locally based on docker image built by remote Racetrack. Docker has to be installed on your system.","title":"Added"},{"location":"docs/CHANGELOG/#changed_39","text":"Fatman permissions model is more restrictive. Now users, ESCs and fatman families can be granted fine-grained permissions. Users can have access to a subset of fatmen and they can see and manage only those that are allowed for them. Old permissions have been migrated (Fatman-to-Fatman and ESC-to-fatman access). By default, Users can now do the following: read all fatman status (browse on a dashboard) call endpoints of every fatman deploy and redeploy fatmen delete only fatmen that has been deployed by the user (starting from the newly deployed ones) Admin can grant or revoke permissions. Permisssion can cover either all fatmen, whole fatman family or a single fatman. Permission is related to one of the operation type (called \"scope\"): read_fatman - list fatman, check fatman details deploy_fatman - deploy fatman in a particular family, redeploy fatman deploy_new_family - deploy new fatman family delete_fatman - move to trash, dismantle from a cluster call_fatman - call fatman endpoints call_admin_api - not important for regular users full_access - not important for regular users Authentication tokens have been revamped and format changed. Users are required to use new tokens in order to use CLI racetrack client. Log in to Dashboard, copy the token and run racetrack login again. Old ESC tokens are still supported to keep backwards compatibility, but they should be regenerated if possible. A method for calling fatman from inside of other fatman has been changed due to new auth model. See python-chain sample . All fatmen calling another fatman are required to be redeployed (to use new tokens). Since there is one type of tokens, Auth header has been unified and renamed to X-Racetrack-Auth . It should be used no matter if it's ESC, User or a Fatman. Old headers are still supported but may be abandoned in the future: X-Racetrack-User-Auth , X-Racetrack-Esc-Auth , X-Racetrack-Fatman-Auth Racetrack services has been adjusted to the new URL format. /ikp-rt prefix has been removed from all URLs. Instead, ikp-rt part may be included in the cluster hostname, making session cookies to work hassle-free and more secure. For instance, Racetrack Dashboard address is now: https://ikp-rt.<cluster.address>/dashboard . Lifecycle address is: https://ikp-rt.<cluster.address>/lifecycle . PUB runs on: https://ikp-rt.<cluster.address>/pub . Thus, Fatman instances can be accessed at: https://ikp-rt.<cluster.address>/pub/fatman/<name>/<version>/<path> .","title":"Changed"},{"location":"docs/CHANGELOG/#120-2022-05-31","text":"","title":"[1.2.0] - 2022-05-31"},{"location":"docs/CHANGELOG/#added_38","text":"fatman.yaml manifest file supports overlays. Including extends field (with a path to base manifest) will make it to merge base and overlay layers. It may come in handy when working with multiple environments (dev/test/prod) referring to the same base manifest. See overlays sample . Job types can be extended with plugins. Check out sample job-type plugin and developing-plugins.md to see a guide how to create and activate plugins. Racetrack documentation is now served at \"Docs\" tab in Dashboard. Check out \"Home\" page to get started. Plugins can extend Racetrack documentation with its own pages written in Markdown (it may be generated at runtime depending on plugin configuration).","title":"Added"},{"location":"docs/CHANGELOG/#changed_40","text":"racetrack validate PATH now evaluates the manifest outcome and prints it to verify the result of extending overlay manifest. Critical actions like deleting fatman or redeploying it open a modal window with confirmation button to prevent accidental removal. Racetrack environments are easier distinguishable between dev, test, preprod and prod due to different colors of the dashboard navbar and additional prefix [prod] in the title.","title":"Changed"},{"location":"docs/CHANGELOG/#110-2022-04-21","text":"","title":"[1.1.0] - 2022-04-21"},{"location":"docs/CHANGELOG/#added_39","text":"New \"Audit Log\" tab in Dashboard shows activity events done by users, eg. \"fatman F1 of user Alice has been deleted by Bob\". It can be filtered by events related to a logged user, whole fatman family or a particular fatman.","title":"Added"},{"location":"docs/CHANGELOG/#changed_41","text":"Once a fatman name (and version) has been claimed, it can never be deployed again to prevent accidental/hostile reusing. Login page has been revamped.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_26","text":"Display error cause on dashboard in case of \"Redeploy\" failure.","title":"Fixed"},{"location":"docs/CHANGELOG/#102-2022-04-06","text":"","title":"[1.0.2] - 2022-04-06"},{"location":"docs/CHANGELOG/#changed_42","text":"Set default fatman maximum memory to 8GiB","title":"Changed"},{"location":"docs/CHANGELOG/#101-2022-04-06","text":"","title":"[1.0.1] - 2022-04-06"},{"location":"docs/CHANGELOG/#changed_43","text":"Fatman dependency graph is interactive, nodes are movable. Clicking on a node filters out all its neighbours to see which fatmen can be accessed by particular ESC or which ESCs have access to a particular fatman.","title":"Changed"},{"location":"docs/CHANGELOG/#changed_44","text":"Set default fatman minimum memory to 150MiB","title":"Changed"},{"location":"docs/CHANGELOG/#100-2022-03-29","text":"","title":"[1.0.0] - 2022-03-29"},{"location":"docs/CHANGELOG/#added_40","text":"Fatman manifest can declare resources to be allocated to the Fatman with resources field, including minimum/maximum memory, minimum/maximum CPU cores. See Fatman Manifest File Dashboard shows datetime when the last call was made to a fatman. Fatman redeployment is needed for the changes to take effect. Dashboard has a new \"Portfolio\" tab for browsing fatmen freely with custom criteria and showing the candidates for removal.","title":"Added"},{"location":"docs/CHANGELOG/#078-2022-03-18","text":"","title":"[0.7.8] - 2022-03-18"},{"location":"docs/CHANGELOG/#077-2022-03-18","text":"","title":"[0.7.7] - 2022-03-18"},{"location":"docs/CHANGELOG/#changed_45","text":"Racetrack client validates given token when running racetrack login and complains immediately if it's wrong Fatman returns 400 BAD REQUEST response in case of missing argument, unexpected argument or any other TypeError.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_27","text":"Fixed Fatmen getting Lost. Fatmen shouldn't wrongly change its status to \"Lost\" any longer.","title":"Fixed"},{"location":"docs/CHANGELOG/#076-2022-03-04","text":"","title":"[0.7.6] - 2022-03-04"},{"location":"docs/CHANGELOG/#075-2022-03-01","text":"","title":"[0.7.5] - 2022-03-01"},{"location":"docs/CHANGELOG/#added_41","text":"Log messages contain Request Tracing ID, which allows to track down what was happening during the request processing by different services. PUB can read tracing ID from a specific HTTP request header. If not given, it will be generated. On IKP clusters this header is named traceparent (by default it's X-Request-Tracing-Id ). ID is also returned in the response headers, so you can use it when reporting bugs. Fatman redeployment is needed for the changes to take effect.","title":"Added"},{"location":"docs/CHANGELOG/#074-2022-03-01","text":"","title":"[0.7.4] - 2022-03-01"},{"location":"docs/CHANGELOG/#added_42","text":"PUB collects histogram of fatman response times and metrics related to endless requests and response codes.","title":"Added"},{"location":"docs/CHANGELOG/#073-2022-02-28","text":"","title":"[0.7.3] - 2022-02-28"},{"location":"docs/CHANGELOG/#changed_46","text":"GET /metrics 200 requests are hidden from access logs. Reading from stdin is prohibited at fatman initialization and it raises an error once detected, showing stack trace with the place where there was an attempt. Fatman doesn't go unresponsive any longer, awaiting input indefinitely.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_28","text":"Fatman logs doesn't show misleading \"Timing out client\" errors anymore. It was related to HTTP sessions kept alive by browsers, not actual fatman request time-outs.","title":"Fixed"},{"location":"docs/CHANGELOG/#072-2022-02-14","text":"","title":"[0.7.2] - 2022-02-14"},{"location":"docs/CHANGELOG/#changed_47","text":"Access logs are not polluted with redundant /live , /ready & /health requests.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_29","text":"Racetrack logs show non-ASCII characters","title":"Fixed"},{"location":"docs/CHANGELOG/#071-2022-02-07","text":"","title":"[0.7.1] - 2022-02-07"},{"location":"docs/CHANGELOG/#added_43","text":"Racetrack dashboard shows active plugins along with their versions.","title":"Added"},{"location":"docs/CHANGELOG/#changed_48","text":"Fatman manifest is checked against using HTTPS git remote only (SSH is not supported). None values metrics are allowed. They're ignored instead of logging error as there is no null in Prometheus, only samples that don't exist. Fatman access logs have shorter, concise format. Time, HTTP method, path and response code are the only ones displayed.","title":"Changed"},{"location":"docs/CHANGELOG/#070-2022-01-31","text":"","title":"[0.7.0] - 2022-01-31"},{"location":"docs/CHANGELOG/#added_44","text":"When accessing a Fatman through a PUB, there can be used wildcard version format: 1.x or 1.2.x . This points to a highest stable version of the Fatman that matches the pattern. For instance, 1.x keeps track of all v1 updates, but skips next major versions 2.0.0 and above. Fatman redeployment is needed for the changes to take effect.","title":"Added"},{"location":"docs/CHANGELOG/#changed_49","text":"\"latest\" fatman version now points to the highest version disregarding versions with any label (eg. -dev ). Thus deploying 2.0.0-alpha pre-release versions doesn't affect \"latest\" alias until 2.0.0 is deployed.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_30","text":"Public endpoints are properly handled with version aliases (eg. \"latest\", \"1.x\")","title":"Fixed"},{"location":"docs/CHANGELOG/#061-2022-01-24","text":"","title":"[0.6.1] - 2022-01-24"},{"location":"docs/CHANGELOG/#added_45","text":"Phases of deployment are tracked by showing \"building image\", \"creating cluster resources\", \"initializing Fatman entrypoint\", etc. in RT client in place of bare \"deployment in progress...\" message.","title":"Added"},{"location":"docs/CHANGELOG/#changed_50","text":"Fatman versions are validated according to SemVer format . Version should match MAJOR.MINOR.PATCH[LABEL] , eg.: 0.1.2 or 0.0.0-dev .","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_31","text":"Zero-value metrics are shown properly at /metrics endpoint of a Fatman.","title":"Fixed"},{"location":"docs/CHANGELOG/#060-2022-01-17","text":"","title":"[0.6.0] - 2022-01-17"},{"location":"docs/CHANGELOG/#added_46","text":"Racetrack functionality can be extended with plugins. Check out developing-plugins.md to see a guide how to create and activate plugins.","title":"Added"},{"location":"docs/CHANGELOG/#changed_51","text":"Whole directories are handled in static_endpoints to serve recursively all of its content. Metrics from Postgres database and PUB are exported. It gives better monitoring of Fatman requests performance. Registration form has been revamped to show errors in a more user-friendly way. Pods resource requests have been adjusted to its actual needs. Cluster resource shortages shouldn't happen so prematurely anymore.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_32","text":"Show real reason of internal exceptions, eg. Neo4j auth error","title":"Fixed"},{"location":"docs/CHANGELOG/#050-2022-01-10","text":"","title":"[0.5.0] - 2022-01-10"},{"location":"docs/CHANGELOG/#changed_52","text":"Overwriting fatmen is disabled. It's not allowed to deploy a fatman with the same name & version as an existing one, unless it's deployed with --force flag. Duplicated env vars error is now more meaningful: \"found illegal runtime env vars, which conflict with reserved names: RK_HOST\"","title":"Changed"},{"location":"docs/CHANGELOG/#041-2022-01-05","text":"","title":"[0.4.1] - 2022-01-05"},{"location":"docs/CHANGELOG/#fixed_33","text":"Fixed Manifests getting empty on Racetrack upgrade.","title":"Fixed"},{"location":"docs/CHANGELOG/#040-2022-01-03","text":"","title":"[0.4.0] - 2022-01-03"},{"location":"docs/CHANGELOG/#added_47","text":"Number of running instances of the Fatman can be configured with replicas field in a manifest.yaml. In case of multiple replicas, racetrack logs collects logs from all instances at once. Deployed fatmen can be deleted through racetrack client using new command: racetrack delete <workdir> <racetrack_url> [--version X.Y.Z] .","title":"Added"},{"location":"docs/CHANGELOG/#035-2021-12-09","text":"","title":"[0.3.5] - 2021-12-09"},{"location":"docs/CHANGELOG/#added_48","text":"Fatman endpoints that are supposed to be accessed publicly (without authentication) can be declared in public_endpoints field of manifest. This makes a request that needs to be approved by Racetrack admin. See python-ui-flask sample .","title":"Added"},{"location":"docs/CHANGELOG/#fixed_34","text":"Improved overall Fatman response time.","title":"Fixed"},{"location":"docs/CHANGELOG/#034-2021-12-06","text":"","title":"[0.3.4] - 2021-12-06"},{"location":"docs/CHANGELOG/#changed_53","text":"Fatman's /live , /ready and /health endpoints doesn't require authentication.","title":"Changed"},{"location":"docs/CHANGELOG/#033-2021-11-22","text":"","title":"[0.3.3] - 2021-11-22"},{"location":"docs/CHANGELOG/#changed_54","text":"Fatman \"dashboard\" renamed to \"webview\". See Python Job Type docs: Custom Webview UI. On local dev Racetrack there's no need to commit every job change prior to deployment (it gets the working directory snapshot).","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_35","text":"Default admin account is no longer recreated.","title":"Fixed"},{"location":"docs/CHANGELOG/#031-2021-11-15","text":"","title":"[0.3.1] - 2021-11-15"},{"location":"docs/CHANGELOG/#added_49","text":"New optional field in Manifest: labels - dictionary with metadata describing fatman for humans. Labels are shown on a dashboard page besides fatman name. Example input data (shown on Swagger UI) can be defined for auxiliary endpoints by implementing docs_input_examples . See Python Job Type docs: Auxiliary endpoints section and python-auxiliary-endpoints sample .","title":"Added"},{"location":"docs/CHANGELOG/#fixed_36","text":"RT specific Dashboard cookie is removed upon logout Error returned by Fatman contains more information (including Lifecycle response) and uses correct status code (instead of 500) Fixed showing spurious success of Fatman redeployment (ensure new Fatman responds to health probes).","title":"Fixed"},{"location":"docs/CHANGELOG/#030-2021-11-08","text":"","title":"[0.3.0] - 2021-11-08"},{"location":"docs/CHANGELOG/#added_50","text":"racetrack logs command follows Fatman logs when called along with --follow flag.","title":"Added"},{"location":"docs/CHANGELOG/#changed_55","text":"Lifecycle API is fully authenticated, every request to Fatman needs to have either User, ESC or Fatman token. X-Racetrack-Caller header has changed to X-Racetrack-Esc-Auth","title":"Changed"},{"location":"docs/CHANGELOG/#021-2021-10-25","text":"","title":"[0.2.1] - 2021-10-25"},{"location":"docs/CHANGELOG/#added_51","text":"Multiple versions of the same fatman are allowed. Obsolete versions are not removed automatically. ESC permissions are assigned to whole fatmen family by its name. Build logs are always captured and can be seen on Racetrack Dashboard by clicking \"Logs\" / \"Build logs\" or by issuing racetrack build-logs <workdir> <racetrack_url> . Fatman logs are displayed on Dashboard under option \"Logs\" / \"Runtime logs\".","title":"Added"},{"location":"docs/CHANGELOG/#changed_56","text":"Fatman base URL is changed to /ikp-rt/pub/fatman/<fatman-name>/<version>/ . Version can be e.g. 0.0.1 or latest .","title":"Changed"},{"location":"docs/CHANGELOG/#020-2021-10-11","text":"","title":"[0.2.0] - 2021-10-11"},{"location":"docs/CHANGELOG/#added_52","text":"Environment vars can be configured in manifest (including build time env, runtime env and secrets). Therefore, pip dependencies can be installed from a private repository. See Python Job Type docs: Environment variables section. Swagger UI allows to set X-Racetrack-Caller header for making authorized Fatman calls. Dashboard profile shows a user token Racetrack client deploy and logs commands enforces logging with token racetrack login and logout commands racetrack config show for viewing racetrack config","title":"Added"},{"location":"docs/CHANGELOG/#changed_57","text":"docs_input_example outcome is validated at deployment stage. It's checked against JSON-serializability and its size should not exceed 1 MB. This solves unresponsive Swagger UI pages, but it may enforce to use a brief example at the expense of having representative one.","title":"Changed"},{"location":"docs/CHANGELOG/#011-2021-10-04","text":"","title":"[0.1.1] - 2021-10-04"},{"location":"docs/CHANGELOG/#added_53","text":"Customized Prometheus metrics can be exported by implementing metrics method.","title":"Added"},{"location":"docs/CHANGELOG/#changed_58","text":"Dashboard allows users to register, to be able to login and view list of Fatmen.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_37","text":"Generating graph on RT Dashboard Generating token for ESC in RT admin panel","title":"Fixed"},{"location":"docs/CHANGELOG/#010-2021-09-27","text":"","title":"[0.1.0] - 2021-09-27"},{"location":"docs/CHANGELOG/#changed_59","text":"Hide \"Open\" button for erroneous Fatman on Dashboard page. Instead, show a section containing error message and logs. Racetrack client infers full lifecycle URL, namely https protocol and /ikp-rt/lifecycle path, if not given. For instance, ikp-dev-cluster.example.com is a valid URL to deploy fatmen. Admin panel not available on Dashboard (moved to Lifecycle).","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_38","text":"Use DNS hostname of a cluster instead of raw IP address when redirecting to a Fatman URL (fixed in dashboard \"Open\" link as well as in racetrack deploy output).","title":"Fixed"},{"location":"docs/CHANGELOG/#0017-2021-09-20","text":"","title":"[0.0.17] - 2021-09-20"},{"location":"docs/CHANGELOG/#added_54","text":"Fatman serves 3 version numbers at /health endpoint: git_version - arising from the git history of a job source code, fatman_version - taken from version field in manifest (this is also displayed at SwaggerUI page). deployed_by_racetrack_version - version of the Racetrack the Fatman was deployed with. Dashboard displays current Racetrack version in the footer. Static endpoints for serving local files at particular path (eg: /xrai endpoint serving xrai.yaml ) Authorization to PUB (optional right now) for accessing Fatman /perform . Requires creating ESC and distributing its token to user.","title":"Added"},{"location":"docs/CHANGELOG/#fixed_39","text":"Show localized Dashboard dates on Firefox","title":"Fixed"},{"location":"docs/CHANGELOG/#0016-2021-09-13","text":"","title":"[0.0.16] - 2021-09-13"},{"location":"docs/CHANGELOG/#added_55","text":"build-essential package is incorporated into python3 base image. gcc and g++ packages are there out-of-the-box, so there is no need to include them manually to your system_dependencies any longer. Dashboard page shows both dates for each Fatman: \"Created at\" and \"Last updated\" (deployed at) along with its age (e.g. \"5 minutes ago\", \"5 days ago\").","title":"Added"},{"location":"docs/CHANGELOG/#changed_60","text":"In case of python3 initialization error, full traceback coming from fatman is displayed to a user (eg: it's easier to find out where is a bad import). Deployment errors displayed by CLI client are shortened and yet more meaningful. Client's traceback is not displayed if not needed. Datetimes shown on Dashboard get a localized timezone from a browser, so it's no longer UTC time (unless you work from UK). Fatmen list on Dashboard is ordered by last deployment date.","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_40","text":"Fix retrieving logs right after deploying a Fatman. Increased timeout for creating a pod. It takes into account longer image pulling.","title":"Fixed"},{"location":"docs/CHANGELOG/#0015-2021-09-07","text":"","title":"[0.0.15] - 2021-09-07"},{"location":"docs/CHANGELOG/#added_56","text":"New required owner_email field (email address of the Fatman's owner to reach out) in manifest List of Fatmen (in Dashboard) shows who deployed the fatman recently (username is taken from git credentials) Auxiliary endpoints - custom endpoint paths handled by entrypoint methods (eg. /explain endpoint)","title":"Added"},{"location":"docs/CHANGELOG/#changed_61","text":"Racetrack client shows full logs from building the image in case of failure.","title":"Changed"},{"location":"docs/CHANGELOG/#0014-2021-09-06","text":"","title":"[0.0.14] - 2021-09-06"},{"location":"docs/CHANGELOG/#changed_62","text":"Fatman has read-write access to local working directory. Deployment errors are more meaningful and less misleading due to revised health checks (ie. cluster errors are distinguished from job syntax errors or initialization errors). One common versioning for all Racetrack components (backend and CLI client)","title":"Changed"},{"location":"docs/CHANGELOG/#fixed_41","text":"Fix \"Critical worker timeout\" WSGI error Increase memory limits for building images (up to 8Gi) Fix Lifecycle resurrecting dead jobs","title":"Fixed"},{"location":"docs/CHANGELOG/#005-2021-08-26","text":"","title":"[0.0.5] - 2021-08-26"},{"location":"docs/CHANGELOG/#added_57","text":"Setting aliases for Racetrack servers","title":"Added"},{"location":"docs/CHANGELOG/#changed_63","text":"Syntax for configuring private registries credentials Input payload is flattened (without \"args\" & \"kwargs\")","title":"Changed"},{"location":"docs/CHANGELOG/#004-2021-08-20","text":"","title":"[0.0.4] - 2021-08-20"},{"location":"docs/CHANGELOG/#added_58","text":"Showing recent logs from Fatman","title":"Added"},{"location":"docs/CHANGELOG/#002-2021-08-10","text":"","title":"[0.0.2] - 2021-08-10"},{"location":"docs/CHANGELOG/#added_59","text":"Deploying Fatman","title":"Added"},{"location":"docs/admin/","text":"Administering Racetrack \u00b6 Maintaining Racetrack \u00b6 Managing users \u00b6 Creating user account \u00b6 In order to create an account, user needs to go to Racetrack Dashboard and register new account there. Then he should ask Racetrack Admin to activate his account. Racetrack Admin goes to Admin panel, Users tab, selects user, sets Active checkbox and clicks Save . Changing user's password \u00b6 Admin can change any user's password by going to Admin panel, Users tab, selecting user, clicking \"change the password using this form\". Managing Job Portfolio \u00b6 Audit Log \u00b6 \"Audit Log\" tab in Dashboard shows activity events done by users, eg. \"job F1 of user Alice has been deleted by Bob\". It can be filtered by events related to a logged user, whole job family or a particular job. Portfolio table \u00b6 \"Portfolio\" tab in Dashboard allows to browse jobs freely with custom criteria and showing the candidates for removal. Use the filters above each column to filter and limit table data. Advanced searches can be performed by using the following operators: <, <=, >, >=, =, *, !, {, }, ||,&&, [empty], [nonempty], rgx: You can delete job from there. \"Purge score\" column shows assessed penalty points representing usability of a job. A higher value means a better candidate for removal. \"Purge score\" value is explained in \"Purge reasons\" column with suggestions explaining why job is a candidate for removal. Changing Job attributes \u00b6 If you want to overwrite some deployment attributes of a job in runtime (eg. minimum memory amount, number of replicas), you can go to Admin panel, choose \"Jobs\" tab, select particular one, change its \"Manifest\" field by editing the YAML and click \"Save\". Then go back to Racetrack Dashboard, tab \"Jobs\" and click Redeploy button under the selected job. However, keep in mind that this change will be overwritten by the next deployment, so better ask maintaner of a job to change the manifest in the git repository as well. Permissions \u00b6 You can view the permissions-graph in Racetrack Dashboard, under Graph page. Click on the node to see the details and filter out the neighbours of the selected node. Permissions can be managed in Administration panel, under \"Admin panel\" tab in dashboard. See how to manage permissions . Allowing ESC to Job permissions \u00b6 In the Racetrack admin panel, go to ESC list, create new ESC. Copy the caller token code. It's base64, so it's not safely encrypted, ie. base64 -d <<< caller-token will view the underlying json structure with exact esc-id and api-token. Make sure it's safely transported to ESC developer, ie. using email encrypted with public-private key scheme like PGP. If this key becomes stolen, to prevent attacker from using it, reset the ESC api-token in Auth Subject edit page. Allowing Job to Job permissions \u00b6 Job to Job permissions are setup on Job family basis; that is you just have to set it once that family Adder can be called by Badder, then all Badder versions can communicate with all Adder versions. The relation is one way only, so Adder won't be able to call Badder unless it's permitted too. Resetting admin password \u00b6 If there's other admin user, ask him to do the reset in admin panel: Users -> select user, next to password field there will be link to change form. If none of the admins remember their password, then somebody has to exec to Lifecycle pod, cd /src/lifecycle/lifecycle/django and run python manage.py changepassword <my_admin> , or python manage.py createsuperuser . Cleaning up Docker Registry \u00b6 There is a Container Registry utility for cleaning obsolete images from the registry (collecting garbage). See registry_cleaner . Maintenance Mode \u00b6 Racetrack can be turned into a Maintenance mode. During that time, users are unable to make changes. Therefore, deploying, deleting and moving jobs is disabled. To configure it, go to Racetrack Dashboard, Administration tab, open Lifecycle Admin panel, go to Settings, add or update a setting object: Name: maintenance_mode Value: true to enable or false to disable the maintenance mode. Troubleshooting \u00b6 If something's malfunctioning, check out the following places to find more information: Racetrack dashboard pages: jobs list, audit log, dependencies graph, Job logs (Dashboard / Jobs / Logs / Runtime logs) Job build logs: via Dashboard (Jobs / Logs / Build logs), in image-builder container, at /var/log/racetrack/image-builder/build-logs , through Django Admin Panel: Deployments model, \"Build logs\" field, Django Admin panel (Dashboard > Administration > Lifecycle Admin panel): browse stored models (eg. Users, Auth subjects, Auth resource permissions, Deployments, Jobs, Job families, External service consumers), Racetrack component logs: dashboard, lifecycle, lifecycle-supervisor, image-builder, pub, postgres, pgbouncer Backup & Restore \u00b6 Here's the overview of the places where Racetrack data are stored: Postgres Database - keeps information about jobs (that are expected to be running), deployments, users, permissions, etc. Plugins Volume - a persistent volume containing plugins currently installed in the Racetrack instance. Docker Registry - a registry for keeping built job images. If a job gets killed somehow, it will be recreated from the image taken from here. Backing up the Docker Registry is not always an obligatory step, if it's fine for you to have a cluster with all jobs glowing red (requiring to redeploy) after a wipeout. If you want the jobs to be brought back to life after a wipeout, make sure to back up the Docker Registry. Job Secrets - Job secrets (git credentials and secret vars) are kept by Racetrack inside Kubernetes Secrets. If you skip to back it up, the jobs making use of secret vars won't be reproduced (the others should work fine), unless you redeploy them manually later on. Postgres Database \u00b6 Backing up \u00b6 If Postgres database runs outside kubernetes on an external server, use pgAdmin tool to make a backup of the database. Otherwise, if your database runs inside kubernetes, exec to postgres pod and use pg_dump Restoring \u00b6 If Postgres database runs outside kubernetes, use pgAdmin tool to restore the database. Otherwise, if your database runs inside kubernetes, exec to postgres pod and use pg_restore . Rotating Auth Key \u00b6 If you restore a snapshot of the Racetrack's database originating from the other environment (with different AUTH_KEY ), it can invalidate the auth tokens, making the signature invalid with the current AUTH_KEY . In this case, once the migration is done, do the following: Exec into the lifecycle-supervisor pod/container Run python -m lifecycle generate-auth admin to create a valid Auth token for you. Copy it. Go to Lifecycle-Supervisor or Lifecycle API page ( /lifecycle ) and Authorize with your Racetrack Auth Token. Call endpoints POST /api/v1/auth/token/user/all/regenerate and POST /api/v1/auth/token/job_family/regenerate to recreate valid signatures for the tokens. Plugins Volume \u00b6 Plugins are stored in a Persistent Volume called racetrack-plugins-pvc . Copy all of its contents with the help of your Kubernetes Admin. To do a restore, copy saved files back to racetrack-plugins-pvc volume and restart all the Racetrack pods. Docker Registry \u00b6 To do a backup of the Docker Registry, you can pull the images you're interested in (eg. to your local registry). When restoring, just push the images back. Check out image-builder config to see what's the URL of the registry configured with your Racetrack instance: docker_registry : ghcr.io docker_registry_namespace : theracetrack/racetrack Job Secrets \u00b6 Contact your Kubernetes administrator to back up all the Secret resources associated with the racetrack/job label.","title":"Administering"},{"location":"docs/admin/#administering-racetrack","text":"","title":"Administering Racetrack"},{"location":"docs/admin/#maintaining-racetrack","text":"","title":"Maintaining Racetrack"},{"location":"docs/admin/#managing-users","text":"","title":"Managing users"},{"location":"docs/admin/#creating-user-account","text":"In order to create an account, user needs to go to Racetrack Dashboard and register new account there. Then he should ask Racetrack Admin to activate his account. Racetrack Admin goes to Admin panel, Users tab, selects user, sets Active checkbox and clicks Save .","title":"Creating user account"},{"location":"docs/admin/#changing-users-password","text":"Admin can change any user's password by going to Admin panel, Users tab, selecting user, clicking \"change the password using this form\".","title":"Changing user's password"},{"location":"docs/admin/#managing-job-portfolio","text":"","title":"Managing Job Portfolio"},{"location":"docs/admin/#audit-log","text":"\"Audit Log\" tab in Dashboard shows activity events done by users, eg. \"job F1 of user Alice has been deleted by Bob\". It can be filtered by events related to a logged user, whole job family or a particular job.","title":"Audit Log"},{"location":"docs/admin/#portfolio-table","text":"\"Portfolio\" tab in Dashboard allows to browse jobs freely with custom criteria and showing the candidates for removal. Use the filters above each column to filter and limit table data. Advanced searches can be performed by using the following operators: <, <=, >, >=, =, *, !, {, }, ||,&&, [empty], [nonempty], rgx: You can delete job from there. \"Purge score\" column shows assessed penalty points representing usability of a job. A higher value means a better candidate for removal. \"Purge score\" value is explained in \"Purge reasons\" column with suggestions explaining why job is a candidate for removal.","title":"Portfolio table"},{"location":"docs/admin/#changing-job-attributes","text":"If you want to overwrite some deployment attributes of a job in runtime (eg. minimum memory amount, number of replicas), you can go to Admin panel, choose \"Jobs\" tab, select particular one, change its \"Manifest\" field by editing the YAML and click \"Save\". Then go back to Racetrack Dashboard, tab \"Jobs\" and click Redeploy button under the selected job. However, keep in mind that this change will be overwritten by the next deployment, so better ask maintaner of a job to change the manifest in the git repository as well.","title":"Changing Job attributes"},{"location":"docs/admin/#permissions","text":"You can view the permissions-graph in Racetrack Dashboard, under Graph page. Click on the node to see the details and filter out the neighbours of the selected node. Permissions can be managed in Administration panel, under \"Admin panel\" tab in dashboard. See how to manage permissions .","title":"Permissions"},{"location":"docs/admin/#allowing-esc-to-job-permissions","text":"In the Racetrack admin panel, go to ESC list, create new ESC. Copy the caller token code. It's base64, so it's not safely encrypted, ie. base64 -d <<< caller-token will view the underlying json structure with exact esc-id and api-token. Make sure it's safely transported to ESC developer, ie. using email encrypted with public-private key scheme like PGP. If this key becomes stolen, to prevent attacker from using it, reset the ESC api-token in Auth Subject edit page.","title":"Allowing ESC to Job permissions"},{"location":"docs/admin/#allowing-job-to-job-permissions","text":"Job to Job permissions are setup on Job family basis; that is you just have to set it once that family Adder can be called by Badder, then all Badder versions can communicate with all Adder versions. The relation is one way only, so Adder won't be able to call Badder unless it's permitted too.","title":"Allowing Job to Job permissions"},{"location":"docs/admin/#resetting-admin-password","text":"If there's other admin user, ask him to do the reset in admin panel: Users -> select user, next to password field there will be link to change form. If none of the admins remember their password, then somebody has to exec to Lifecycle pod, cd /src/lifecycle/lifecycle/django and run python manage.py changepassword <my_admin> , or python manage.py createsuperuser .","title":"Resetting admin password"},{"location":"docs/admin/#cleaning-up-docker-registry","text":"There is a Container Registry utility for cleaning obsolete images from the registry (collecting garbage). See registry_cleaner .","title":"Cleaning up Docker Registry"},{"location":"docs/admin/#maintenance-mode","text":"Racetrack can be turned into a Maintenance mode. During that time, users are unable to make changes. Therefore, deploying, deleting and moving jobs is disabled. To configure it, go to Racetrack Dashboard, Administration tab, open Lifecycle Admin panel, go to Settings, add or update a setting object: Name: maintenance_mode Value: true to enable or false to disable the maintenance mode.","title":"Maintenance Mode"},{"location":"docs/admin/#troubleshooting","text":"If something's malfunctioning, check out the following places to find more information: Racetrack dashboard pages: jobs list, audit log, dependencies graph, Job logs (Dashboard / Jobs / Logs / Runtime logs) Job build logs: via Dashboard (Jobs / Logs / Build logs), in image-builder container, at /var/log/racetrack/image-builder/build-logs , through Django Admin Panel: Deployments model, \"Build logs\" field, Django Admin panel (Dashboard > Administration > Lifecycle Admin panel): browse stored models (eg. Users, Auth subjects, Auth resource permissions, Deployments, Jobs, Job families, External service consumers), Racetrack component logs: dashboard, lifecycle, lifecycle-supervisor, image-builder, pub, postgres, pgbouncer","title":"Troubleshooting"},{"location":"docs/admin/#backup-restore","text":"Here's the overview of the places where Racetrack data are stored: Postgres Database - keeps information about jobs (that are expected to be running), deployments, users, permissions, etc. Plugins Volume - a persistent volume containing plugins currently installed in the Racetrack instance. Docker Registry - a registry for keeping built job images. If a job gets killed somehow, it will be recreated from the image taken from here. Backing up the Docker Registry is not always an obligatory step, if it's fine for you to have a cluster with all jobs glowing red (requiring to redeploy) after a wipeout. If you want the jobs to be brought back to life after a wipeout, make sure to back up the Docker Registry. Job Secrets - Job secrets (git credentials and secret vars) are kept by Racetrack inside Kubernetes Secrets. If you skip to back it up, the jobs making use of secret vars won't be reproduced (the others should work fine), unless you redeploy them manually later on.","title":"Backup &amp; Restore"},{"location":"docs/admin/#postgres-database","text":"","title":"Postgres Database"},{"location":"docs/admin/#backing-up","text":"If Postgres database runs outside kubernetes on an external server, use pgAdmin tool to make a backup of the database. Otherwise, if your database runs inside kubernetes, exec to postgres pod and use pg_dump","title":"Backing up"},{"location":"docs/admin/#restoring","text":"If Postgres database runs outside kubernetes, use pgAdmin tool to restore the database. Otherwise, if your database runs inside kubernetes, exec to postgres pod and use pg_restore .","title":"Restoring"},{"location":"docs/admin/#rotating-auth-key","text":"If you restore a snapshot of the Racetrack's database originating from the other environment (with different AUTH_KEY ), it can invalidate the auth tokens, making the signature invalid with the current AUTH_KEY . In this case, once the migration is done, do the following: Exec into the lifecycle-supervisor pod/container Run python -m lifecycle generate-auth admin to create a valid Auth token for you. Copy it. Go to Lifecycle-Supervisor or Lifecycle API page ( /lifecycle ) and Authorize with your Racetrack Auth Token. Call endpoints POST /api/v1/auth/token/user/all/regenerate and POST /api/v1/auth/token/job_family/regenerate to recreate valid signatures for the tokens.","title":"Rotating Auth Key"},{"location":"docs/admin/#plugins-volume","text":"Plugins are stored in a Persistent Volume called racetrack-plugins-pvc . Copy all of its contents with the help of your Kubernetes Admin. To do a restore, copy saved files back to racetrack-plugins-pvc volume and restart all the Racetrack pods.","title":"Plugins Volume"},{"location":"docs/admin/#docker-registry","text":"To do a backup of the Docker Registry, you can pull the images you're interested in (eg. to your local registry). When restoring, just push the images back. Check out image-builder config to see what's the URL of the registry configured with your Racetrack instance: docker_registry : ghcr.io docker_registry_namespace : theracetrack/racetrack","title":"Docker Registry"},{"location":"docs/admin/#job-secrets","text":"Contact your Kubernetes administrator to back up all the Secret resources associated with the racetrack/job label.","title":"Job Secrets"},{"location":"docs/glossary/","text":"Glossary \u00b6 Workloads: Deployment - a request by user to run Image on cluster (Image becoming a Job) Job Manifest - a YAML file in the root of your Job, which specifies the job type and provides configuration values for your Job Job - The source code converted to REST microservice workload with a standardized interface, served on Racetrack. Job Image - Job built into Docker image Job Type - one of the languages and frameworks supported by Racetrack that user choose to develop a job in Job Type Version - the specific revision of the job standard including the way of handling the requests and its features. Language Wrappers are installed in Racetrack to support particular Job Type Versions. ML Model - An application using Machine Learning algorithms created by Data Scentists or Developers, transforming input data & parameters into results. Platform: ESC (External Service Consumer) - A system external to the Racetrack using its services. Job Webview - User-defined web endpoint for human use served by Job Language Wrapper - A wrapper for a specific programming language, which is used to convert source code into a Job Image. Lifecycle - Lifecycle is a subcomponent of Racetrack responsible for automated and simplified PUC deployment and monitoring condition of deployed workloads. PUB - Subcomponent of Racetrack taking care of public access, security & routing, handling requests from External Consumers. PUC (Platform Utility Component) - Any business workload running on the platform. In particular, it might be a Machine Learning Model or any other service. RBAC - Role-based access control. A system selectively restricting access to some group of users. Racetrack admin panel - A panel for managing database models, hosted by Lifecycle component, eg. at https://racetrack.<cluster_dns>/lifecycle/admin Racetrack Dashboard - Web page showing current state of Racetrack and summary of deployed jobs in a cluster.","title":"Glossary"},{"location":"docs/glossary/#glossary","text":"Workloads: Deployment - a request by user to run Image on cluster (Image becoming a Job) Job Manifest - a YAML file in the root of your Job, which specifies the job type and provides configuration values for your Job Job - The source code converted to REST microservice workload with a standardized interface, served on Racetrack. Job Image - Job built into Docker image Job Type - one of the languages and frameworks supported by Racetrack that user choose to develop a job in Job Type Version - the specific revision of the job standard including the way of handling the requests and its features. Language Wrappers are installed in Racetrack to support particular Job Type Versions. ML Model - An application using Machine Learning algorithms created by Data Scentists or Developers, transforming input data & parameters into results. Platform: ESC (External Service Consumer) - A system external to the Racetrack using its services. Job Webview - User-defined web endpoint for human use served by Job Language Wrapper - A wrapper for a specific programming language, which is used to convert source code into a Job Image. Lifecycle - Lifecycle is a subcomponent of Racetrack responsible for automated and simplified PUC deployment and monitoring condition of deployed workloads. PUB - Subcomponent of Racetrack taking care of public access, security & routing, handling requests from External Consumers. PUC (Platform Utility Component) - Any business workload running on the platform. In particular, it might be a Machine Learning Model or any other service. RBAC - Role-based access control. A system selectively restricting access to some group of users. Racetrack admin panel - A panel for managing database models, hosted by Lifecycle component, eg. at https://racetrack.<cluster_dns>/lifecycle/admin Racetrack Dashboard - Web page showing current state of Racetrack and summary of deployed jobs in a cluster.","title":"Glossary"},{"location":"docs/manifest-schema/","text":"The Job Manifest File Schema \u00b6 To deploy a Job, the Developer should provide a build recipe called a Manifest, describing how to build, run, and deploy. The Manifest should be kept in job.yaml file located in the root of a Job's repository. Some fields are required, while optional ones will be assigned a default value if not provided. The YAML manifest file can have the following fields: name , string ( required ) - name of the current service to be deployed by means of this mainfest file. It cannot contain underscores (but can contain dashes). jobtype , string ( required ) - Jobtype wrapper used to embed model. This should be one of the supported wrapper names combined with the wrapper version: python3:2.4.0 , python3:latest , golang:latest or docker-http:latest , etc. git , object ( required ) - the object describes the place where the source code can be found using git VCS. remote , string ( required ) - HTTPS URL of git remote. This is also root of your git repo, which will become the \"current working directory\" at runtime of Job. SSH remote URLs are NOT supported. branch , string - name of the branch (if other than master) directory , string - subdirectory relative to git repo root where the project is owner_email , string ( required ) - email address of the Job's owner to reach out extends , string - relative path to base manifest file, which will be extended by this manifest version , string - Version of the Job. It must adhere to Semantic Versioning standard. jobtype_extra , object - Jobtype specific extra parameters Fields specified and validated by the jobtype. build_env , object: string to string - dictionary of environment variables that should be set when building the image image_type , string - type of deployed image. Only docker is currently available. infrastructure_target , string - Back-end platform where to deploy the service. labels , object: string to string - dictionary with metadata describing job for humans public_endpoints , array of strings - list of public job endpoints that can be accessed without authentication replicas , integer - number of running instances of the Job to deploy resources , object - resources demands to allocate to the Job memory_min , string - minimum memory amount in bytes, eg. 256Mi memory_max , string - maximum memory amount in bytes, eg. 1Gi cpu_min , string - minimum CPU consumption in cores, eg. 10m cpu_max , string - maximum CPU consumption in cores, eg. 1000m runtime_env , object: string to string - dictionary of environment variables that should be set when running Job secret_build_env_file , string - path to a secret file (on a client machine) with build environment variables secret_runtime_env_file , string - path to a secret file (on a client machine) with runtime environment variables system_dependencies , array of strings - list of system-wide packages that should be installed with package manager (apt or apk depending on base image type) Example \u00b6 This example is not valid as a whole, but it contains all fields with exemplary values: name : skynet jobtype : python3:latest git : remote : https://github.com/racetrack/supersmart-model branch : master directory : 'examples/skynet' owner_email : arnold@skynet.com extends : './base/job.yaml' version : '1.2.3-alpha' jobtype_extra : requirements_path : 'python/requirements.txt' entrypoint_path : 'python/entrypoint.py' entrypoint_class : 'JobClazz' build_env : DEBIAN_FRONTEND : 'noninteractive' image_type : docker infrastructure_target : kubernetes labels : model : linear-regression public_endpoints : - '/api/v1/perform' - '/api/v1/webview' replicas : 1 resources : memory_min : 256Mi memory_max : 1Gi cpu_min : 10m cpu_max : 1000m runtime_env : DJANGO_DEBUG : 'true' TORCH_MODEL_ZOO : zoo secret_build_env_file : '.secrets/build.env' secret_runtime_env_file : .secrets/runtime.env' system_dependencies : - 'libgomp1'","title":"Job Manifest File Schema"},{"location":"docs/manifest-schema/#the-job-manifest-file-schema","text":"To deploy a Job, the Developer should provide a build recipe called a Manifest, describing how to build, run, and deploy. The Manifest should be kept in job.yaml file located in the root of a Job's repository. Some fields are required, while optional ones will be assigned a default value if not provided. The YAML manifest file can have the following fields: name , string ( required ) - name of the current service to be deployed by means of this mainfest file. It cannot contain underscores (but can contain dashes). jobtype , string ( required ) - Jobtype wrapper used to embed model. This should be one of the supported wrapper names combined with the wrapper version: python3:2.4.0 , python3:latest , golang:latest or docker-http:latest , etc. git , object ( required ) - the object describes the place where the source code can be found using git VCS. remote , string ( required ) - HTTPS URL of git remote. This is also root of your git repo, which will become the \"current working directory\" at runtime of Job. SSH remote URLs are NOT supported. branch , string - name of the branch (if other than master) directory , string - subdirectory relative to git repo root where the project is owner_email , string ( required ) - email address of the Job's owner to reach out extends , string - relative path to base manifest file, which will be extended by this manifest version , string - Version of the Job. It must adhere to Semantic Versioning standard. jobtype_extra , object - Jobtype specific extra parameters Fields specified and validated by the jobtype. build_env , object: string to string - dictionary of environment variables that should be set when building the image image_type , string - type of deployed image. Only docker is currently available. infrastructure_target , string - Back-end platform where to deploy the service. labels , object: string to string - dictionary with metadata describing job for humans public_endpoints , array of strings - list of public job endpoints that can be accessed without authentication replicas , integer - number of running instances of the Job to deploy resources , object - resources demands to allocate to the Job memory_min , string - minimum memory amount in bytes, eg. 256Mi memory_max , string - maximum memory amount in bytes, eg. 1Gi cpu_min , string - minimum CPU consumption in cores, eg. 10m cpu_max , string - maximum CPU consumption in cores, eg. 1000m runtime_env , object: string to string - dictionary of environment variables that should be set when running Job secret_build_env_file , string - path to a secret file (on a client machine) with build environment variables secret_runtime_env_file , string - path to a secret file (on a client machine) with runtime environment variables system_dependencies , array of strings - list of system-wide packages that should be installed with package manager (apt or apk depending on base image type)","title":"The Job Manifest File Schema"},{"location":"docs/manifest-schema/#example","text":"This example is not valid as a whole, but it contains all fields with exemplary values: name : skynet jobtype : python3:latest git : remote : https://github.com/racetrack/supersmart-model branch : master directory : 'examples/skynet' owner_email : arnold@skynet.com extends : './base/job.yaml' version : '1.2.3-alpha' jobtype_extra : requirements_path : 'python/requirements.txt' entrypoint_path : 'python/entrypoint.py' entrypoint_class : 'JobClazz' build_env : DEBIAN_FRONTEND : 'noninteractive' image_type : docker infrastructure_target : kubernetes labels : model : linear-regression public_endpoints : - '/api/v1/perform' - '/api/v1/webview' replicas : 1 resources : memory_min : 256Mi memory_max : 1Gi cpu_min : 10m cpu_max : 1000m runtime_env : DJANGO_DEBUG : 'true' TORCH_MODEL_ZOO : zoo secret_build_env_file : '.secrets/build.env' secret_runtime_env_file : .secrets/runtime.env' system_dependencies : - 'libgomp1'","title":"Example"},{"location":"docs/permissions/","text":"Permission model \u00b6 Racetrack v2 introduces a restrictive Job permissions model. Users , ESCs and Job Families can be granted fine-grained permissions. For instance, users can have access to a subset of jobs and they can see and manage only those that are allowed for them. By default, nobody is allowed to do anything, if the permissions list is empty. However, when a new user is created, a set of default permissions is created accordingly. Admin can revoke these permissions later on. New user comes with the permissions allowing him to: read all jobs status (browse on a dashboard) call endpoints of every job deploy and redeploy jobs delete only jobs that has been deployed by him Admin can grant or revoke permissions in an Admin Panel. In general, single permission is an inclusive rule that can be stated as a sentence: <SUBJECT> can <SCOPE> with the <RESOURCE>. <SUBJECT> - someone who is allowed to perform the action. <SCOPE> is the verb in this sentence. It denotes the action that can be performed. <RESOURCE> denotes the system resource or a group of resources that the action can be executed on. Subject \u00b6 The Auth Subject can be one of these: User Job Family ESC (External Service Consumer) Auth Subject record is automatically created in a database by Racetrack in \"Auth Subjects\" table, so it doesn't have to be managed by the Admin. One Auth Subject can have multiple Auth Token entities. By default, it comes with a one, unique Auth Token, with no expiration date. Auth Tokens can be temporarily deactivated, permanently deleted or set an expiration time. Scope \u00b6 Auth Resource Permission is related to one of the operation type, called \"scope\": read_job - list job, check job details deploy_job - deploy job in a particular family, redeploy job deploy_new_family - deploy new job family delete_job - move to trash, dismantle from a cluster call_job - call job endpoints call_admin_api - not important for regular users. Intended for internal communication between Racetrack services full_access - not important for regular users. Covers all above. Intended for administrators. Resource \u00b6 Permisssion can cover either all jobs, whole job family, single job or a signle endpoint. When adding Auth Resource Permission (in Admin panel), there are filter fields, which narrow down the permission only to the particular resources matching criteria. If filter is not set, it covers all resources. The resource can be filtered by the following fields: Job family Job (e.g. \"adder v0.0.1\") Endpoint (e.g. /api/v1/perform ) All of these fields are optional. Each of them narrows down the filtering of the resources, which the permission covers. For instance, if Job field is empty and the Family is set, the permission works for all jobs within the family. If none of the fields is filled, the permission works on all the resources (all families, all jobs, all endpoints). Keep in mind that the permission may give an access not only to the existing resources, but also for those that are yet to come up in the future. For example, new job version from the same family will be affected, while there is a permission covering the whole family. How to grant a permission? \u00b6 If you've run into an \"Unauthorized\" error like this: Unauthorized: no permission to do this operation: auth subject \"Job Family: python-chain\" does not have permission to access endpoint /api/v1/perform at resource \"adder v0.0.1\" with scope \"call_job\" do the following to add the missing Auth Resource Permission : Go to Lifecycle Admin panel ( /lifecycle/admin ). Click \"Auth resource permissions\", \"Add\". Select Auth Subject (you can search it by name). For instance, type the name of the job family: python-chain . Select the Scope, eg. call_job . Narrow down \"Job family\", \"Job\" and \"Endpoint\" fields if needed. Click Save.","title":"Permission model"},{"location":"docs/permissions/#permission-model","text":"Racetrack v2 introduces a restrictive Job permissions model. Users , ESCs and Job Families can be granted fine-grained permissions. For instance, users can have access to a subset of jobs and they can see and manage only those that are allowed for them. By default, nobody is allowed to do anything, if the permissions list is empty. However, when a new user is created, a set of default permissions is created accordingly. Admin can revoke these permissions later on. New user comes with the permissions allowing him to: read all jobs status (browse on a dashboard) call endpoints of every job deploy and redeploy jobs delete only jobs that has been deployed by him Admin can grant or revoke permissions in an Admin Panel. In general, single permission is an inclusive rule that can be stated as a sentence: <SUBJECT> can <SCOPE> with the <RESOURCE>. <SUBJECT> - someone who is allowed to perform the action. <SCOPE> is the verb in this sentence. It denotes the action that can be performed. <RESOURCE> denotes the system resource or a group of resources that the action can be executed on.","title":"Permission model"},{"location":"docs/permissions/#subject","text":"The Auth Subject can be one of these: User Job Family ESC (External Service Consumer) Auth Subject record is automatically created in a database by Racetrack in \"Auth Subjects\" table, so it doesn't have to be managed by the Admin. One Auth Subject can have multiple Auth Token entities. By default, it comes with a one, unique Auth Token, with no expiration date. Auth Tokens can be temporarily deactivated, permanently deleted or set an expiration time.","title":"Subject"},{"location":"docs/permissions/#scope","text":"Auth Resource Permission is related to one of the operation type, called \"scope\": read_job - list job, check job details deploy_job - deploy job in a particular family, redeploy job deploy_new_family - deploy new job family delete_job - move to trash, dismantle from a cluster call_job - call job endpoints call_admin_api - not important for regular users. Intended for internal communication between Racetrack services full_access - not important for regular users. Covers all above. Intended for administrators.","title":"Scope"},{"location":"docs/permissions/#resource","text":"Permisssion can cover either all jobs, whole job family, single job or a signle endpoint. When adding Auth Resource Permission (in Admin panel), there are filter fields, which narrow down the permission only to the particular resources matching criteria. If filter is not set, it covers all resources. The resource can be filtered by the following fields: Job family Job (e.g. \"adder v0.0.1\") Endpoint (e.g. /api/v1/perform ) All of these fields are optional. Each of them narrows down the filtering of the resources, which the permission covers. For instance, if Job field is empty and the Family is set, the permission works for all jobs within the family. If none of the fields is filled, the permission works on all the resources (all families, all jobs, all endpoints). Keep in mind that the permission may give an access not only to the existing resources, but also for those that are yet to come up in the future. For example, new job version from the same family will be affected, while there is a permission covering the whole family.","title":"Resource"},{"location":"docs/permissions/#how-to-grant-a-permission","text":"If you've run into an \"Unauthorized\" error like this: Unauthorized: no permission to do this operation: auth subject \"Job Family: python-chain\" does not have permission to access endpoint /api/v1/perform at resource \"adder v0.0.1\" with scope \"call_job\" do the following to add the missing Auth Resource Permission : Go to Lifecycle Admin panel ( /lifecycle/admin ). Click \"Auth resource permissions\", \"Add\". Select Auth Subject (you can search it by name). For instance, type the name of the job family: python-chain . Select the Scope, eg. call_job . Narrow down \"Job family\", \"Job\" and \"Endpoint\" fields if needed. Click Save.","title":"How to grant a permission?"},{"location":"docs/quickstart/","text":"Quickstart \u00b6 This guide shows how to start a local instance of Racetrack and how to deploy a sample job there. Prerequisites \u00b6 Python 3.8 (or higher) - verify with python3 --version Docker v20.10 (or higher) managed by a non-root user - verify with docker ps && docker --version Docker Compose plugin - verify with docker compose version curl For instance, on Debian-based systems, it can be installed with: sudo apt update && sudo apt install curl python3 python3-pip python3-venv # Install user-managed docker curl -fsSL https://get.docker.com -o install-docker.sh sh install-docker.sh sudo usermod -aG docker $USER newgrp docker 1. Install local Racetrack \u00b6 Pick installation directory: mkdir -p racetrack && cd racetrack and install Racetrack components with an installer script : sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose docker infrastructure target (default one). Shortly after, your Racetrack instance will be ready to accept python3 jobs at 127.0.0.1:7102 . Pay attention to the output, it contains your unique admin password. 2. Install Racetrack client \u00b6 Install racetrack CLI client: python3 -m pip install --upgrade racetrack-client racetrack set remote http://127.0.0.1:7102 racetrack login --username admin # and enter your admin password 4. Deploy a Job \u00b6 Let's create a model which purpose is to add numbers. Create sample/entrypoint.py file with your application logic: class Entrypoint : def perform ( self , a : float , b : float ) -> float : \"\"\"Add numbers\"\"\" return a + b And a sample/job.yaml file describing what's inside: name : adder owner_email : sample@example.com jobtype : python3:latest git : remote : https://github.com/TheRacetrack/racetrack jobtype_extra : entrypoint_path : 'entrypoint.py' entrypoint_class : 'Entrypoint' Finally, submit your job to Racetrack: racetrack deploy sample/ This will convert your source code to a REST microservice workload, called \"Job\". 5. Call a Job \u00b6 You can find your application on the Racetrack Dashboard, which is available at http://127.0.0.1:7103/dashboard (use login admin and password provided by the installer script). Also, you should get the link to your Job from the racetrack client output. Check it out at http://127.0.0.1:7105/pub/job/adder/0.0.1 . This opens a SwaggerUI page, from which you can call your function (try /perform endpoint with {\"a\": 40, \"b\": 2} body). You can do it from CLI with an HTTP client as well: curl -X POST \"http://127.0.0.1:7105/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"a\": 40, \"b\": 2}' # Expect: 42 6. Clean up \u00b6 Tear down Racetrack instance using Makefile created by the installer script: make clean What's next? \u00b6 User Manual User Guide - Deploying a Job Local Kubernetes Setup Available plugins Installation to standalone host","title":"Quickstart"},{"location":"docs/quickstart/#quickstart","text":"This guide shows how to start a local instance of Racetrack and how to deploy a sample job there.","title":"Quickstart"},{"location":"docs/quickstart/#prerequisites","text":"Python 3.8 (or higher) - verify with python3 --version Docker v20.10 (or higher) managed by a non-root user - verify with docker ps && docker --version Docker Compose plugin - verify with docker compose version curl For instance, on Debian-based systems, it can be installed with: sudo apt update && sudo apt install curl python3 python3-pip python3-venv # Install user-managed docker curl -fsSL https://get.docker.com -o install-docker.sh sh install-docker.sh sudo usermod -aG docker $USER newgrp docker","title":"Prerequisites"},{"location":"docs/quickstart/#1-install-local-racetrack","text":"Pick installation directory: mkdir -p racetrack && cd racetrack and install Racetrack components with an installer script : sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose docker infrastructure target (default one). Shortly after, your Racetrack instance will be ready to accept python3 jobs at 127.0.0.1:7102 . Pay attention to the output, it contains your unique admin password.","title":"1. Install local Racetrack"},{"location":"docs/quickstart/#2-install-racetrack-client","text":"Install racetrack CLI client: python3 -m pip install --upgrade racetrack-client racetrack set remote http://127.0.0.1:7102 racetrack login --username admin # and enter your admin password","title":"2. Install Racetrack client"},{"location":"docs/quickstart/#4-deploy-a-job","text":"Let's create a model which purpose is to add numbers. Create sample/entrypoint.py file with your application logic: class Entrypoint : def perform ( self , a : float , b : float ) -> float : \"\"\"Add numbers\"\"\" return a + b And a sample/job.yaml file describing what's inside: name : adder owner_email : sample@example.com jobtype : python3:latest git : remote : https://github.com/TheRacetrack/racetrack jobtype_extra : entrypoint_path : 'entrypoint.py' entrypoint_class : 'Entrypoint' Finally, submit your job to Racetrack: racetrack deploy sample/ This will convert your source code to a REST microservice workload, called \"Job\".","title":"4. Deploy a Job"},{"location":"docs/quickstart/#5-call-a-job","text":"You can find your application on the Racetrack Dashboard, which is available at http://127.0.0.1:7103/dashboard (use login admin and password provided by the installer script). Also, you should get the link to your Job from the racetrack client output. Check it out at http://127.0.0.1:7105/pub/job/adder/0.0.1 . This opens a SwaggerUI page, from which you can call your function (try /perform endpoint with {\"a\": 40, \"b\": 2} body). You can do it from CLI with an HTTP client as well: curl -X POST \"http://127.0.0.1:7105/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"a\": 40, \"b\": 2}' # Expect: 42","title":"5. Call a Job"},{"location":"docs/quickstart/#6-clean-up","text":"Tear down Racetrack instance using Makefile created by the installer script: make clean","title":"6. Clean up"},{"location":"docs/quickstart/#whats-next","text":"User Manual User Guide - Deploying a Job Local Kubernetes Setup Available plugins Installation to standalone host","title":"What's next?"},{"location":"docs/deployment/k8s-installation/","text":"Installation to non-local Kubernetes \u00b6 This guide will walk you through the steps to install Racetrack on a non-localhost Kubernetes cluster, such as AKS, GKE, EKS or a self-hosted Kubernetes. Prerequisites \u00b6 Python 3.8+ with pip and venv kubectl (version 1.24.3 or higher) curl Create a Kubernetes cluster \u00b6 The first step is to create a Kubernetes cluster. Let's assume you have already created an AKS cluster on Azure , and you have access to it using the kubectl tool. Verify the connection to your cluster using the kubectl get nodes command. Next, set this cluster as the default one: kubectl config get-contexts kubectl config use-context cloud-racetrack # k8s context is `cloud-racetrack` in this tutorial kubectl config set-context --current --namespace = racetrack Install Racetrack \u00b6 Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose kubernetes infrastructure target. Shortly after, your Racetrack instance will be ready. Pay attention to the output, it contains your unique admin password. Docker Registry \u00b6 Racetrack needs a Docker registry to store the images of the jobs. We need to instruct Kubernetes to pull images from there. That's why you need to provide Docker registry hostname, username, READ_REGISTRY_TOKEN and WRITE_REGISTRY_TOKEN tokens for reading and writing images respectively. Static IP \u00b6 During the installation, you can set a static LoadBalancer IP for all public services exposed by an Ingress Controller. Sometimes cloud providers doesn't let you know the IP address before creating a resource. If you don't know the exact IP address of the LoadBalancer, you can skip this step for now. It will be assigned after the first deployment, then you can get back to this step, set the IP address and apply it again. Reviewing Kubernetes resources \u00b6 Installer generates unique, secure database password, authentication secrets and tokens. It creates Kubernetes resources files in \"generated\" directory, based on your configuration. Please review them before applying. If needed, abort, make adjustments in generated/ files and apply them on your own. See Production Deployment section before deploying Racetrack to production. Verify Racetrack \u00b6 After running the installer, verify the status of your deployments using one of your favorite tools: kubectl get pods Cloud Console Kubernetes Dashboard k9s Assuming your Ingress Controller is now deployed at public IP $YOUR_IP , you can look up the following services: Racetrack Dashboard at http://$YOUR_IP/dashboard , Lifecycle at http://$YOUR_IP/lifecycle , PUB at http://$YOUR_IP/pub , Configure Racetrack \u00b6 Install racetrack-client: python3 -m pip install --upgrade racetrack-client Log in to the Racetrack Dashboard at http://$YOUR_IP/dashboard with login admin and password provided to you by the installer. Then, go to the Profile tab and get your auth token. Go back to the command line and configure a few things with the racetrack client: # Set the current Racetrack's remote address racetrack set remote http:// $YOUR_IP /lifecycle # Login to Racetrack (enter your admin password provided by the installer) racetrack login --username admin # Activate python3 job type in the Racetrack - we're gonna deploy Python jobs racetrack plugin install github.com/TheRacetrack/plugin-python-job-type # Activate kubernetes infrastructure target in the Racetrack racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure Deploy a first job \u00b6 Let's use the Racetrack's sample model which purpose is to add numbers. Run racetrack deploy command on the sample directory: racetrack deploy sample/python-class This will convert the source code to a REST microservice workload, called Job . Call your Job \u00b6 Go to the Dashboard at http://$YOUR_IP/dashboard to find your job there. Also, you should get the link to your job from the racetrack client's output. Check it out at http://$YOUR_IP/pub/job/adder/latest . This opens a SwaggerUI page, from which you can call your function (try /perform endpoint with {\"numbers\": [40, 2]} body). You can do it from CLI with an HTTP client as well: curl -X POST \" $( racetrack get pub ) /job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42 Congratulations, your Racetrack Job is up and running! Troubleshooting \u00b6 Use one of these tools to inspect your cluster resources: kubectl Cloud Console Kubernetes Dashboard k9s Check what resources you're actually trying to deploy with kubectl kustomize generated Production Deployment \u00b6 Bunch of improvements to keep in mind before deploying Racetrack to production: Make sure to enable TLS traffic to your cluster, since PUB and Lifecycle API will receive secret tokens, which otherwise would be sent plaintext. Encrypt your secrets, for instance, using SOPS tool in order not to store them in your repository. Clean up \u00b6 Delete the resources when you're done: kubectl delete -k generated/","title":"Installation to non-local Kubernetes"},{"location":"docs/deployment/k8s-installation/#installation-to-non-local-kubernetes","text":"This guide will walk you through the steps to install Racetrack on a non-localhost Kubernetes cluster, such as AKS, GKE, EKS or a self-hosted Kubernetes.","title":"Installation to non-local Kubernetes"},{"location":"docs/deployment/k8s-installation/#prerequisites","text":"Python 3.8+ with pip and venv kubectl (version 1.24.3 or higher) curl","title":"Prerequisites"},{"location":"docs/deployment/k8s-installation/#create-a-kubernetes-cluster","text":"The first step is to create a Kubernetes cluster. Let's assume you have already created an AKS cluster on Azure , and you have access to it using the kubectl tool. Verify the connection to your cluster using the kubectl get nodes command. Next, set this cluster as the default one: kubectl config get-contexts kubectl config use-context cloud-racetrack # k8s context is `cloud-racetrack` in this tutorial kubectl config set-context --current --namespace = racetrack","title":"Create a Kubernetes cluster"},{"location":"docs/deployment/k8s-installation/#install-racetrack","text":"Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose kubernetes infrastructure target. Shortly after, your Racetrack instance will be ready. Pay attention to the output, it contains your unique admin password.","title":"Install Racetrack"},{"location":"docs/deployment/k8s-installation/#docker-registry","text":"Racetrack needs a Docker registry to store the images of the jobs. We need to instruct Kubernetes to pull images from there. That's why you need to provide Docker registry hostname, username, READ_REGISTRY_TOKEN and WRITE_REGISTRY_TOKEN tokens for reading and writing images respectively.","title":"Docker Registry"},{"location":"docs/deployment/k8s-installation/#static-ip","text":"During the installation, you can set a static LoadBalancer IP for all public services exposed by an Ingress Controller. Sometimes cloud providers doesn't let you know the IP address before creating a resource. If you don't know the exact IP address of the LoadBalancer, you can skip this step for now. It will be assigned after the first deployment, then you can get back to this step, set the IP address and apply it again.","title":"Static IP"},{"location":"docs/deployment/k8s-installation/#reviewing-kubernetes-resources","text":"Installer generates unique, secure database password, authentication secrets and tokens. It creates Kubernetes resources files in \"generated\" directory, based on your configuration. Please review them before applying. If needed, abort, make adjustments in generated/ files and apply them on your own. See Production Deployment section before deploying Racetrack to production.","title":"Reviewing Kubernetes resources"},{"location":"docs/deployment/k8s-installation/#verify-racetrack","text":"After running the installer, verify the status of your deployments using one of your favorite tools: kubectl get pods Cloud Console Kubernetes Dashboard k9s Assuming your Ingress Controller is now deployed at public IP $YOUR_IP , you can look up the following services: Racetrack Dashboard at http://$YOUR_IP/dashboard , Lifecycle at http://$YOUR_IP/lifecycle , PUB at http://$YOUR_IP/pub ,","title":"Verify Racetrack"},{"location":"docs/deployment/k8s-installation/#configure-racetrack","text":"Install racetrack-client: python3 -m pip install --upgrade racetrack-client Log in to the Racetrack Dashboard at http://$YOUR_IP/dashboard with login admin and password provided to you by the installer. Then, go to the Profile tab and get your auth token. Go back to the command line and configure a few things with the racetrack client: # Set the current Racetrack's remote address racetrack set remote http:// $YOUR_IP /lifecycle # Login to Racetrack (enter your admin password provided by the installer) racetrack login --username admin # Activate python3 job type in the Racetrack - we're gonna deploy Python jobs racetrack plugin install github.com/TheRacetrack/plugin-python-job-type # Activate kubernetes infrastructure target in the Racetrack racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure","title":"Configure Racetrack"},{"location":"docs/deployment/k8s-installation/#deploy-a-first-job","text":"Let's use the Racetrack's sample model which purpose is to add numbers. Run racetrack deploy command on the sample directory: racetrack deploy sample/python-class This will convert the source code to a REST microservice workload, called Job .","title":"Deploy a first job"},{"location":"docs/deployment/k8s-installation/#call-your-job","text":"Go to the Dashboard at http://$YOUR_IP/dashboard to find your job there. Also, you should get the link to your job from the racetrack client's output. Check it out at http://$YOUR_IP/pub/job/adder/latest . This opens a SwaggerUI page, from which you can call your function (try /perform endpoint with {\"numbers\": [40, 2]} body). You can do it from CLI with an HTTP client as well: curl -X POST \" $( racetrack get pub ) /job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42 Congratulations, your Racetrack Job is up and running!","title":"Call your Job"},{"location":"docs/deployment/k8s-installation/#troubleshooting","text":"Use one of these tools to inspect your cluster resources: kubectl Cloud Console Kubernetes Dashboard k9s Check what resources you're actually trying to deploy with kubectl kustomize generated","title":"Troubleshooting"},{"location":"docs/deployment/k8s-installation/#production-deployment","text":"Bunch of improvements to keep in mind before deploying Racetrack to production: Make sure to enable TLS traffic to your cluster, since PUB and Lifecycle API will receive secret tokens, which otherwise would be sent plaintext. Encrypt your secrets, for instance, using SOPS tool in order not to store them in your repository.","title":"Production Deployment"},{"location":"docs/deployment/k8s-installation/#clean-up","text":"Delete the resources when you're done: kubectl delete -k generated/","title":"Clean up"},{"location":"docs/deployment/local-kubernetes-setup/","text":"Local Kubernetes setup \u00b6 This tutorial deploys Racetrack locally on your computer in a KinD (a baby Kubernetes for testing) cluster. It is intended to give you the muscle memory for using a production instance of Racetrack, and to help you get used to the core Racetrack concepts. If you want to set up Racetrack on local Docker engine, see Quickstart . Prerequisites \u00b6 Docker v20.10+ managed by a non-root user Docker Compose plugin Python 3.8+ (python3 and python3-venv) kind Kubectl (version 1.24.3 or higher) (optional) k9s Installing Racetrack Locally \u00b6 Fetch the Racetrack sources: git clone https://github.com/TheRacetrack/racetrack.git Execute the following: # Enter the source root cd racetrack # install the command line client make setup # Activate Python virtual environment . venv/bin/activate # Install racetrack CLI python3 -m pip install --upgrade racetrack-client # Deploy the KinD cluster and install Racetrack in it make kind-up After a period ranging between 30-50 years, KinD will be up and running and Racetrack will be deployed inside it. You are ready to deploy a sample application to it. Submitting a Python Class \u00b6 The source code ships with a range of sample Jobs; you can find them in the path sample/ . In this tutorial, we will be Submitting the sample/python-class Job. # Set the current Racetrack's remote address - localhost inside KinD, listening on port 7002 racetrack set remote http://127.0.0.1:7002 # Login to Racetrack prior to deploying a job (with a dev token) racetrack login eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI # Install python3 job type in the Racetrack racetrack plugin install github.com/TheRacetrack/plugin-python-job-type # Install kubernetes infrastructure target in the Racetrack racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure # go to the sample directory cd sample/python-class/ # deploy from the current directory to the Racetrack service racetrack deploy After a pretty short time, the racetrack command will exit successfully and let you know the Job is deployed, giving you the URL. Before opening this URL, open Dashboard page and log in with default admin username and admin password. That will set up a session cookie allowing you to access Jobs through your browser. The code in the sample/python-class/adder.py module has been converted by Racetrack into a fully functional and well-formed Kubernetes micro-service; our Job. Please examine this file sample/python-class/adder.py in order to understand what to expect. Testing the Resulting Job \u00b6 You now have the following running on your developer workstation: A KinD cluster Racetrack deployed inside it A Python 3 micro-service converted to a Job, running inside this Racetrack There are several ways you can interact with Racetrack and this Job: Calling the Job \u00b6 The function in adder.py now hangs off an HTTP endpoint, and can be used as a ReST service. You can use curl to test this: curl -X POST \" $( racetrack get pub ) /job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42 Checking the Job Swagger \u00b6 Racetrack generates free Swagger API documentation . You can access it in your web browser here , but first you need to authenticate in order to make requests through your browser. Open Dashboard page and log in with default admin username and admin password. That will set up a session cookie allowing you to call Jobs. Checking the Job Health \u00b6 You also get a free service health endpoint : curl \" $( racetrack get pub ) /job/adder/latest/health\" # Expect: # {\"service\": \"job\", \"job_name\": \"adder\", \"status\": \"pass\"} Checking Job logs \u00b6 To see recent logs from your Job output, run racetrack logs command: racetrack logs adder racetrack logs [NAME] has 1 argument NAME (name of the job) and a couple of options: --version VERSION - version of the job, default is the latest one --remote REMOTE - Racetrack server's URL or alias name. --tail LINES - number of recent lines to show, default is 20. --follow or -f - follow logs output stream Inspecting the Job in the Racetrack Dashboard \u00b6 Racetrack ships with a dashboard. In production, it will be the admin who has access to this, but you're testing locally, so you can see it here and you can see your adder job. (optional) Inspecting the Job inside KinD Using k9s \u00b6 Invoke k9s on your command line and navigate to the pods view using :pods . Hit 0 to display all Kubernetes namespaces. Under the racetrack namespace, you should see job-adder-v-0-0-2 . Authentication \u00b6 Racetrack requires you to authenticate with a token. To manage users and tokens, visit Racetrack dashboard page . Default super-user is admin with password admin . Once the Racetrack is started, it is recommended to create other users, and deactivate default admin user for security purposes. Then visit your Profile page to see your auth token. Authentication applies to both deploying a Job and calling it: In order to deploy a Job (or use other management commands), run racetrack login command with your token in first place. For instance: racetrack login eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI --remote http://127.0.0.1:7002 In order to call a Job (fetch results from it), include your token in X-Racetrack-Auth header. For instance: curl -X POST \" $( racetrack get pub ) /job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"numbers\": [40, 2]}' Tearing it Down \u00b6 Assuming you are standing in the root directory of the Racetrack source code: # kill KinD and what it contains make kind-down # stop and remove the local testing docker registry docker stop racetrack-registry && docker rm -v racetrack-registry # exit the Python venv deactivate # optionally, clean up git clean -fxd Troubleshooting \u00b6 If you have problems running Racetrack locally, please do the following: make clean rm -rf venv make setup source venv/bin/activate docker system prune -a make kind-up # Wait 10 minutes make kind-test racetrack set remote http://127.0.0.1:7002 racetrack plugin install github.com/TheRacetrack/plugin-python-job-type racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure racetrack deploy sample/python-class If it doesn't work, diagnostic commands: kubectl get pods - do everything has \"Running\" status? If not, view logs of that pod (i.e. kubectl logs <failing_pod_name> ) look into kubectl logs service/lifecycle Can you access the server at http://127.0.0.1:7002 (in browser)? netstat -tuanpl | grep 7002 - is the port blocked by something? FAQ \u00b6 I want to build on an ARM system (such as M1 Mac) \u00b6 Disclaimer: we don't officially support or test for those architectures, but after little tweaking you should be still able to run on them. To build on an ARM system, you need to add the docker arm64 repositories to the image_builder and lifecycle Dockerfiles so apt-get can find the docker tools for amd64. You can do so by applying the following diff (save it in repo root and git apply arm64_enable.diff ): File `arm64_enable.diff` diff --git a/image_builder/Dockerfile b/image_builder/Dockerfile index 4dddd57..110564e 100644 --- a/image_builder/Dockerfile +++ b/image_builder/Dockerfile @@ -11,6 +11,9 @@ RUN apt-get update -y && apt-get install -y \\ RUN curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &&\\ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ + $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ + echo \\ + \"deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ apt-get update -y && apt-get install -y docker-ce-cli diff --git a/lifecycle/Dockerfile b/lifecycle/Dockerfile index 2063911..a2e196d 100644 --- a/lifecycle/Dockerfile +++ b/lifecycle/Dockerfile @@ -11,6 +11,9 @@ RUN apt-get update -y && apt-get install -y \\ RUN curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &&\\ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ + $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ + echo \\ + \"deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ apt-get update -y && apt-get install -y docker-ce-cli","title":"Local Kubernetes Setup"},{"location":"docs/deployment/local-kubernetes-setup/#local-kubernetes-setup","text":"This tutorial deploys Racetrack locally on your computer in a KinD (a baby Kubernetes for testing) cluster. It is intended to give you the muscle memory for using a production instance of Racetrack, and to help you get used to the core Racetrack concepts. If you want to set up Racetrack on local Docker engine, see Quickstart .","title":"Local Kubernetes setup"},{"location":"docs/deployment/local-kubernetes-setup/#prerequisites","text":"Docker v20.10+ managed by a non-root user Docker Compose plugin Python 3.8+ (python3 and python3-venv) kind Kubectl (version 1.24.3 or higher) (optional) k9s","title":"Prerequisites"},{"location":"docs/deployment/local-kubernetes-setup/#installing-racetrack-locally","text":"Fetch the Racetrack sources: git clone https://github.com/TheRacetrack/racetrack.git Execute the following: # Enter the source root cd racetrack # install the command line client make setup # Activate Python virtual environment . venv/bin/activate # Install racetrack CLI python3 -m pip install --upgrade racetrack-client # Deploy the KinD cluster and install Racetrack in it make kind-up After a period ranging between 30-50 years, KinD will be up and running and Racetrack will be deployed inside it. You are ready to deploy a sample application to it.","title":"Installing Racetrack Locally"},{"location":"docs/deployment/local-kubernetes-setup/#submitting-a-python-class","text":"The source code ships with a range of sample Jobs; you can find them in the path sample/ . In this tutorial, we will be Submitting the sample/python-class Job. # Set the current Racetrack's remote address - localhost inside KinD, listening on port 7002 racetrack set remote http://127.0.0.1:7002 # Login to Racetrack prior to deploying a job (with a dev token) racetrack login eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI # Install python3 job type in the Racetrack racetrack plugin install github.com/TheRacetrack/plugin-python-job-type # Install kubernetes infrastructure target in the Racetrack racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure # go to the sample directory cd sample/python-class/ # deploy from the current directory to the Racetrack service racetrack deploy After a pretty short time, the racetrack command will exit successfully and let you know the Job is deployed, giving you the URL. Before opening this URL, open Dashboard page and log in with default admin username and admin password. That will set up a session cookie allowing you to access Jobs through your browser. The code in the sample/python-class/adder.py module has been converted by Racetrack into a fully functional and well-formed Kubernetes micro-service; our Job. Please examine this file sample/python-class/adder.py in order to understand what to expect.","title":"Submitting a Python Class"},{"location":"docs/deployment/local-kubernetes-setup/#testing-the-resulting-job","text":"You now have the following running on your developer workstation: A KinD cluster Racetrack deployed inside it A Python 3 micro-service converted to a Job, running inside this Racetrack There are several ways you can interact with Racetrack and this Job:","title":"Testing the Resulting Job"},{"location":"docs/deployment/local-kubernetes-setup/#calling-the-job","text":"The function in adder.py now hangs off an HTTP endpoint, and can be used as a ReST service. You can use curl to test this: curl -X POST \" $( racetrack get pub ) /job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42","title":"Calling the Job"},{"location":"docs/deployment/local-kubernetes-setup/#checking-the-job-swagger","text":"Racetrack generates free Swagger API documentation . You can access it in your web browser here , but first you need to authenticate in order to make requests through your browser. Open Dashboard page and log in with default admin username and admin password. That will set up a session cookie allowing you to call Jobs.","title":"Checking the Job Swagger"},{"location":"docs/deployment/local-kubernetes-setup/#checking-the-job-health","text":"You also get a free service health endpoint : curl \" $( racetrack get pub ) /job/adder/latest/health\" # Expect: # {\"service\": \"job\", \"job_name\": \"adder\", \"status\": \"pass\"}","title":"Checking the Job Health"},{"location":"docs/deployment/local-kubernetes-setup/#checking-job-logs","text":"To see recent logs from your Job output, run racetrack logs command: racetrack logs adder racetrack logs [NAME] has 1 argument NAME (name of the job) and a couple of options: --version VERSION - version of the job, default is the latest one --remote REMOTE - Racetrack server's URL or alias name. --tail LINES - number of recent lines to show, default is 20. --follow or -f - follow logs output stream","title":"Checking Job logs"},{"location":"docs/deployment/local-kubernetes-setup/#inspecting-the-job-in-the-racetrack-dashboard","text":"Racetrack ships with a dashboard. In production, it will be the admin who has access to this, but you're testing locally, so you can see it here and you can see your adder job.","title":"Inspecting the Job in the Racetrack Dashboard"},{"location":"docs/deployment/local-kubernetes-setup/#optional-inspecting-the-job-inside-kind-using-k9s","text":"Invoke k9s on your command line and navigate to the pods view using :pods . Hit 0 to display all Kubernetes namespaces. Under the racetrack namespace, you should see job-adder-v-0-0-2 .","title":"(optional) Inspecting the Job inside KinD Using k9s"},{"location":"docs/deployment/local-kubernetes-setup/#authentication","text":"Racetrack requires you to authenticate with a token. To manage users and tokens, visit Racetrack dashboard page . Default super-user is admin with password admin . Once the Racetrack is started, it is recommended to create other users, and deactivate default admin user for security purposes. Then visit your Profile page to see your auth token. Authentication applies to both deploying a Job and calling it: In order to deploy a Job (or use other management commands), run racetrack login command with your token in first place. For instance: racetrack login eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI --remote http://127.0.0.1:7002 In order to call a Job (fetch results from it), include your token in X-Racetrack-Auth header. For instance: curl -X POST \" $( racetrack get pub ) /job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"numbers\": [40, 2]}'","title":"Authentication"},{"location":"docs/deployment/local-kubernetes-setup/#tearing-it-down","text":"Assuming you are standing in the root directory of the Racetrack source code: # kill KinD and what it contains make kind-down # stop and remove the local testing docker registry docker stop racetrack-registry && docker rm -v racetrack-registry # exit the Python venv deactivate # optionally, clean up git clean -fxd","title":"Tearing it Down"},{"location":"docs/deployment/local-kubernetes-setup/#troubleshooting","text":"If you have problems running Racetrack locally, please do the following: make clean rm -rf venv make setup source venv/bin/activate docker system prune -a make kind-up # Wait 10 minutes make kind-test racetrack set remote http://127.0.0.1:7002 racetrack plugin install github.com/TheRacetrack/plugin-python-job-type racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure racetrack deploy sample/python-class If it doesn't work, diagnostic commands: kubectl get pods - do everything has \"Running\" status? If not, view logs of that pod (i.e. kubectl logs <failing_pod_name> ) look into kubectl logs service/lifecycle Can you access the server at http://127.0.0.1:7002 (in browser)? netstat -tuanpl | grep 7002 - is the port blocked by something?","title":"Troubleshooting"},{"location":"docs/deployment/local-kubernetes-setup/#faq","text":"","title":"FAQ"},{"location":"docs/deployment/local-kubernetes-setup/#i-want-to-build-on-an-arm-system-such-as-m1-mac","text":"Disclaimer: we don't officially support or test for those architectures, but after little tweaking you should be still able to run on them. To build on an ARM system, you need to add the docker arm64 repositories to the image_builder and lifecycle Dockerfiles so apt-get can find the docker tools for amd64. You can do so by applying the following diff (save it in repo root and git apply arm64_enable.diff ): File `arm64_enable.diff` diff --git a/image_builder/Dockerfile b/image_builder/Dockerfile index 4dddd57..110564e 100644 --- a/image_builder/Dockerfile +++ b/image_builder/Dockerfile @@ -11,6 +11,9 @@ RUN apt-get update -y && apt-get install -y \\ RUN curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &&\\ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ + $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ + echo \\ + \"deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ apt-get update -y && apt-get install -y docker-ce-cli diff --git a/lifecycle/Dockerfile b/lifecycle/Dockerfile index 2063911..a2e196d 100644 --- a/lifecycle/Dockerfile +++ b/lifecycle/Dockerfile @@ -11,6 +11,9 @@ RUN apt-get update -y && apt-get install -y \\ RUN curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg &&\\ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ + $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ + echo \\ + \"deb [arch=arm64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null &&\\ apt-get update -y && apt-get install -y docker-ce-cli","title":"I want to build on an ARM system (such as M1 Mac)"},{"location":"docs/deployment/racetrack-behind-https/","text":"Setting Up Racetrack with HTTPS \u00b6 This guide will walk you through how to serve Racetrack behind a secure HTTPS protocol. TLS/SSL Certificate \u00b6 To start, you'll need a valid TLS certificate. While this guide will show you how to create a self-signed certificate, keep in mind that it's not suitable for production environments due to security risks, such as being prone to man-in-the-middle attacks. For better security, consider obtaining a certificate from a trusted source, such as Let's Encrypt. TLS Termination \u00b6 It's a common practice to split responsibilities into two separate tasks that can be handled by different teams: Core Application Setup (using HTTP) - not accessible externally, developers don't need to worry about implementing TLS properly TLS Termination at the Edge - a secure HTTPS gateway at the system's first public entry point. Example of using a self-signed certificate in local Kubernetes \u00b6 Prerequisites \u00b6 Local Kubernetes running in Kind . Racetrack deployed in the racetrack namespace. Setup \u00b6 To create a custom self-signed TLS certificate for use on an HTTPS server, follow these steps: Generate a private key and certificate: openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=ingress.local/O=nginxsvc\" Create a Kubernetes secret: kubectl create secret tls tls-secret --namespace = racetrack --key tls.key --cert tls.crt Note that this will create tls.crt which is a self-signed certificate, which is not trusted by browsers and will trigger security warnings. Self-signed certificates are suitable for development and testing purposes, but not recommended for production environments. For production use, obtain a certificate from a trusted CA. If using a self-signed certificate in a controlled environment, you can create a root CA and use it to sign server certificates, then install the root CA certificate on client machines to avoid security warnings. Set up an Ingress Controller: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml If you're using a local Kubernetes environment like Kind or Minikube, you might need to patch the ingress-nginx-controller service to use NodePort kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{\"spec\": {\"type\": \"NodePort\", \"ports\": [{\"nodePort\": 30443, \"port\": 443, \"targetPort\": 443, \"protocol\": \"TCP\", \"name\": \"https\"}]}}' Don't forget to open port 443 in your Kind configuration: apiVersion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : extraPortMappings : # TLS Ingress - hostPort : 443 containerPort : 30443 listenAddress : \"127.0.0.1\" protocol : TCP Create Ingress in a ingress.yaml file. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : namespace : racetrack name : nginx-test annotations : nginx.ingress.kubernetes.io/ssl-redirect : \"true\" spec : tls : - hosts : - ingress.local secretName : tls-secret ingressClassName : nginx rules : - http : paths : - path : /lifecycle pathType : Prefix backend : service : name : lifecycle port : number : 7002 - path : /lifecycle-supervisor pathType : Prefix backend : service : name : lifecycle-supervisor port : number : 7006 - path : /image-builder pathType : Prefix backend : service : name : image-builder port : number : 7001 - path : /dashboard pathType : Prefix backend : service : name : dashboard port : number : 7003 - path : /pub pathType : Prefix backend : service : name : pub port : number : 7005 - path : /prometheus pathType : Prefix backend : service : name : prometheus port : number : 9090 - path : /grafana pathType : Prefix backend : service : name : grafana port : number : 3000 Apply this configuration to Kubernetes: kubectl apply -f ingress.yaml Now you should be able to access the application at https://127.0.0.1/dashboard or https://ingress.local/dashboard . For more detailed information, visit the TLS Termination with Ingress-Nginx Controller documentation.","title":"Setting Up Racetrack with HTTPS"},{"location":"docs/deployment/racetrack-behind-https/#setting-up-racetrack-with-https","text":"This guide will walk you through how to serve Racetrack behind a secure HTTPS protocol.","title":"Setting Up Racetrack with HTTPS"},{"location":"docs/deployment/racetrack-behind-https/#tlsssl-certificate","text":"To start, you'll need a valid TLS certificate. While this guide will show you how to create a self-signed certificate, keep in mind that it's not suitable for production environments due to security risks, such as being prone to man-in-the-middle attacks. For better security, consider obtaining a certificate from a trusted source, such as Let's Encrypt.","title":"TLS/SSL Certificate"},{"location":"docs/deployment/racetrack-behind-https/#tls-termination","text":"It's a common practice to split responsibilities into two separate tasks that can be handled by different teams: Core Application Setup (using HTTP) - not accessible externally, developers don't need to worry about implementing TLS properly TLS Termination at the Edge - a secure HTTPS gateway at the system's first public entry point.","title":"TLS Termination"},{"location":"docs/deployment/racetrack-behind-https/#example-of-using-a-self-signed-certificate-in-local-kubernetes","text":"","title":"Example of using a self-signed certificate in local Kubernetes"},{"location":"docs/deployment/racetrack-behind-https/#prerequisites","text":"Local Kubernetes running in Kind . Racetrack deployed in the racetrack namespace.","title":"Prerequisites"},{"location":"docs/deployment/racetrack-behind-https/#setup","text":"To create a custom self-signed TLS certificate for use on an HTTPS server, follow these steps: Generate a private key and certificate: openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=ingress.local/O=nginxsvc\" Create a Kubernetes secret: kubectl create secret tls tls-secret --namespace = racetrack --key tls.key --cert tls.crt Note that this will create tls.crt which is a self-signed certificate, which is not trusted by browsers and will trigger security warnings. Self-signed certificates are suitable for development and testing purposes, but not recommended for production environments. For production use, obtain a certificate from a trusted CA. If using a self-signed certificate in a controlled environment, you can create a root CA and use it to sign server certificates, then install the root CA certificate on client machines to avoid security warnings. Set up an Ingress Controller: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml If you're using a local Kubernetes environment like Kind or Minikube, you might need to patch the ingress-nginx-controller service to use NodePort kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{\"spec\": {\"type\": \"NodePort\", \"ports\": [{\"nodePort\": 30443, \"port\": 443, \"targetPort\": 443, \"protocol\": \"TCP\", \"name\": \"https\"}]}}' Don't forget to open port 443 in your Kind configuration: apiVersion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : extraPortMappings : # TLS Ingress - hostPort : 443 containerPort : 30443 listenAddress : \"127.0.0.1\" protocol : TCP Create Ingress in a ingress.yaml file. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : namespace : racetrack name : nginx-test annotations : nginx.ingress.kubernetes.io/ssl-redirect : \"true\" spec : tls : - hosts : - ingress.local secretName : tls-secret ingressClassName : nginx rules : - http : paths : - path : /lifecycle pathType : Prefix backend : service : name : lifecycle port : number : 7002 - path : /lifecycle-supervisor pathType : Prefix backend : service : name : lifecycle-supervisor port : number : 7006 - path : /image-builder pathType : Prefix backend : service : name : image-builder port : number : 7001 - path : /dashboard pathType : Prefix backend : service : name : dashboard port : number : 7003 - path : /pub pathType : Prefix backend : service : name : pub port : number : 7005 - path : /prometheus pathType : Prefix backend : service : name : prometheus port : number : 9090 - path : /grafana pathType : Prefix backend : service : name : grafana port : number : 3000 Apply this configuration to Kubernetes: kubectl apply -f ingress.yaml Now you should be able to access the application at https://127.0.0.1/dashboard or https://ingress.local/dashboard . For more detailed information, visit the TLS Termination with Ingress-Nginx Controller documentation.","title":"Setup"},{"location":"docs/deployment/remote-docker-gateway/","text":"Remote Docker Gateway \u00b6 Once you have Racetrack up and running somewhere, you might want to connect it to remote Docker Daemon host to keep your Jobs there. In other words, you can distribute services between 2 separate infrastructures: Main Hub - hosting core Racetrack services Remote Jobs cluster - an infrastructure hosting jobs and remote Pub gateway. To do that, you need to deploy Jobs gateway and configure infrastructure target. Remote Pub gateway protects Jobs from unauthorized access. Requirements \u00b6 Python 3.8+ with pip and venv Docker v20.10 (or higher) managed by a non-root user curl Install Remote Gateway \u00b6 Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose remote-docker infrastructure target. Install plugin \u00b6 On your Racetrack's \"Main Hub\", install remote Docker plugin . You can do it with: racetrack plugin install github.com/TheRacetrack/plugin-remote-docker Next, fill plugin's configuration with the content that has been given to you by installer: infrastructure_targets : remote-docker-daemon : remote_gateway_url : 'http://1.2.3.4:7105/pub' remote_gateway_token : '5tr0nG_PA55VoRD' docker : docker_registry : 'docker.registry.example.com' username : 'DOCKER_USERNAME' password : 'READ_WRITE_TOKEN' After that, you should see a new infrastructure target available in Racetrack.","title":"Remote Docker Gateway"},{"location":"docs/deployment/remote-docker-gateway/#remote-docker-gateway","text":"Once you have Racetrack up and running somewhere, you might want to connect it to remote Docker Daemon host to keep your Jobs there. In other words, you can distribute services between 2 separate infrastructures: Main Hub - hosting core Racetrack services Remote Jobs cluster - an infrastructure hosting jobs and remote Pub gateway. To do that, you need to deploy Jobs gateway and configure infrastructure target. Remote Pub gateway protects Jobs from unauthorized access.","title":"Remote Docker Gateway"},{"location":"docs/deployment/remote-docker-gateway/#requirements","text":"Python 3.8+ with pip and venv Docker v20.10 (or higher) managed by a non-root user curl","title":"Requirements"},{"location":"docs/deployment/remote-docker-gateway/#install-remote-gateway","text":"Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose remote-docker infrastructure target.","title":"Install Remote Gateway"},{"location":"docs/deployment/remote-docker-gateway/#install-plugin","text":"On your Racetrack's \"Main Hub\", install remote Docker plugin . You can do it with: racetrack plugin install github.com/TheRacetrack/plugin-remote-docker Next, fill plugin's configuration with the content that has been given to you by installer: infrastructure_targets : remote-docker-daemon : remote_gateway_url : 'http://1.2.3.4:7105/pub' remote_gateway_token : '5tr0nG_PA55VoRD' docker : docker_registry : 'docker.registry.example.com' username : 'DOCKER_USERNAME' password : 'READ_WRITE_TOKEN' After that, you should see a new infrastructure target available in Racetrack.","title":"Install plugin"},{"location":"docs/deployment/remote-kubernetes-gateway/","text":"Remote Kubernetes Gateway \u00b6 Once you have Racetrack up and running somewhere, you might want to connect it to an external Kubernetes cluster to keep your Jobs there. In other words, you can distribute services between 2 separate infrastructures: Main Hub - hosting core Racetrack services Remote Jobs cluster - an infrastructure hosting jobs and remote Pub gateway. To do that, you need to deploy Jobs gateway and configure infrastructure target. Remote Pub gateway protects Jobs from unauthorized access. Requirements \u00b6 Python 3.8+ with pip and venv kubectl (version 1.24.3 or higher) curl Install Remote Gateway \u00b6 Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose remote-kubernetes infrastructure target. Install plugin \u00b6 On your Racetrack's \"Main Hub\", install remote Kubernetes plugin . You can do it with: racetrack plugin install github.com/TheRacetrack/plugin-remote-kubernetes Next, fill plugin's configuration with the content that has been given to you by installer: infrastructure_targets : remote-k8s : remote_gateway_url : 'http://1.2.3.4:7105/pub' remote_gateway_token : '5tr0nG_PA55VoRD' job_k8s_namespace : 'racetrack' docker : docker_registry : 'docker.registry.example.com' username : 'DOCKER_USERNAME' password : 'READ_WRITE_TOKEN' After that, you should see a new infrastructure target available in Racetrack.","title":"Remote Kubernetes Gateway"},{"location":"docs/deployment/remote-kubernetes-gateway/#remote-kubernetes-gateway","text":"Once you have Racetrack up and running somewhere, you might want to connect it to an external Kubernetes cluster to keep your Jobs there. In other words, you can distribute services between 2 separate infrastructures: Main Hub - hosting core Racetrack services Remote Jobs cluster - an infrastructure hosting jobs and remote Pub gateway. To do that, you need to deploy Jobs gateway and configure infrastructure target. Remote Pub gateway protects Jobs from unauthorized access.","title":"Remote Kubernetes Gateway"},{"location":"docs/deployment/remote-kubernetes-gateway/#requirements","text":"Python 3.8+ with pip and venv kubectl (version 1.24.3 or higher) curl","title":"Requirements"},{"location":"docs/deployment/remote-kubernetes-gateway/#install-remote-gateway","text":"Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose remote-kubernetes infrastructure target.","title":"Install Remote Gateway"},{"location":"docs/deployment/remote-kubernetes-gateway/#install-plugin","text":"On your Racetrack's \"Main Hub\", install remote Kubernetes plugin . You can do it with: racetrack plugin install github.com/TheRacetrack/plugin-remote-kubernetes Next, fill plugin's configuration with the content that has been given to you by installer: infrastructure_targets : remote-k8s : remote_gateway_url : 'http://1.2.3.4:7105/pub' remote_gateway_token : '5tr0nG_PA55VoRD' job_k8s_namespace : 'racetrack' docker : docker_registry : 'docker.registry.example.com' username : 'DOCKER_USERNAME' password : 'READ_WRITE_TOKEN' After that, you should see a new infrastructure target available in Racetrack.","title":"Install plugin"},{"location":"docs/deployment/standalone-host/","text":"Installation to standalone host \u00b6 You can install Racetrack to a standalone host (e.g. EC2 host or fresh VM instance) using the installer script that runs it on the Docker Engine infrastructure. Requirements \u00b6 Python 3.8+ with pip and venv Docker v20.10 (or higher) managed by a non-root user Docker Compose plugin curl For instance, on Debian-based systems do: sudo apt update && sudo apt install curl python3 python3-pip python3-venv # Install user-managed docker curl -fsSL https://get.docker.com -o install-docker.sh sh install-docker.sh sudo usermod -aG docker $USER newgrp docker Install Racetrack \u00b6 Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose docker infrastructure target. Shortly after, your Racetrack instance will be ready. Notes \u00b6 Pay attention to the output. The installer will generate unique passwords for your setup, you'll be provided with the Dashboard address and superuser credentials. Set the environment variable export RT_NON_INTERACTIVE=1 to skip answering installer's questions and go with the defaults. You can also set custom values for parameters by setting environment variables: export RT_EXTERNAL_ADDRESS=\"http://127.0.0.1\" to set the external address that your Racetrack will be accessed at (IP or domain name). export RT_INFRASTRUCTURE=\"docker\" to set the infrastructure target to deploy Racetrack. Edit or remove local setup configuration at setup.json and run installer again to reconfigure installation steps. You can use locally installed racetrack CLI client after activating venv . venv/bin/activate . It's already logged in and has remote address configured. The following services are hosted on address 0.0.0.0 , thus publicly available: Dashboard on port 7103 Lifecycle on port 7102 Pub on port 7105 Lifecycle Supervisor on port 7106 Grafana on port 3100 Jobs and other services should be running only in the internal network. This installer makes use of Docker Infrastructure plugin to deploy jobs to in-place Docker Engine infrastructure target. Manage Racetrack \u00b6 Installer creates Makefile , which contains a few useful commands: make down to stop the Racetrack. make up to start it up again or upgrade to the latest version. make clean to shut down and clean up the Racetrack.","title":"Installation to standalone host"},{"location":"docs/deployment/standalone-host/#installation-to-standalone-host","text":"You can install Racetrack to a standalone host (e.g. EC2 host or fresh VM instance) using the installer script that runs it on the Docker Engine infrastructure.","title":"Installation to standalone host"},{"location":"docs/deployment/standalone-host/#requirements","text":"Python 3.8+ with pip and venv Docker v20.10 (or higher) managed by a non-root user Docker Compose plugin curl For instance, on Debian-based systems do: sudo apt update && sudo apt install curl python3 python3-pip python3-venv # Install user-managed docker curl -fsSL https://get.docker.com -o install-docker.sh sh install-docker.sh sudo usermod -aG docker $USER newgrp docker","title":"Requirements"},{"location":"docs/deployment/standalone-host/#install-racetrack","text":"Pick an installation directory: mkdir -p ~/racetrack && cd ~/racetrack and run sh < ( curl -fsSL https://raw.githubusercontent.com/TheRacetrack/racetrack/master/utils/standalone-wizard/runner.sh ) Follow the installation steps. Choose docker infrastructure target. Shortly after, your Racetrack instance will be ready.","title":"Install Racetrack"},{"location":"docs/deployment/standalone-host/#notes","text":"Pay attention to the output. The installer will generate unique passwords for your setup, you'll be provided with the Dashboard address and superuser credentials. Set the environment variable export RT_NON_INTERACTIVE=1 to skip answering installer's questions and go with the defaults. You can also set custom values for parameters by setting environment variables: export RT_EXTERNAL_ADDRESS=\"http://127.0.0.1\" to set the external address that your Racetrack will be accessed at (IP or domain name). export RT_INFRASTRUCTURE=\"docker\" to set the infrastructure target to deploy Racetrack. Edit or remove local setup configuration at setup.json and run installer again to reconfigure installation steps. You can use locally installed racetrack CLI client after activating venv . venv/bin/activate . It's already logged in and has remote address configured. The following services are hosted on address 0.0.0.0 , thus publicly available: Dashboard on port 7103 Lifecycle on port 7102 Pub on port 7105 Lifecycle Supervisor on port 7106 Grafana on port 3100 Jobs and other services should be running only in the internal network. This installer makes use of Docker Infrastructure plugin to deploy jobs to in-place Docker Engine infrastructure target.","title":"Notes"},{"location":"docs/deployment/standalone-host/#manage-racetrack","text":"Installer creates Makefile , which contains a few useful commands: make down to stop the Racetrack. make up to start it up again or upgrade to the latest version. make clean to shut down and clean up the Racetrack.","title":"Manage Racetrack"},{"location":"docs/development/bom/","text":"Bill of Materials \u00b6 This document contains a non-exhaustive \"bill of materials\" for the upstream open source projects on which we depend. The intention is to maintain a list to help us manage our software supply chain security by being aware of the open source projects we need to track as part of the long term lifecycle of Racetrack. Here's the list of the open source projects we use in Racetrack. The columns are: Our version : the version we use in Racetrack Latest version : newest version from the provider Purpose : what we use the project for Tool Our version Latest version Purpose a2wsgi 1.7.0 1.7.0 serving Django app in ASGI server Axios 1.3.5 1.4.0 HTTP front-end client backoff 2.2.1 2.2.1 repeating attempts in case of error Bootstrap 5.0.1 5.3 Dashboard's UI components coverage 7.2.1 7.2.1 measuring test coverage Django 4.1.7 4.1.7 Lifecycle Admin panel, Dashboard UI Docker 20.10.17 23.0.1 distributing Racetrack, building job images FastAPI 0.92.0 0.92.0 serving API by Python components Gin 1.9.0 1.9.0 Web framework for Go Grafana 10.1.4 10.1.4 Metrics visualization github-markdown-css 5.1.0 5.2.0 rendering docs git 2.44.0 2.44.0 cloning job repositories Go 1.22 1.22 PUB code httpretty 1.1.4 1.1.4 mocking HTTP requests httpx 0.23.3 0.23.3 making HTTP requests Jinja2 3.1.2 3.1.2 templating Dockerfiles Highlight.js 11.8.0 11.8.0 YAML syntax highlighter on front-end kubectl 1.26.2 1.26.2 managing Kubernetes Kubernetes Python Client 26.1.0 26.1.0 managing Kubernetes log15 3.0.0 3.0.0 structured logging in Go Node.js 18.16 20.1.0 Dashboard's development environment nvm 0.39.3 0.39.3 Management of Node versions pkg/errors ( archived repository ) 0.9.1 0.9.1 wrapping errors context PostgreSQL 16.0 16.0 Database for jobs and users PostgreSQL Server Exporter 0.14.0 0.14.0 PostgreSQL metrics PgBouncer 1.20.1 1.20.1 Connection pooling for PostgreSQL Prism 1.28.0 1.29.0 rendering code snippets in docs Prometheus 2.47.1 2.47.1 collecting metrics Prometheus Go client library 1.14.0 1.14.0 metrics exposure in Go prometheus-client 0.16.0 0.16.0 metrics exposure psutil 5.9.4 5.9.4 monitoring running processes psycopg 3.1.12 3.1.12 connecting to PostgreSQL pydantic 1.10.5 1.10.5 schema validation pyjwt 2.6.0 2.6.0 handling JWT tokens pytest 7.2.1 7.2.1 running tests Python markdown 3.4.1 3.4.1 rendering docs in Dashboard python-multipart 0.0.6 0.0.6 handling uploaded plugins python-socketio 5.7.2 5.7.2 Streaming logs of the job in follow mode Python 3.11 3.12 Racetrack client, Lifecycle, Image builder code PyYAML 6.0 6.0 parsing YAML config files Quasar Framework 2.11.10 2.11.10 UI components on Dashboard schedule 1.1.0 1.1.0 running periodic tasks typer 0.7.0 0.7.0 parsing command line arguments Typescript 4.8.4 5.0.4 Dashboard UI code uvicorn 0.20.0 0.20.0 serving ASGI app vis-network 9.1.1 9.1.4 Jobs graph in Dashboard Vite 4.1.4 4.3.4 Dashboard's local development and packaging vue-router 4.1.6 4.1.6 Routing Vue.js pages vue-toastification 2.0.0-rc.5 2.0.0-rc.5 Web UI notifications Vue.js 3.2.47 3.2.47 UI Web framework watchdog 2.3.1 2.3.1 watching for changes in plugins werkzeug 2.2.3 2.2.3 serving SocketIO server in tests","title":"Bill of Materials"},{"location":"docs/development/bom/#bill-of-materials","text":"This document contains a non-exhaustive \"bill of materials\" for the upstream open source projects on which we depend. The intention is to maintain a list to help us manage our software supply chain security by being aware of the open source projects we need to track as part of the long term lifecycle of Racetrack. Here's the list of the open source projects we use in Racetrack. The columns are: Our version : the version we use in Racetrack Latest version : newest version from the provider Purpose : what we use the project for Tool Our version Latest version Purpose a2wsgi 1.7.0 1.7.0 serving Django app in ASGI server Axios 1.3.5 1.4.0 HTTP front-end client backoff 2.2.1 2.2.1 repeating attempts in case of error Bootstrap 5.0.1 5.3 Dashboard's UI components coverage 7.2.1 7.2.1 measuring test coverage Django 4.1.7 4.1.7 Lifecycle Admin panel, Dashboard UI Docker 20.10.17 23.0.1 distributing Racetrack, building job images FastAPI 0.92.0 0.92.0 serving API by Python components Gin 1.9.0 1.9.0 Web framework for Go Grafana 10.1.4 10.1.4 Metrics visualization github-markdown-css 5.1.0 5.2.0 rendering docs git 2.44.0 2.44.0 cloning job repositories Go 1.22 1.22 PUB code httpretty 1.1.4 1.1.4 mocking HTTP requests httpx 0.23.3 0.23.3 making HTTP requests Jinja2 3.1.2 3.1.2 templating Dockerfiles Highlight.js 11.8.0 11.8.0 YAML syntax highlighter on front-end kubectl 1.26.2 1.26.2 managing Kubernetes Kubernetes Python Client 26.1.0 26.1.0 managing Kubernetes log15 3.0.0 3.0.0 structured logging in Go Node.js 18.16 20.1.0 Dashboard's development environment nvm 0.39.3 0.39.3 Management of Node versions pkg/errors ( archived repository ) 0.9.1 0.9.1 wrapping errors context PostgreSQL 16.0 16.0 Database for jobs and users PostgreSQL Server Exporter 0.14.0 0.14.0 PostgreSQL metrics PgBouncer 1.20.1 1.20.1 Connection pooling for PostgreSQL Prism 1.28.0 1.29.0 rendering code snippets in docs Prometheus 2.47.1 2.47.1 collecting metrics Prometheus Go client library 1.14.0 1.14.0 metrics exposure in Go prometheus-client 0.16.0 0.16.0 metrics exposure psutil 5.9.4 5.9.4 monitoring running processes psycopg 3.1.12 3.1.12 connecting to PostgreSQL pydantic 1.10.5 1.10.5 schema validation pyjwt 2.6.0 2.6.0 handling JWT tokens pytest 7.2.1 7.2.1 running tests Python markdown 3.4.1 3.4.1 rendering docs in Dashboard python-multipart 0.0.6 0.0.6 handling uploaded plugins python-socketio 5.7.2 5.7.2 Streaming logs of the job in follow mode Python 3.11 3.12 Racetrack client, Lifecycle, Image builder code PyYAML 6.0 6.0 parsing YAML config files Quasar Framework 2.11.10 2.11.10 UI components on Dashboard schedule 1.1.0 1.1.0 running periodic tasks typer 0.7.0 0.7.0 parsing command line arguments Typescript 4.8.4 5.0.4 Dashboard UI code uvicorn 0.20.0 0.20.0 serving ASGI app vis-network 9.1.1 9.1.4 Jobs graph in Dashboard Vite 4.1.4 4.3.4 Dashboard's local development and packaging vue-router 4.1.6 4.1.6 Routing Vue.js pages vue-toastification 2.0.0-rc.5 2.0.0-rc.5 Web UI notifications Vue.js 3.2.47 3.2.47 UI Web framework watchdog 2.3.1 2.3.1 watching for changes in plugins werkzeug 2.2.3 2.2.3 serving SocketIO server in tests","title":"Bill of Materials"},{"location":"docs/development/develop/","text":"Developer Manual \u00b6 Components Diagram \u00b6 Prerequisites \u00b6 Install: Python 3.8+ ( sudo apt install python3.8 python3.8-dev python3.8-venv on Ubuntu 18+) Docker v20.10+ kubectl v1.24.3+ - if you're going to deploy to Kind kind - if you're going to deploy to Kind Go 1.19+ - if you're going to develop components in Go (PUB, go_wrapper) nvm - if you're going to rebuild Dashboard front-end Development Setup \u00b6 Setup & activate Python venv (this is required for all next steps): # in a project-root directory make setup . venv/bin/activate Components can be run in 3 different ways, every next way is more integrated and closer to target setup, but it boots up longer: Localhost Docker compose Kind Quickstart \u00b6 If you're new to Racetrack, you can just run the following command to launch a local Racetrack instance relatively quickly: make compose-up Then, you can visit http://127.0.0.1:7103 to see the Racetrack Dashboard (default user/password: admin/admin). Lifecycle server runs on http://127.0.0.1:7102 (it's the URL you deploy your jobs there). Let's create a \"dev\" alias for it and set it as a current remote: racetrack set alias dev http://127.0.0.1:7102 racetrack set remote dev Login to Racetrack prior to deploying a job (you can find it in the \"Profile\" tab of the Dashboard): racetrack login eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI Activate python3 job type in the Racetrack by installing the plugin: racetrack plugin install github.com/TheRacetrack/plugin-python-job-type Install docker infrastructure plugin: racetrack plugin install github.com/TheRacetrack/plugin-docker-infrastructure Finally, you can deploy some jobs there, eg.: racetrack deploy sample/python-class After all, run make clean to dismantle the local instance. Localhost \u00b6 Single components running on localhost (outside docker), independently of the others. Best for developing/debugging single component, as changes can be most quickly tested. Each component supports make run for directly running it, ie. cd lifecycle && make run Notes: image_builder - Call image_builder build in sample/python-class/ to test just building a job image. pub - use make send-payload-post for testing the proxying of payload to Job dashboard - it will print out on which port a Django UI is available job_wrapper - Call job_wrapper run adder.py in sample/python-class/ to just test a Python class wrapper. Submitting a job: racetrack deploy sample/python-class/ --remote http://127.0.0.1:7202 New container should be created. It can be accessed at http://127.0.0.1:7000 You need to docker rm or make docker-clean-job to clean leftover job on your own. In case of errors, troubleshoot with docker ps and docker logs -f <job_name> . Job can be accessed through the PUB at http://127.0.0.1:7205/pub/job/adder/latest , where \"adder\" is a name of a job from job.yaml . Docker compose \u00b6 Jobs can also run as local docker containers. make compose-up - runs services in detached mode make compose-up-service service=dashboard - rebuilds and reruns one selected service make compose-run - runs services with logs make compose-down to clean up the setup Submitting a job: racetrack deploy sample/python-class/ --remote http://127.0.0.1:7102 # or: compose-deploy-sample Job management/access is the same as in Localhost case. Kind \u00b6 A Kubernetes cluster in a Docker container. make kind-up to set it up, make kind-down to tear down. After applying some changes, redeploy using make kind-redeploy . To deploy a job you need kubernetes infrastructure plugin: racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure Submitting a job: racetrack deploy sample/python-class/ --remote http://127.0.0.1:7002 Jobs are deployed as k8s pods, and should be managed as such. Dashboard \u00b6 Racetrack admin panel is at: http://127.0.0.1:7002/lifecycle/admin/ (user/password: admin) Racetrack dashboard (for public consumption) is at: http://127.0.0.1:7003/dashboard/ (ports might need to be adjusted according to below table) Port numbers \u00b6 service kind/Kubernetes (X) docker-compose (X+100) localhost (X+200) Lifecycle 7002 7102 7202 Image Builder 7001 7101 7201 Dashboard 7003 7103 7203 Job 7000 7100 7200 PUB 7005 7105 7205 Lifecycle Supervisor 7006 7106 7202 postgres 5432 5532 --- (1) Prometheus 9090 9192 Grafana 3000 3100 (1) - none as Postgres is not run on localhost Calling a model \u00b6 On any of localhost setups: curl -X POST \"http://127.0.0.1:7005/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42 The 7005 port needs to be adjusted according to dev setup, as in table above. Calling model on remote Racetrack instance: curl -k -X POST \"https://<cluster ip>/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42 Deploy Job to Kubernetes \u00b6 Enter directory with job.yaml and issue: racetrack deploy . --remote https://racetrack.<cluster name>/lifecycle See Installation to Kubernetes for more details on how to deploy a job to the Racetrack instance running on Kubernetes as an end user. Testing \u00b6 Run the following command to perform all tests (unit tests and End-to-End): make kind-up kind-test clean You can also run E2E tests on docker-compose setup: make compose-up compose-test clean Run unit tests only: make test-unit Debugging \u00b6 In order to view Lifecycle Postgres db, in k8s dashboard exec into postgres pod and: psql -h 127.0.0.1 -d racetrack -U racetrack -p 5432 Debugging in docker \u00b6 Services have additional docker images prepared that allow attaching debugger to them. compose-debug-override.yaml contains overrides for default docker compose configuration. You can start cluster with debuggers by using make compose-up-debug Python services are run using debugpy , while go ones(only PUB at this moment) use Delve . Debugger ports for services are as follows: service Debugger Port Lifecycle debugpy 5678 Lifecycle Supervisor debugpy 5679 Image Builder debugpy 5680 Dashboard debugpy 5681 PUB Delve 12345","title":"Developer manual"},{"location":"docs/development/develop/#developer-manual","text":"","title":"Developer Manual"},{"location":"docs/development/develop/#components-diagram","text":"","title":"Components Diagram"},{"location":"docs/development/develop/#prerequisites","text":"Install: Python 3.8+ ( sudo apt install python3.8 python3.8-dev python3.8-venv on Ubuntu 18+) Docker v20.10+ kubectl v1.24.3+ - if you're going to deploy to Kind kind - if you're going to deploy to Kind Go 1.19+ - if you're going to develop components in Go (PUB, go_wrapper) nvm - if you're going to rebuild Dashboard front-end","title":"Prerequisites"},{"location":"docs/development/develop/#development-setup","text":"Setup & activate Python venv (this is required for all next steps): # in a project-root directory make setup . venv/bin/activate Components can be run in 3 different ways, every next way is more integrated and closer to target setup, but it boots up longer: Localhost Docker compose Kind","title":"Development Setup"},{"location":"docs/development/develop/#quickstart","text":"If you're new to Racetrack, you can just run the following command to launch a local Racetrack instance relatively quickly: make compose-up Then, you can visit http://127.0.0.1:7103 to see the Racetrack Dashboard (default user/password: admin/admin). Lifecycle server runs on http://127.0.0.1:7102 (it's the URL you deploy your jobs there). Let's create a \"dev\" alias for it and set it as a current remote: racetrack set alias dev http://127.0.0.1:7102 racetrack set remote dev Login to Racetrack prior to deploying a job (you can find it in the \"Profile\" tab of the Dashboard): racetrack login eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI Activate python3 job type in the Racetrack by installing the plugin: racetrack plugin install github.com/TheRacetrack/plugin-python-job-type Install docker infrastructure plugin: racetrack plugin install github.com/TheRacetrack/plugin-docker-infrastructure Finally, you can deploy some jobs there, eg.: racetrack deploy sample/python-class After all, run make clean to dismantle the local instance.","title":"Quickstart"},{"location":"docs/development/develop/#localhost","text":"Single components running on localhost (outside docker), independently of the others. Best for developing/debugging single component, as changes can be most quickly tested. Each component supports make run for directly running it, ie. cd lifecycle && make run Notes: image_builder - Call image_builder build in sample/python-class/ to test just building a job image. pub - use make send-payload-post for testing the proxying of payload to Job dashboard - it will print out on which port a Django UI is available job_wrapper - Call job_wrapper run adder.py in sample/python-class/ to just test a Python class wrapper. Submitting a job: racetrack deploy sample/python-class/ --remote http://127.0.0.1:7202 New container should be created. It can be accessed at http://127.0.0.1:7000 You need to docker rm or make docker-clean-job to clean leftover job on your own. In case of errors, troubleshoot with docker ps and docker logs -f <job_name> . Job can be accessed through the PUB at http://127.0.0.1:7205/pub/job/adder/latest , where \"adder\" is a name of a job from job.yaml .","title":"Localhost"},{"location":"docs/development/develop/#docker-compose","text":"Jobs can also run as local docker containers. make compose-up - runs services in detached mode make compose-up-service service=dashboard - rebuilds and reruns one selected service make compose-run - runs services with logs make compose-down to clean up the setup Submitting a job: racetrack deploy sample/python-class/ --remote http://127.0.0.1:7102 # or: compose-deploy-sample Job management/access is the same as in Localhost case.","title":"Docker compose"},{"location":"docs/development/develop/#kind","text":"A Kubernetes cluster in a Docker container. make kind-up to set it up, make kind-down to tear down. After applying some changes, redeploy using make kind-redeploy . To deploy a job you need kubernetes infrastructure plugin: racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure Submitting a job: racetrack deploy sample/python-class/ --remote http://127.0.0.1:7002 Jobs are deployed as k8s pods, and should be managed as such.","title":"Kind"},{"location":"docs/development/develop/#dashboard","text":"Racetrack admin panel is at: http://127.0.0.1:7002/lifecycle/admin/ (user/password: admin) Racetrack dashboard (for public consumption) is at: http://127.0.0.1:7003/dashboard/ (ports might need to be adjusted according to below table)","title":"Dashboard"},{"location":"docs/development/develop/#port-numbers","text":"service kind/Kubernetes (X) docker-compose (X+100) localhost (X+200) Lifecycle 7002 7102 7202 Image Builder 7001 7101 7201 Dashboard 7003 7103 7203 Job 7000 7100 7200 PUB 7005 7105 7205 Lifecycle Supervisor 7006 7106 7202 postgres 5432 5532 --- (1) Prometheus 9090 9192 Grafana 3000 3100 (1) - none as Postgres is not run on localhost","title":"Port numbers"},{"location":"docs/development/develop/#calling-a-model","text":"On any of localhost setups: curl -X POST \"http://127.0.0.1:7005/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42 The 7005 port needs to be adjusted according to dev setup, as in table above. Calling model on remote Racetrack instance: curl -k -X POST \"https://<cluster ip>/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: 42","title":"Calling a model"},{"location":"docs/development/develop/#deploy-job-to-kubernetes","text":"Enter directory with job.yaml and issue: racetrack deploy . --remote https://racetrack.<cluster name>/lifecycle See Installation to Kubernetes for more details on how to deploy a job to the Racetrack instance running on Kubernetes as an end user.","title":"Deploy Job to Kubernetes"},{"location":"docs/development/develop/#testing","text":"Run the following command to perform all tests (unit tests and End-to-End): make kind-up kind-test clean You can also run E2E tests on docker-compose setup: make compose-up compose-test clean Run unit tests only: make test-unit","title":"Testing"},{"location":"docs/development/develop/#debugging","text":"In order to view Lifecycle Postgres db, in k8s dashboard exec into postgres pod and: psql -h 127.0.0.1 -d racetrack -U racetrack -p 5432","title":"Debugging"},{"location":"docs/development/develop/#debugging-in-docker","text":"Services have additional docker images prepared that allow attaching debugger to them. compose-debug-override.yaml contains overrides for default docker compose configuration. You can start cluster with debuggers by using make compose-up-debug Python services are run using debugpy , while go ones(only PUB at this moment) use Delve . Debugger ports for services are as follows: service Debugger Port Lifecycle debugpy 5678 Lifecycle Supervisor debugpy 5679 Image Builder debugpy 5680 Dashboard debugpy 5681 PUB Delve 12345","title":"Debugging in docker"},{"location":"docs/development/developing-plugins/","text":"Developing plugins \u00b6 How does plugin work? \u00b6 When Racetrack starts up, it scans for plugins. Each plugin is a zip file, and the process for loading them goes like this: Extract the plugin code from the zip file. If the plugin includes a requirements.txt file, it installs the dependencies. Racetrack loads the plugin.py file, which should define a class called Plugin . This class is then instantiated and kept in internal plugins list. When certain customizable events occur, the plugin engine is notified. All the plugin hooks are then called, in order of their priority: Plugins have a default priority of 0. Plugins with the lowest priority get executed first. If multiple plugins have the same priority, they are executed in the order they were added in the configuration file. Creating a plugin \u00b6 Create a git repo, copy plugin_sample to it and modify the hooks implementation as you like. Use same virtualenv from racetrack repository (your plugin can use the same dependencies as Lifecycle does). Check out plugins-job-types.md to see how to create a job-type plugin. Using additional dependencies \u00b6 Plugins can use additional dependencies (Python packages). You can include them in a requirements.txt file. It will be installed using pip so you can use these dependencies in your plugin's code. Create a plugin manifest \u00b6 Create plugin-manifest.yaml file in a plugin directory. Basically, it contains the metadata of the plugin. It can have the following fields in YAML format: name ( required , string ) - name of the plugin version ( required , string ) - version of the plugin url (optional, string ) - a link to the plugin page priority (optional, integer ) - order in plugins sequence, lowest priority gets executed first. Integer field, 0 by default. category (optional, string ) - kind of the plugin, either 'infrastructure', 'job-type' or 'core' components (optional, list of strings ) - list of Racetrack components that the plugin should be running on, e.g. lifecycle , image-builder . If it's empty, plugins are loaded on all components. Example: name : skynet-watcher version : 1.2.3 url : https://github.com/TheRacetrack/racetrack priority : -1 category : core components : - lifecycle Building a plugin \u00b6 Local source code of the plugin can be turned into a ZIP file by means of a racetrack client tool. Install racetrack client: python3 -m pip install --upgrade racetrack-client Go to the directory where your plugin is located. Make sure the plugin version inside plugin-manifest.yaml is up-to-date. Run racetrack plugin bundle to turn a plugin into a ZIP file. Zipped plugin will be generated in a plugin directory. See the output to locate the outcome package. See plugin_sample for an example of a plugin. Supported hooks \u00b6 To add functionality to the plugin, you can implement one of the following hooks (Python methods): post_job_deploy \u00b6 post_job_deploy implements supplementary actions invoked after a job is deployed. from racetrack_client.manifest import Manifest from racetrack_commons.entities.dto import JobDto class Plugin : def post_job_deploy ( self , manifest : Manifest , job : JobDto , image_name : str , deployer_username : str = None ): ... job_runtime_env_vars \u00b6 job_runtime_env_vars provides supplementary env vars dictionary added to runtime vars when deploying a Job. class Plugin : def job_runtime_env_vars ( self ) -> dict [ str , str ] | None : return { 'SKYNET_ENGAGED' : '1' , } job_types \u00b6 job_types method declares all Job types provided by this plugin. It returns a dictionary, where a key is a job type name with version (e.g. python3:1.0.2 ) and the value is a Job Type Definition object. Job Type Definition describes the details of the job images used by the job. It's a dictionary that has the key images , which is a list of Job Image Definition objects. If the job consists of a single container, the list will contain only one element. Job Image Definition is a dictionary with the following properties: dockerfile_path ( string ) - a relative path to the Dockerfile source ( string enum ) - it's either \"jobtype\" or \"job\" , depending on the location of the expected Dockerfile. Choose \"jobtype\" , if a Dockerfile is provided by a job type plugin, in a ZIP bundle. Choose \"job\" , if a Dockerfile will be provided by the Job itself, taken from the Job's repository. template ( boolean ) - whether Dockerfile is a template and contains variables to evaluate. For instance, if a job type needs just one container, job_types may return the following object with a path pointing to a Dockerfile template: class Plugin : def job_types ( self ) -> dict [ str , dict ]: return { 'python3:1.0.2' : { 'images' : [ { 'source' : 'jobtype' , 'dockerfile_path' : 'job-template.Dockerfile' , 'template' : True , }, ], }, } Another example of a \"dockerfile\" job type, where the Jobs provide its own image: class Plugin : def job_types ( self ) -> dict [ str , dict ]: return { 'dockerfile:1.0.0' : { 'images' : [ { 'source' : 'job' , 'dockerfile_path' : 'Dockerfile' , 'template' : False , }, ], }, } infrastructure_targets \u00b6 Infrastructure Targets (deployment targets for Jobs) are provided by infrastructure_targets method of the plugin. from typing import Any class Plugin : def infrastructure_targets ( self ) -> dict [ str , Any ]: \"\"\" Infrastructure Targets (deployment targets for Jobs) provided by this plugin Infrastructure Target should contain Job Deployer, Job Monitor and Job Logs Streamer. :return dict of infrastructure name -> an instance of lifecycle.infrastructure.model.InfrastructureTarget \"\"\" return {} infrastructure_targets hook expects instances of lifecycle.infrastructure.model.InfrastructureTarget . Here's the overview of the most important classes: Class lifecycle.infrastructure.model.InfrastructureTarget \u00b6 name: str | None - name of the infrastructure target job_deployer: JobDeployer | None - instance of lifecycle.deployer.base.JobDeployer , see below. job_monitor: JobMonitor | None - instance of lifecycle.monitor.base.JobMonitor , see below. logs_streamer: LogsStreamer | None - instance of lifecycle.monitor.base.LogsStreamer , see below. remote_gateway_url: str | None - Address of a remote Pub in case of \"remote gateway mode\". remote_gateway_token: str | None - Auth token for internal communication in case of \"remote gateway mode\". Class lifecycle.deployer.base.JobDeployer \u00b6 JobDeployer should contain the logic responsible for deploying jobs, deleting jobs and managing secrets. deploy_job - Deploy a Job from a manifest file from racetrack_client.manifest import Manifest from racetrack_commons.entities.dto import JobDto , JobFamilyDto from racetrack_commons.plugin.engine import PluginEngine from lifecycle.config import Config class JobDeployer : def deploy_job ( self , manifest : Manifest , config : Config , plugin_engine : PluginEngine , tag : str , runtime_env_vars : dict [ str , str ], family : JobFamilyDto , containers_num : int = 1 , runtime_secret_vars : dict [ str , str ] | None = None , ) -> JobDto : ... delete_job - Delete a Job based on its name class JobDeployer : def delete_job ( self , job_name : str , job_version : str ) -> None : ... job_exists - Tell whether a Job already exists or not class JobDeployer : def job_exists ( self , job_name : str , job_version : str ) -> bool : ... save_job_secrets - Create or update secrets needed to build and deploy a Job from lifecycle.deployer.secrets import JobSecrets class JobDeployer : def save_job_secrets ( self , job_name : str , job_version : str , job_secrets : JobSecrets , ) -> None : ... get_job_secrets - Retrieve secrets for building and deploying a Job from lifecycle.deployer.secrets import JobSecrets class JobDeployer : def get_job_secrets ( self , job_name : str , job_version : str , ) -> JobSecrets : ... Class lifecycle.monitor.base.JobMonitor \u00b6 JobMonitor implements the logic responsible for discovering workloads running in a cluster and monitoring their condition. list_jobs - List jobs deployed in a cluster from typing import Iterable from racetrack_commons.entities.dto import JobDto from lifecycle.config import Config class JobMonitor : def list_jobs ( self , config : Config ) -> Iterable [ JobDto ]: yield JobDto ( ... ) check_job_condition - Verify if deployed Job is really operational. If not, raise exception with reason from typing import Callable from racetrack_commons.entities.dto import JobDto class JobMonitor : def check_job_condition ( self , job : JobDto , deployment_timestamp : int = 0 , on_job_alive : Callable = None , logs_on_error : bool = True , ) -> None : \"\"\" Verify if deployed Job is really operational. If not, raise exception with reason :param job: job data :param deployment_timestamp: timestamp of deployment to verify if the running version is really the expected one If set to zero, checking version is skipped. :param on_job_alive: handler called when Job is live, but not ready yet (server running already, but still initializing) :param logs_on_error: if True, logs are read from the Job and returned in the exception in case of failure \"\"\" read_recent_logs - Return last output logs from a Job from racetrack_commons.entities.dto import JobDto class JobMonitor : def read_recent_logs ( self , job : JobDto , tail : int = 20 ) -> str : \"\"\" Return last output logs from a Job :param job: job data :param tail: number of recent lines to show :return logs output lines joined in a one string \"\"\" Class lifecycle.monitor.base.LogsStreamer \u00b6 LogsStreamer is responsible for producing logs from the jobs, setting up & tearing down sessions providing live logs stream. create_session - Start a session transmitting messages to client from typing import Callable class LogsStreamer : def create_session ( self , session_id : str , resource_properties : dict [ str , str ], on_next_line : Callable [[ str , str ], None ]) -> None : \"\"\" Start a session transmitting messages to client. Session should call `broadcast` method when next message arrives. :param session_id: ID of a client session to be referred when closing :param resource_properties: properties describing a resource to be monitored (job name, version, etc) :param on_next_line: callback for sending log messages to connected clients. Parameters: session_id: str, message: str \"\"\" close_session - Close session when a client disconnects class LogsStreamer : def close_session ( self , session_id : str ) -> None : ... markdown_docs \u00b6 Return documentation for this plugin in markdown format class Plugin : def markdown_docs ( self ) -> str | None : return \"# Plugin Reference\" post_job_delete \u00b6 Supplementary actions invoked after job is deleted from racetrack_commons.entities.dto import JobDto class Plugin : def post_job_delete ( self , job : JobDto , username_executor : str = None ): \"\"\" Supplementary actions invoked after job is deleted :param username_executor: username of the user who deleted the job \"\"\" run_action \u00b6 Call a supplementary action of a plugin from typing import Any class Plugin : def run_action ( self , ** kwargs ) -> Any : \"\"\"Call a supplementary action of a plugin\"\"\" validate_job_manifest \u00b6 Validate job's manifest in terms of job type specific parts from racetrack_client.manifest import Manifest class Plugin : def validate_job_manifest ( self , manifest : Manifest , job_type : str ): \"\"\" Validate job's manifest in terms of job type specific parts. :param manifest: job's manifest :param job_type: job type name with the version :raise Exception in case of validation error \"\"\"","title":"Developing plugins"},{"location":"docs/development/developing-plugins/#developing-plugins","text":"","title":"Developing plugins"},{"location":"docs/development/developing-plugins/#how-does-plugin-work","text":"When Racetrack starts up, it scans for plugins. Each plugin is a zip file, and the process for loading them goes like this: Extract the plugin code from the zip file. If the plugin includes a requirements.txt file, it installs the dependencies. Racetrack loads the plugin.py file, which should define a class called Plugin . This class is then instantiated and kept in internal plugins list. When certain customizable events occur, the plugin engine is notified. All the plugin hooks are then called, in order of their priority: Plugins have a default priority of 0. Plugins with the lowest priority get executed first. If multiple plugins have the same priority, they are executed in the order they were added in the configuration file.","title":"How does plugin work?"},{"location":"docs/development/developing-plugins/#creating-a-plugin","text":"Create a git repo, copy plugin_sample to it and modify the hooks implementation as you like. Use same virtualenv from racetrack repository (your plugin can use the same dependencies as Lifecycle does). Check out plugins-job-types.md to see how to create a job-type plugin.","title":"Creating a plugin"},{"location":"docs/development/developing-plugins/#using-additional-dependencies","text":"Plugins can use additional dependencies (Python packages). You can include them in a requirements.txt file. It will be installed using pip so you can use these dependencies in your plugin's code.","title":"Using additional dependencies"},{"location":"docs/development/developing-plugins/#create-a-plugin-manifest","text":"Create plugin-manifest.yaml file in a plugin directory. Basically, it contains the metadata of the plugin. It can have the following fields in YAML format: name ( required , string ) - name of the plugin version ( required , string ) - version of the plugin url (optional, string ) - a link to the plugin page priority (optional, integer ) - order in plugins sequence, lowest priority gets executed first. Integer field, 0 by default. category (optional, string ) - kind of the plugin, either 'infrastructure', 'job-type' or 'core' components (optional, list of strings ) - list of Racetrack components that the plugin should be running on, e.g. lifecycle , image-builder . If it's empty, plugins are loaded on all components. Example: name : skynet-watcher version : 1.2.3 url : https://github.com/TheRacetrack/racetrack priority : -1 category : core components : - lifecycle","title":"Create a plugin manifest"},{"location":"docs/development/developing-plugins/#building-a-plugin","text":"Local source code of the plugin can be turned into a ZIP file by means of a racetrack client tool. Install racetrack client: python3 -m pip install --upgrade racetrack-client Go to the directory where your plugin is located. Make sure the plugin version inside plugin-manifest.yaml is up-to-date. Run racetrack plugin bundle to turn a plugin into a ZIP file. Zipped plugin will be generated in a plugin directory. See the output to locate the outcome package. See plugin_sample for an example of a plugin.","title":"Building a plugin"},{"location":"docs/development/developing-plugins/#supported-hooks","text":"To add functionality to the plugin, you can implement one of the following hooks (Python methods):","title":"Supported hooks"},{"location":"docs/development/developing-plugins/#post_job_deploy","text":"post_job_deploy implements supplementary actions invoked after a job is deployed. from racetrack_client.manifest import Manifest from racetrack_commons.entities.dto import JobDto class Plugin : def post_job_deploy ( self , manifest : Manifest , job : JobDto , image_name : str , deployer_username : str = None ): ...","title":"post_job_deploy"},{"location":"docs/development/developing-plugins/#job_runtime_env_vars","text":"job_runtime_env_vars provides supplementary env vars dictionary added to runtime vars when deploying a Job. class Plugin : def job_runtime_env_vars ( self ) -> dict [ str , str ] | None : return { 'SKYNET_ENGAGED' : '1' , }","title":"job_runtime_env_vars"},{"location":"docs/development/developing-plugins/#job_types","text":"job_types method declares all Job types provided by this plugin. It returns a dictionary, where a key is a job type name with version (e.g. python3:1.0.2 ) and the value is a Job Type Definition object. Job Type Definition describes the details of the job images used by the job. It's a dictionary that has the key images , which is a list of Job Image Definition objects. If the job consists of a single container, the list will contain only one element. Job Image Definition is a dictionary with the following properties: dockerfile_path ( string ) - a relative path to the Dockerfile source ( string enum ) - it's either \"jobtype\" or \"job\" , depending on the location of the expected Dockerfile. Choose \"jobtype\" , if a Dockerfile is provided by a job type plugin, in a ZIP bundle. Choose \"job\" , if a Dockerfile will be provided by the Job itself, taken from the Job's repository. template ( boolean ) - whether Dockerfile is a template and contains variables to evaluate. For instance, if a job type needs just one container, job_types may return the following object with a path pointing to a Dockerfile template: class Plugin : def job_types ( self ) -> dict [ str , dict ]: return { 'python3:1.0.2' : { 'images' : [ { 'source' : 'jobtype' , 'dockerfile_path' : 'job-template.Dockerfile' , 'template' : True , }, ], }, } Another example of a \"dockerfile\" job type, where the Jobs provide its own image: class Plugin : def job_types ( self ) -> dict [ str , dict ]: return { 'dockerfile:1.0.0' : { 'images' : [ { 'source' : 'job' , 'dockerfile_path' : 'Dockerfile' , 'template' : False , }, ], }, }","title":"job_types"},{"location":"docs/development/developing-plugins/#infrastructure_targets","text":"Infrastructure Targets (deployment targets for Jobs) are provided by infrastructure_targets method of the plugin. from typing import Any class Plugin : def infrastructure_targets ( self ) -> dict [ str , Any ]: \"\"\" Infrastructure Targets (deployment targets for Jobs) provided by this plugin Infrastructure Target should contain Job Deployer, Job Monitor and Job Logs Streamer. :return dict of infrastructure name -> an instance of lifecycle.infrastructure.model.InfrastructureTarget \"\"\" return {} infrastructure_targets hook expects instances of lifecycle.infrastructure.model.InfrastructureTarget . Here's the overview of the most important classes:","title":"infrastructure_targets"},{"location":"docs/development/developing-plugins/#class-lifecycleinfrastructuremodelinfrastructuretarget","text":"name: str | None - name of the infrastructure target job_deployer: JobDeployer | None - instance of lifecycle.deployer.base.JobDeployer , see below. job_monitor: JobMonitor | None - instance of lifecycle.monitor.base.JobMonitor , see below. logs_streamer: LogsStreamer | None - instance of lifecycle.monitor.base.LogsStreamer , see below. remote_gateway_url: str | None - Address of a remote Pub in case of \"remote gateway mode\". remote_gateway_token: str | None - Auth token for internal communication in case of \"remote gateway mode\".","title":"Class lifecycle.infrastructure.model.InfrastructureTarget"},{"location":"docs/development/developing-plugins/#class-lifecycledeployerbasejobdeployer","text":"JobDeployer should contain the logic responsible for deploying jobs, deleting jobs and managing secrets. deploy_job - Deploy a Job from a manifest file from racetrack_client.manifest import Manifest from racetrack_commons.entities.dto import JobDto , JobFamilyDto from racetrack_commons.plugin.engine import PluginEngine from lifecycle.config import Config class JobDeployer : def deploy_job ( self , manifest : Manifest , config : Config , plugin_engine : PluginEngine , tag : str , runtime_env_vars : dict [ str , str ], family : JobFamilyDto , containers_num : int = 1 , runtime_secret_vars : dict [ str , str ] | None = None , ) -> JobDto : ... delete_job - Delete a Job based on its name class JobDeployer : def delete_job ( self , job_name : str , job_version : str ) -> None : ... job_exists - Tell whether a Job already exists or not class JobDeployer : def job_exists ( self , job_name : str , job_version : str ) -> bool : ... save_job_secrets - Create or update secrets needed to build and deploy a Job from lifecycle.deployer.secrets import JobSecrets class JobDeployer : def save_job_secrets ( self , job_name : str , job_version : str , job_secrets : JobSecrets , ) -> None : ... get_job_secrets - Retrieve secrets for building and deploying a Job from lifecycle.deployer.secrets import JobSecrets class JobDeployer : def get_job_secrets ( self , job_name : str , job_version : str , ) -> JobSecrets : ...","title":"Class lifecycle.deployer.base.JobDeployer"},{"location":"docs/development/developing-plugins/#class-lifecyclemonitorbasejobmonitor","text":"JobMonitor implements the logic responsible for discovering workloads running in a cluster and monitoring their condition. list_jobs - List jobs deployed in a cluster from typing import Iterable from racetrack_commons.entities.dto import JobDto from lifecycle.config import Config class JobMonitor : def list_jobs ( self , config : Config ) -> Iterable [ JobDto ]: yield JobDto ( ... ) check_job_condition - Verify if deployed Job is really operational. If not, raise exception with reason from typing import Callable from racetrack_commons.entities.dto import JobDto class JobMonitor : def check_job_condition ( self , job : JobDto , deployment_timestamp : int = 0 , on_job_alive : Callable = None , logs_on_error : bool = True , ) -> None : \"\"\" Verify if deployed Job is really operational. If not, raise exception with reason :param job: job data :param deployment_timestamp: timestamp of deployment to verify if the running version is really the expected one If set to zero, checking version is skipped. :param on_job_alive: handler called when Job is live, but not ready yet (server running already, but still initializing) :param logs_on_error: if True, logs are read from the Job and returned in the exception in case of failure \"\"\" read_recent_logs - Return last output logs from a Job from racetrack_commons.entities.dto import JobDto class JobMonitor : def read_recent_logs ( self , job : JobDto , tail : int = 20 ) -> str : \"\"\" Return last output logs from a Job :param job: job data :param tail: number of recent lines to show :return logs output lines joined in a one string \"\"\"","title":"Class lifecycle.monitor.base.JobMonitor"},{"location":"docs/development/developing-plugins/#class-lifecyclemonitorbaselogsstreamer","text":"LogsStreamer is responsible for producing logs from the jobs, setting up & tearing down sessions providing live logs stream. create_session - Start a session transmitting messages to client from typing import Callable class LogsStreamer : def create_session ( self , session_id : str , resource_properties : dict [ str , str ], on_next_line : Callable [[ str , str ], None ]) -> None : \"\"\" Start a session transmitting messages to client. Session should call `broadcast` method when next message arrives. :param session_id: ID of a client session to be referred when closing :param resource_properties: properties describing a resource to be monitored (job name, version, etc) :param on_next_line: callback for sending log messages to connected clients. Parameters: session_id: str, message: str \"\"\" close_session - Close session when a client disconnects class LogsStreamer : def close_session ( self , session_id : str ) -> None : ...","title":"Class lifecycle.monitor.base.LogsStreamer"},{"location":"docs/development/developing-plugins/#markdown_docs","text":"Return documentation for this plugin in markdown format class Plugin : def markdown_docs ( self ) -> str | None : return \"# Plugin Reference\"","title":"markdown_docs"},{"location":"docs/development/developing-plugins/#post_job_delete","text":"Supplementary actions invoked after job is deleted from racetrack_commons.entities.dto import JobDto class Plugin : def post_job_delete ( self , job : JobDto , username_executor : str = None ): \"\"\" Supplementary actions invoked after job is deleted :param username_executor: username of the user who deleted the job \"\"\"","title":"post_job_delete"},{"location":"docs/development/developing-plugins/#run_action","text":"Call a supplementary action of a plugin from typing import Any class Plugin : def run_action ( self , ** kwargs ) -> Any : \"\"\"Call a supplementary action of a plugin\"\"\"","title":"run_action"},{"location":"docs/development/developing-plugins/#validate_job_manifest","text":"Validate job's manifest in terms of job type specific parts from racetrack_client.manifest import Manifest class Plugin : def validate_job_manifest ( self , manifest : Manifest , job_type : str ): \"\"\" Validate job's manifest in terms of job type specific parts. :param manifest: job's manifest :param job_type: job type name with the version :raise Exception in case of validation error \"\"\"","title":"validate_job_manifest"},{"location":"docs/development/plugins-job-types/","text":"Creating Job Type plugins \u00b6 This document covers how to create your own job type plugin to extend Racetrack. Quickstart Step-by-step \u00b6 Create a git repository for your plugin Create a plugin manifest in a plugin subdirectory Write a wrapper for the software you are making a job type for A wrapper is a program that runs given piece of software, wraps it up in a web server, adds features to it (eg. metrics, swagger page) and forwards HTTP requests calling the wrapped code. Prepare the Job template dockerfile The Job template dockerfile is a Jinja2 template with the following variables, that gets built for each individual Job automatically by Racetrack. env_vars - dict with environment variables that should be assigned to the Job container manifest - whole Job Manifest object (see Job Manifest Schema ) git_version - version of the Job code taken from git repository deployed_by_racetrack_version - version of the Racetrack that has been used to build this image. The templated Dockerfile will be built by Racetrack. The final image should contain the wrapper code, the source code of a job as well as any individual logic that depends on the specific job manifest. Create an appropriate plugin.py plugin.py describes the Plugin class - to be considered a job type, your plugin.py must at minimum implements the job_types method as described here in the documentation of all available hooks. Create a .racetrackignore Files not needed for the plugin should be added to the .racetrackignore file. Bundle plugin into a zipfile with racetrack plugin bundle Wrapper Principles \u00b6 Every wrapper has to follow some rules: HTTP server MUST run on port 7000, address 0.0.0.0 . HTTP server MUST mount endpoints at /pub/job/{name}/{version} base URL, where {name} is the name of the job taken from JOB_NAME environment variable (it will be assigned by docker) and {version} should match any string (due to job can be accessed by explicit version or by latest alias). HTTP server MUST have /live and /ready endpoints returning 200 status code, once it's alive and ready to accept requests. /live endpoint MUST return {\"deployment_timestamp\": 1654779364} JSON object. \"deployment_timestamp\" integer value should be taken from JOB_DEPLOYMENT_TIMESTAMP environment variable (it will be set by docker). This is the timestamp of the deployment, it's needed to distinguish versions in case of asynchronous redeployment of the job. /live endpoint MAY contain other JSON fields as well. You MAY implement swagger documentation for your endpoints on root endpoint. You MAY implement /metrics endpoint for exposing Prometheus metrics. You MAY expose any other endpoints. Job MAY read its name from JOB_NAME environment variable applied by infrastructure plugin. Job MAY read its version from JOB_VERSION environment variable applied by infrastructure plugin. Job MAY read its manifest YAML from JOB_MANIFEST_YAML environment variable applied by infrastructure plugin. Calls from jobs to other jobs SHOULD be made by importing a dedicated function from the job type plugin's library ( example ). Be careful to isolate libraries / requirements installed by the user from the versions of the libraries used by the core wrapper. Example job type \u00b6 The Go job type is a fully featured job type maintained by the racetrack team that serves as an example job type that implements all features (including optional ones) provided by racetrack. A barebones quickstart version of said jobtype following the guide above would look as follows: 1. Create a git repository \u00b6 Create https://github.com/TheRacetrack/plugin-go-job-type . 2. Create a plugin manifest in a plugin subdirectory \u00b6 Create golang-job-type subdirectory, create plugin-manifest.yaml in it. It should look as follows: name : golang-job-type version : 1.3.0 url : https://github.com/TheRacetrack/plugin-go-job-type 3. Write a wrapper for running Go code \u00b6 Create the go_wrapper subdirectory in the golang-job-type subdirectory. It should look like: go_wrapper \u251c\u2500\u2500 go.mod \u251c\u2500\u2500 go.sum \u251c\u2500\u2500 handler \u2502 \u251c\u2500\u2500 go.mod \u2502 \u251c\u2500\u2500 go.sum \u2502 \u2514\u2500\u2500 perform.go \u251c\u2500\u2500 health.go \u251c\u2500\u2500 main.go \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 server.go go_wrapper/src/handler/ is for handling the user's code, it will be injected there by docker when building the image. It looks like this: // This is just a stub for IDE. // It gets replaced by user's Job code in wrappers/docker/golang/job-template.Dockerfile module stub go 1.16 require ( github . com / go - stack / stack v1 .8.1 // indirect github . com / inconshreveable / log15 v0 .0.0 - 20201112154412 - 8562 bdadbbac github . com / mattn / go - colorable v0 .1.12 // indirect ) go_wrapper/main.go contains the main function setting up the server: File `go_wrapper/main.go` package main import ( handler \"racetrack/job\" ) func main () { err := WrapAndServe ( handler . Perform ) if err != nil { panic ( err ) } } go_wrapper/server.go contains the function that starts the server and redirects calls to the perform function: File `go_wrapper/server.go` package main import ( \"encoding/json\" \"fmt\" \"net/http\" \"os\" \"github.com/gin-gonic/gin\" log \"github.com/inconshreveable/log15\" \"github.com/pkg/errors\" ) // WrapAndServe embeds given function in a HTTP server and listens for requests func WrapAndServe ( entrypoint EntrypointHandler ) error { performHandler := buildHandler ( entrypoint ) jobName := os . Getenv ( \"JOB_NAME\" ) // Serve endpoints at raw path (to facilitate debugging) and prefixed path (when accessed through PUB). // Accept any version so that job can be called by its many version names (\"latest\", \"1.x\"). baseUrls := [] string { fmt . Sprintf ( \"/pub/job/%s/:version\" , jobName ), \"\" , } gin . SetMode ( gin . ReleaseMode ) //Hide debug routings router := gin . New () router . Use ( gin . Recovery ()) for _ , baseUrl := range baseUrls { router . POST ( baseUrl + \"/api/v1/perform\" , performHandler ) router . GET ( baseUrl + \"/health\" , HealthHandler ) router . GET ( baseUrl + \"/live\" , LiveHandler ) router . GET ( baseUrl + \"/ready\" , ReadyHandler ) MountOpenApi ( router , baseUrl ) } router . Use ( gin . Logger ()) listenAddress := \"0.0.0.0:7000\" log . Info ( \"Listening on\" , log . Ctx { \"listenAddress\" : listenAddress , \"baseUrls\" : baseUrls , }) if err := router . Run ( listenAddress ); err != nil { log . Error ( \"Serving http\" , log . Ctx { \"error\" : err }) return errors . Wrap ( err , \"Failed to serve\" ) } return nil } type EntrypointHandler func ( input map [ string ] interface {}) ( interface {}, error ) func buildHandler ( entrypointHandler EntrypointHandler ) func ( c * gin . Context ) { return func ( c * gin . Context ) { log . Debug ( \"Perform request received\" ) var input map [ string ] interface {} err := json . NewDecoder ( c . Request . Body ). Decode ( & input ) if err != nil { http . Error ( c . Writer , err . Error (), http . StatusBadRequest ) return } output , err := entrypointHandler ( input ) if err != nil { http . Error ( c . Writer , err . Error (), http . StatusInternalServerError ) return } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } } func wrapHandler ( h http . Handler ) gin . HandlerFunc { return func ( c * gin . Context ) { h . ServeHTTP ( c . Writer , c . Request ) } } go_wrapper/health.go handles liveness and readiness probes: File `go_wrapper/health.go` package main import ( \"encoding/json\" \"os\" \"strconv\" \"github.com/gin-gonic/gin\" ) type HealthResponse struct { Service string `json:\"service\"` JobName string `json:\"job_name\"` JobVersion string `json:\"job_version\"` GitVersion string `json:\"git_version\"` DeployedByRacetrackVersion string `json:\"deployed_by_racetrack_version\"` Status string `json:\"status\"` DeploymentTimestamp int `json:\"deployment_timestamp\"` } type LiveResponse struct { Status string `json:\"status\"` DeploymentTimestamp int `json:\"deployment_timestamp\"` } type ReadyResponse struct { Status string `json:\"status\"` } func HealthHandler ( c * gin . Context ) { deploymentTimestamp , _ := strconv . Atoi ( os . Getenv ( \"JOB_DEPLOYMENT_TIMESTAMP\" )) output := & HealthResponse { Service : \"job\" , JobName : os . Getenv ( \"JOB_NAME\" ), JobVersion : os . Getenv ( \"JOB_VERSION\" ), GitVersion : os . Getenv ( \"GIT_VERSION\" ), DeployedByRacetrackVersion : os . Getenv ( \"DEPLOYED_BY_RACETRACK_VERSION\" ), DeploymentTimestamp : deploymentTimestamp , Status : \"pass\" , } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } func LiveHandler ( c * gin . Context ) { deploymentTimestamp , _ := strconv . Atoi ( os . Getenv ( \"JOB_DEPLOYMENT_TIMESTAMP\" )) output := & LiveResponse { Status : \"live\" , DeploymentTimestamp : deploymentTimestamp , } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } func ReadyHandler ( c * gin . Context ) { output := & ReadyResponse { Status : \"ready\" , } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } go_wrapper/go.mod and go_wrapper/go.sum are Go specific dependency files. 4 through 7: Put needed files in the golang-job-type subdirectory \u00b6 File `go-job-type/job-template.Dockerfile` FROM golang:1.20-alpine WORKDIR /src/go_wrapper # Copy wrapper code to the image & remove the stub that is about to be replaced # Note `COPY --from=jobtype` as we want to copy from the job type plugin files rather than the job files COPY --from = jobtype go_wrapper/. /src/go_wrapper/ RUN go get ./... && rm -rf /src/go_wrapper/handler CMD ./go_wrapper < /dev/null # Label image so the container can be identified as Job (for automated cleanup) LABEL racetrack-component = \"job\" # Setting environment variables from env_vars { % for env_key, env_value in env_vars.items () % } ENV {{ env_key }} \"{{ env_value }}\" { % endfor % } # Install additional libraries requested by user in its manifest # Note: package manager should be compliant with the base image we used earlier { % if manifest.system_dependencies and manifest.system_dependencies | length > 0 % } RUN apk add \\ {{ manifest.system_dependencies | join ( ' ' ) }} { % endif % } { % if manifest.jobtype_extra.gomod % } COPY \"{{ manifest.jobtype_extra.gomod }}\" /src/job/ RUN cd /src/job && go mod download { % endif % # Finally, copy the Job source code in the place where the wrapper expects it COPY . /src/go_wrapper/handler/ # Make sure directory is writable and build the executable RUN chmod -R a+rw /src/go_wrapper && cd /src/go_wrapper/ && go mod download # Build Go Job RUN go get ./... && go build -o go_wrapper # Set environment variables that are expected by Job executable ENV JOB_NAME \"{{ manifest.name }}\" ENV JOB_VERSION \"{{ manifest.version }}\" ENV GIT_VERSION \"{{ git_version }}\" ENV DEPLOYED_BY_RACETRACK_VERSION \"{{ deployed_by_racetrack_version }}\" File `go-job-type/plugin.py` class Plugin : def job_types ( self ) -> dict [ str , dict ]: \"\"\" Job types provided by this plugin :return dict of job type name (with version) mapped to a definition of images to build \"\"\" plugin_version : str = getattr ( self , 'plugin_manifest' ) . version return { f 'golang: { plugin_version } ' : { 'images' : [ { 'source' : 'jobtype' , 'dockerfile_path' : 'job-template.Dockerfile' , 'template' : True , }, ], }, } Finally, also create the .racetrackignore file: Makefile go.sum 8. Bundle plugin into a ZIP file \u00b6 Install the racetrack client python3 -m pip install --upgrade racetrack-client And then run racetrack plugin bundle in the go-job-type directory.","title":"Job type plugins"},{"location":"docs/development/plugins-job-types/#creating-job-type-plugins","text":"This document covers how to create your own job type plugin to extend Racetrack.","title":"Creating Job Type plugins"},{"location":"docs/development/plugins-job-types/#quickstart-step-by-step","text":"Create a git repository for your plugin Create a plugin manifest in a plugin subdirectory Write a wrapper for the software you are making a job type for A wrapper is a program that runs given piece of software, wraps it up in a web server, adds features to it (eg. metrics, swagger page) and forwards HTTP requests calling the wrapped code. Prepare the Job template dockerfile The Job template dockerfile is a Jinja2 template with the following variables, that gets built for each individual Job automatically by Racetrack. env_vars - dict with environment variables that should be assigned to the Job container manifest - whole Job Manifest object (see Job Manifest Schema ) git_version - version of the Job code taken from git repository deployed_by_racetrack_version - version of the Racetrack that has been used to build this image. The templated Dockerfile will be built by Racetrack. The final image should contain the wrapper code, the source code of a job as well as any individual logic that depends on the specific job manifest. Create an appropriate plugin.py plugin.py describes the Plugin class - to be considered a job type, your plugin.py must at minimum implements the job_types method as described here in the documentation of all available hooks. Create a .racetrackignore Files not needed for the plugin should be added to the .racetrackignore file. Bundle plugin into a zipfile with racetrack plugin bundle","title":"Quickstart Step-by-step"},{"location":"docs/development/plugins-job-types/#wrapper-principles","text":"Every wrapper has to follow some rules: HTTP server MUST run on port 7000, address 0.0.0.0 . HTTP server MUST mount endpoints at /pub/job/{name}/{version} base URL, where {name} is the name of the job taken from JOB_NAME environment variable (it will be assigned by docker) and {version} should match any string (due to job can be accessed by explicit version or by latest alias). HTTP server MUST have /live and /ready endpoints returning 200 status code, once it's alive and ready to accept requests. /live endpoint MUST return {\"deployment_timestamp\": 1654779364} JSON object. \"deployment_timestamp\" integer value should be taken from JOB_DEPLOYMENT_TIMESTAMP environment variable (it will be set by docker). This is the timestamp of the deployment, it's needed to distinguish versions in case of asynchronous redeployment of the job. /live endpoint MAY contain other JSON fields as well. You MAY implement swagger documentation for your endpoints on root endpoint. You MAY implement /metrics endpoint for exposing Prometheus metrics. You MAY expose any other endpoints. Job MAY read its name from JOB_NAME environment variable applied by infrastructure plugin. Job MAY read its version from JOB_VERSION environment variable applied by infrastructure plugin. Job MAY read its manifest YAML from JOB_MANIFEST_YAML environment variable applied by infrastructure plugin. Calls from jobs to other jobs SHOULD be made by importing a dedicated function from the job type plugin's library ( example ). Be careful to isolate libraries / requirements installed by the user from the versions of the libraries used by the core wrapper.","title":"Wrapper Principles"},{"location":"docs/development/plugins-job-types/#example-job-type","text":"The Go job type is a fully featured job type maintained by the racetrack team that serves as an example job type that implements all features (including optional ones) provided by racetrack. A barebones quickstart version of said jobtype following the guide above would look as follows:","title":"Example job type"},{"location":"docs/development/plugins-job-types/#1-create-a-git-repository","text":"Create https://github.com/TheRacetrack/plugin-go-job-type .","title":"1. Create a git repository"},{"location":"docs/development/plugins-job-types/#2-create-a-plugin-manifest-in-a-plugin-subdirectory","text":"Create golang-job-type subdirectory, create plugin-manifest.yaml in it. It should look as follows: name : golang-job-type version : 1.3.0 url : https://github.com/TheRacetrack/plugin-go-job-type","title":"2. Create a plugin manifest in a plugin subdirectory"},{"location":"docs/development/plugins-job-types/#3-write-a-wrapper-for-running-go-code","text":"Create the go_wrapper subdirectory in the golang-job-type subdirectory. It should look like: go_wrapper \u251c\u2500\u2500 go.mod \u251c\u2500\u2500 go.sum \u251c\u2500\u2500 handler \u2502 \u251c\u2500\u2500 go.mod \u2502 \u251c\u2500\u2500 go.sum \u2502 \u2514\u2500\u2500 perform.go \u251c\u2500\u2500 health.go \u251c\u2500\u2500 main.go \u251c\u2500\u2500 Makefile \u2514\u2500\u2500 server.go go_wrapper/src/handler/ is for handling the user's code, it will be injected there by docker when building the image. It looks like this: // This is just a stub for IDE. // It gets replaced by user's Job code in wrappers/docker/golang/job-template.Dockerfile module stub go 1.16 require ( github . com / go - stack / stack v1 .8.1 // indirect github . com / inconshreveable / log15 v0 .0.0 - 20201112154412 - 8562 bdadbbac github . com / mattn / go - colorable v0 .1.12 // indirect ) go_wrapper/main.go contains the main function setting up the server: File `go_wrapper/main.go` package main import ( handler \"racetrack/job\" ) func main () { err := WrapAndServe ( handler . Perform ) if err != nil { panic ( err ) } } go_wrapper/server.go contains the function that starts the server and redirects calls to the perform function: File `go_wrapper/server.go` package main import ( \"encoding/json\" \"fmt\" \"net/http\" \"os\" \"github.com/gin-gonic/gin\" log \"github.com/inconshreveable/log15\" \"github.com/pkg/errors\" ) // WrapAndServe embeds given function in a HTTP server and listens for requests func WrapAndServe ( entrypoint EntrypointHandler ) error { performHandler := buildHandler ( entrypoint ) jobName := os . Getenv ( \"JOB_NAME\" ) // Serve endpoints at raw path (to facilitate debugging) and prefixed path (when accessed through PUB). // Accept any version so that job can be called by its many version names (\"latest\", \"1.x\"). baseUrls := [] string { fmt . Sprintf ( \"/pub/job/%s/:version\" , jobName ), \"\" , } gin . SetMode ( gin . ReleaseMode ) //Hide debug routings router := gin . New () router . Use ( gin . Recovery ()) for _ , baseUrl := range baseUrls { router . POST ( baseUrl + \"/api/v1/perform\" , performHandler ) router . GET ( baseUrl + \"/health\" , HealthHandler ) router . GET ( baseUrl + \"/live\" , LiveHandler ) router . GET ( baseUrl + \"/ready\" , ReadyHandler ) MountOpenApi ( router , baseUrl ) } router . Use ( gin . Logger ()) listenAddress := \"0.0.0.0:7000\" log . Info ( \"Listening on\" , log . Ctx { \"listenAddress\" : listenAddress , \"baseUrls\" : baseUrls , }) if err := router . Run ( listenAddress ); err != nil { log . Error ( \"Serving http\" , log . Ctx { \"error\" : err }) return errors . Wrap ( err , \"Failed to serve\" ) } return nil } type EntrypointHandler func ( input map [ string ] interface {}) ( interface {}, error ) func buildHandler ( entrypointHandler EntrypointHandler ) func ( c * gin . Context ) { return func ( c * gin . Context ) { log . Debug ( \"Perform request received\" ) var input map [ string ] interface {} err := json . NewDecoder ( c . Request . Body ). Decode ( & input ) if err != nil { http . Error ( c . Writer , err . Error (), http . StatusBadRequest ) return } output , err := entrypointHandler ( input ) if err != nil { http . Error ( c . Writer , err . Error (), http . StatusInternalServerError ) return } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } } func wrapHandler ( h http . Handler ) gin . HandlerFunc { return func ( c * gin . Context ) { h . ServeHTTP ( c . Writer , c . Request ) } } go_wrapper/health.go handles liveness and readiness probes: File `go_wrapper/health.go` package main import ( \"encoding/json\" \"os\" \"strconv\" \"github.com/gin-gonic/gin\" ) type HealthResponse struct { Service string `json:\"service\"` JobName string `json:\"job_name\"` JobVersion string `json:\"job_version\"` GitVersion string `json:\"git_version\"` DeployedByRacetrackVersion string `json:\"deployed_by_racetrack_version\"` Status string `json:\"status\"` DeploymentTimestamp int `json:\"deployment_timestamp\"` } type LiveResponse struct { Status string `json:\"status\"` DeploymentTimestamp int `json:\"deployment_timestamp\"` } type ReadyResponse struct { Status string `json:\"status\"` } func HealthHandler ( c * gin . Context ) { deploymentTimestamp , _ := strconv . Atoi ( os . Getenv ( \"JOB_DEPLOYMENT_TIMESTAMP\" )) output := & HealthResponse { Service : \"job\" , JobName : os . Getenv ( \"JOB_NAME\" ), JobVersion : os . Getenv ( \"JOB_VERSION\" ), GitVersion : os . Getenv ( \"GIT_VERSION\" ), DeployedByRacetrackVersion : os . Getenv ( \"DEPLOYED_BY_RACETRACK_VERSION\" ), DeploymentTimestamp : deploymentTimestamp , Status : \"pass\" , } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } func LiveHandler ( c * gin . Context ) { deploymentTimestamp , _ := strconv . Atoi ( os . Getenv ( \"JOB_DEPLOYMENT_TIMESTAMP\" )) output := & LiveResponse { Status : \"live\" , DeploymentTimestamp : deploymentTimestamp , } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } func ReadyHandler ( c * gin . Context ) { output := & ReadyResponse { Status : \"ready\" , } c . Writer . Header (). Set ( \"Content-Type\" , \"application/json\" ) json . NewEncoder ( c . Writer ). Encode ( output ) } go_wrapper/go.mod and go_wrapper/go.sum are Go specific dependency files.","title":"3. Write a wrapper for running Go code"},{"location":"docs/development/plugins-job-types/#4-through-7-put-needed-files-in-the-golang-job-type-subdirectory","text":"File `go-job-type/job-template.Dockerfile` FROM golang:1.20-alpine WORKDIR /src/go_wrapper # Copy wrapper code to the image & remove the stub that is about to be replaced # Note `COPY --from=jobtype` as we want to copy from the job type plugin files rather than the job files COPY --from = jobtype go_wrapper/. /src/go_wrapper/ RUN go get ./... && rm -rf /src/go_wrapper/handler CMD ./go_wrapper < /dev/null # Label image so the container can be identified as Job (for automated cleanup) LABEL racetrack-component = \"job\" # Setting environment variables from env_vars { % for env_key, env_value in env_vars.items () % } ENV {{ env_key }} \"{{ env_value }}\" { % endfor % } # Install additional libraries requested by user in its manifest # Note: package manager should be compliant with the base image we used earlier { % if manifest.system_dependencies and manifest.system_dependencies | length > 0 % } RUN apk add \\ {{ manifest.system_dependencies | join ( ' ' ) }} { % endif % } { % if manifest.jobtype_extra.gomod % } COPY \"{{ manifest.jobtype_extra.gomod }}\" /src/job/ RUN cd /src/job && go mod download { % endif % # Finally, copy the Job source code in the place where the wrapper expects it COPY . /src/go_wrapper/handler/ # Make sure directory is writable and build the executable RUN chmod -R a+rw /src/go_wrapper && cd /src/go_wrapper/ && go mod download # Build Go Job RUN go get ./... && go build -o go_wrapper # Set environment variables that are expected by Job executable ENV JOB_NAME \"{{ manifest.name }}\" ENV JOB_VERSION \"{{ manifest.version }}\" ENV GIT_VERSION \"{{ git_version }}\" ENV DEPLOYED_BY_RACETRACK_VERSION \"{{ deployed_by_racetrack_version }}\" File `go-job-type/plugin.py` class Plugin : def job_types ( self ) -> dict [ str , dict ]: \"\"\" Job types provided by this plugin :return dict of job type name (with version) mapped to a definition of images to build \"\"\" plugin_version : str = getattr ( self , 'plugin_manifest' ) . version return { f 'golang: { plugin_version } ' : { 'images' : [ { 'source' : 'jobtype' , 'dockerfile_path' : 'job-template.Dockerfile' , 'template' : True , }, ], }, } Finally, also create the .racetrackignore file: Makefile go.sum","title":"4 through 7: Put needed files in the golang-job-type subdirectory"},{"location":"docs/development/plugins-job-types/#8-bundle-plugin-into-a-zip-file","text":"Install the racetrack client python3 -m pip install --upgrade racetrack-client And then run racetrack plugin bundle in the go-job-type directory.","title":"8. Bundle plugin into a ZIP file"},{"location":"docs/development/release/","text":"Releasing a version \u00b6 Versioning \u00b6 For docker tags on master and release branches (like cluster-test , cluster-preprod etc), we use <semver> versioning. Examples: 0.0.15 , 1.0.3 . Versions are bumped only when the new codebase has been tested, and images aren't overridden. For avoiding docker tag conflicts in dev branches (like cluster-dev , cluster-dev2 ) we extend this format to <semver>-<MR-number>-<dev-version> where MR-number stands for gitlab MR number, and dev-version is just sequentially increasing number. Examples: 0.0.15-125-1 , 1.0.3-142-11 . Release new changes of Racetrack to test or dev Cluster \u00b6 Do the following in order to apply your changes to your cluster: In racetrack repository: Increment version make version-bump MR=123 , where 123 is the id of your merge request. Build & push docker images by running: make version-release-private . In racetrack-config repository: Checkout to a branch related with your cluster. If new version involves changes in kustomize files, reset repository to corresponding branch. Run make version-pick VERSION=x.y.z with x.y.z being the version you just bumped. Commit & push to trigger redeployment in Kubernetes. You don't need to specify MR id for futher dev releases, because make version-bump will bump the dev part if MR is set in file, otherwise it bumps just the semver part. Releasing new Racetrack version \u00b6 Make sure CHANGELOG.md has all additions and changes since latest release. Determine new version number x.y.z according to Semver and latest changes: VERSION = x.y.z In changelog rename section \"Unreleased\" to x.y.z and add date, then add a new empty \"Unreleased\" section. Create release branch ie. release-x.y.z : git checkout -b release- $VERSION Increment version: make version-bump-exact VERSION = $VERSION Commit and push all changes from previous points: git commit -am \"Release version $VERSION \" Merge release-x.y.z branch to master : git checkout master && git merge release- $VERSION && git push Tag the resulting commit and push tag: git tag $VERSION && git push origin $VERSION Release racetrack client (if needed) with ( cd racetrack_client && make release-pypi ) Build & push docker images by running: make version-release-private version-release-public","title":"Releasing a version"},{"location":"docs/development/release/#releasing-a-version","text":"","title":"Releasing a version"},{"location":"docs/development/release/#versioning","text":"For docker tags on master and release branches (like cluster-test , cluster-preprod etc), we use <semver> versioning. Examples: 0.0.15 , 1.0.3 . Versions are bumped only when the new codebase has been tested, and images aren't overridden. For avoiding docker tag conflicts in dev branches (like cluster-dev , cluster-dev2 ) we extend this format to <semver>-<MR-number>-<dev-version> where MR-number stands for gitlab MR number, and dev-version is just sequentially increasing number. Examples: 0.0.15-125-1 , 1.0.3-142-11 .","title":"Versioning"},{"location":"docs/development/release/#release-new-changes-of-racetrack-to-test-or-dev-cluster","text":"Do the following in order to apply your changes to your cluster: In racetrack repository: Increment version make version-bump MR=123 , where 123 is the id of your merge request. Build & push docker images by running: make version-release-private . In racetrack-config repository: Checkout to a branch related with your cluster. If new version involves changes in kustomize files, reset repository to corresponding branch. Run make version-pick VERSION=x.y.z with x.y.z being the version you just bumped. Commit & push to trigger redeployment in Kubernetes. You don't need to specify MR id for futher dev releases, because make version-bump will bump the dev part if MR is set in file, otherwise it bumps just the semver part.","title":"Release new changes of Racetrack to test or dev Cluster"},{"location":"docs/development/release/#releasing-new-racetrack-version","text":"Make sure CHANGELOG.md has all additions and changes since latest release. Determine new version number x.y.z according to Semver and latest changes: VERSION = x.y.z In changelog rename section \"Unreleased\" to x.y.z and add date, then add a new empty \"Unreleased\" section. Create release branch ie. release-x.y.z : git checkout -b release- $VERSION Increment version: make version-bump-exact VERSION = $VERSION Commit and push all changes from previous points: git commit -am \"Release version $VERSION \" Merge release-x.y.z branch to master : git checkout master && git merge release- $VERSION && git push Tag the resulting commit and push tag: git tag $VERSION && git push origin $VERSION Release racetrack client (if needed) with ( cd racetrack_client && make release-pypi ) Build & push docker images by running: make version-release-private version-release-public","title":"Releasing new Racetrack version"},{"location":"docs/license/copyright-notices/","text":"This project uses Python standard library (unmodified) https://github.com/python/cpython Copyright (c) 1991 - 1995, Stichting Mathematisch Centrum Amsterdam, The Netherlands. All rights reserved. These files are available under Python License, which you can find at https://github.com/python/cpython/blob/main/LICENSE This project uses Go (unmodified) https://github.com/golang/go Copyright (c) 2009 The Go Authors. All rights reserved. These files are available under BSD 3-Clause License, which you can find at https://github.com/golang/go/blob/master/LICENSE This project uses Django (unmodified) https://github.com/django/django Copyright (c) Django Software Foundation and individual contributors. All rights reserved. These files are available under BSD 3-Clause License, which you can find at https://github.com/django/django/blob/main/LICENSE This project uses Bootstrap (unmodified) https://github.com/twbs/bootstrap Copyright (c) 2011-2022 Twitter, Inc. Copyright (c) 2011-2022 The Bootstrap Authors These files are available under MIT License, which you can find at https://github.com/twbs/bootstrap/blob/main/LICENSE This project uses vis-network (unmodified) https://github.com/visjs/vis-network Copyright (c) 2014-2017 Almende B.V. These files are available under MIT License, which you can find at https://github.com/visjs/vis-network/blob/master/LICENSE-MIT This project uses github-markdown-css (unmodified) https://github.com/sindresorhus/github-markdown-css Copyright (c) Sindre Sorhus sindresorhus@gmail.com (https://sindresorhus.com) These files are available under MIT License, which you can find at https://github.com/sindresorhus/github-markdown-css/blob/main/license This project uses Prism (unmodified) Copyright (c) 2012 Lea Verou https://github.com/PrismJS/prism/ These files are available under MIT License, which you can find at https://github.com/PrismJS/prism/blob/master/LICENSE This project uses python-socketio (unmodified) https://github.com/miguelgrinberg/python-socketio Copyright (c) 2015 Miguel Grinberg These files are available under MIT License, which you can find at https://github.com/miguelgrinberg/python-socketio/blob/main/LICENSE This project uses backoff (unmodified) https://github.com/litl/backoff Copyright (c) 2014 litl, LLC. These files are available under MIT License, which you can find at https://github.com/litl/backoff/blob/master/LICENSE This project uses Kubernetes Python Client (unmodified) https://github.com/kubernetes-client/python Copyright 2014 The Kubernetes Authors. These files are available under Apache License 2.0 License, which you can find at https://github.com/kubernetes-client/python/blob/master/LICENSE This project uses Prometheus Python Client (unmodified) https://github.com/prometheus/client_python These files are available under Apache License 2.0 License, which you can find at https://github.com/prometheus/client_python/blob/master/LICENSE This project uses Werkzeug (unmodified) https://github.com/pallets/werkzeug Copyright 2007 Pallets These files are available under BSD 3-Clause License, which you can find at https://github.com/pallets/werkzeug/blob/main/LICENSE.rst This project uses Python-Markdown (unmodified) https://github.com/Python-Markdown/markdown Copyright 2007, 2008 The Python Markdown Project (v. 1.7 and later) Copyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b) Copyright 2004 Manfred Stienstra (the original version) These files are available under License, which you can find at https://github.com/Python-Markdown/markdown/blob/master/LICENSE.md This project uses FastAPI (unmodified) https://github.com/tiangolo/fastapi Copyright (c) 2018 Sebasti\u00e1n Ram\u00edrez These files are available under MIT License, which you can find at https://github.com/tiangolo/fastapi/blob/master/LICENSE This project uses Kubectl (unmodified) https://github.com/kubernetes/kubectl These files are available under Apache License 2.0 License, which you can find at https://github.com/kubernetes/kubectl/blob/master/LICENSE This project uses pydantic (unmodified) https://github.com/pydantic/pydantic Copyright (c) 2017 - 2022 Samuel Colvin and other contributors These files are available under MIT License, which you can find at https://github.com/pydantic/pydantic/blob/main/LICENSE This project uses PyYAML (unmodified) https://github.com/yaml/pyyaml Copyright (c) 2017-2021 Ingy d\u00f6t Net Copyright (c) 2006-2016 Kirill Simonov These files are available under MIT License, which you can find at https://github.com/yaml/pyyaml/blob/master/LICENSE This project uses Jinja (unmodified) https://github.com/pallets/jinja/ Copyright 2007 Pallets These files are available under BSD 3-Clause License, which you can find at https://github.com/pallets/jinja/blob/main/LICENSE.rst This project uses Typer (unmodified) https://github.com/tiangolo/typer Copyright (c) 2019 Sebasti\u00e1n Ram\u00edrez These files are available under MIT License, which you can find at https://github.com/tiangolo/typer/blob/master/LICENSE This project uses Gin Web Framework (unmodified) https://github.com/gin-gonic/gin Copyright (c) 2014 Manuel Mart\u00ednez-Almeida These files are available under MIT License, which you can find at https://github.com/gin-gonic/gin/blob/master/LICENSE This project uses \"errors\" (unmodified) https://github.com/pkg/errors Copyright (c) 2015, Dave Cheney dave@cheney.net These files are available under BSD 2-Clause License, which you can find at https://github.com/pkg/errors/blob/master/LICENSE This project uses PyJWT (unmodified) https://github.com/jpadilla/pyjwt Copyright (c) 2015-2022 Jos\u00e9 Padilla These files are available under MIT License, which you can find at https://github.com/jpadilla/pyjwt/blob/master/LICENSE This project uses Uvicorn (unmodified) https://github.com/encode/uvicorn Copyright \u00a9 2017-present, Encode OSS Ltd. All rights reserved. These files are available under BSD 3-Clause License, which you can find at https://github.com/encode/uvicorn/blob/master/LICENSE.md This project uses Vite (unmodified) https://github.com/vitejs/vite Copyright (c) 2019-present, Yuxi (Evan) You and Vite contributors These files are available under MIT License, which you can find at https://github.com/vitejs/vite/blob/main/LICENSE This project uses Node.js (unmodified) https://github.com/nodejs/node Copyright Node.js contributors. All rights reserved. These files are available under License, which you can find at https://github.com/nodejs/node/blob/main/LICENSE This project uses nvm (unmodified) https://github.com/nvm-sh/nvm Copyright (c) 2010 Tim Caswell Copyright (c) 2014 Jordan Harband These files are available under MIT License, which you can find at https://github.com/nvm-sh/nvm/blob/master/LICENSE.md This project uses Vue.js (unmodified) https://github.com/vuejs/core Copyright (c) 2018-present, Yuxi (Evan) You These files are available under MIT License, which you can find at https://github.com/vuejs/core/blob/main/LICENSE This project uses vue-router (unmodified) https://github.com/vuejs/router Copyright (c) 2019-present Eduardo San Martin Morote These files are available under MIT License, which you can find at https://github.com/vuejs/router/blob/main/LICENSE This project uses vue-toastification (unmodified) https://github.com/Maronato/vue-toastification Copyright (c) 2019 maronato These files are available under MIT License, which you can find at https://github.com/Maronato/vue-toastification/blob/next/LICENCE This project uses Quasar Framework (unmodified) https://github.com/quasarframework/quasar Copyright (c) 2015-present Razvan Stoenescu These files are available under MIT License, which you can find at https://github.com/quasarframework/quasar/blob/dev/LICENSE This project uses Typescript (unmodified) https://github.com/microsoft/TypeScript These files are available under Apache 2.0 License, which you can find at https https://github.com/microsoft/TypeScript/blob/main/LICENSE.txt This project uses Axios (unmodified) https://github.com/axios/axios Copyright (c) 2014-present Matt Zabriskie & Collaborators These files are available under MIT License, which you can find at https://github.com/axios/axios/blob/v1.x/LICENSE This project uses js-yaml (unmodified) https://github.com/nodeca/js-yaml Copyright (C) 2011-2015 by Vitaly Puzrin These files are available under MIT License, which you can find at https://github.com/nodeca/js-yaml/blob/master/LICENSE This project uses Prometheus (unmodified) https://github.com/prometheus/prometheus These files are available under Apache 2.0 License, which you can find at https://github.com/prometheus/prometheus/blob/main/LICENSE This project uses Grafana (unmodified) https://github.com/grafana/grafana Copyright (C) 2007 Free Software Foundation, Inc. These files are available under GNU Affero General Public License v3.0, which you can find at https://github.com/grafana/grafana/blob/main/LICENSE This project uses PgBouncer (unmodified) https://github.com/pgbouncer/pgbouncer Copyright (c) 2007-2009 Marko Kreen, Skype Technologies O\u00dc These files are available under ISC License, which you can find at https://github.com/pgbouncer/pgbouncer/blob/master/COPYRIGHT This project uses PostgreSQL Server Exporter (unmodified) https://github.com/prometheus-community/postgres_exporter These files are available under Apache License 2.0, which you can find at https://github.com/prometheus-community/postgres_exporter/blob/master/LICENSE","title":"Copyright Notices"},{"location":"docs/user/async-job-calls/","text":"Asynchronous calls to jobs \u00b6 Classic calls to the jobs are made synchronously. While it's fine for short requests, it's not suitable for the very long requests, which may end with the timeout errors, imposed by the proxy in-between. Synchronous calls are still recommended for short requests (up to 30 seconds), especially for serving static resources or webview pages. For long requests, it might be better to use the asynchronous approach described below. Asynchronous call to a job \u00b6 Racetrack exposes special endpoints for making asynchronous calls to the jobs. Instead of waiting for a long request to complete, a client can initiate a new task and immediately receive a task ID. The client then makes another request, this time to a different endpoint, to check the status of the task using the given ID. The task is processed in the background by Racetrack, utilizing the same classic synchronous call to a job under the hood. This means no modifications are needed on the job's end. This is an opt-in feature for clients, classic synchronous calls are still be available. The main idea is to split a long request into 2 parts: Start a new task - a user makes one request to start a task and get the immediate response with the ID of the task. The task is then scheduled and processed by a job. Check on the result - a user checks in a loop if a requested task is already done. If so, he gets a response. The latter endpoint uses the HTTP Long Polling technique, which makes client to wait until task is completed or timeout is reached. Client gets notified as soon as the task is completed, without the need to poll the server periodically. If a timeout occurs or the connection is lost for any reason, the client can simply resend the request without any loss, as the final result can still be retrieved using the task's ID. Using long polling instead of the periodic polling (eg. every 5 seconds) reduces the number of requests and the load on the server. Plus, client gets a real-time response, as soon as the task is completed. API reference \u00b6 Pub component (Racetrack's gateway) will expose 2 new endpoints for making the asynchronous calls to the jobs (beside the classic synchronous endpoint): 1. Starting async task \u00b6 This endpoint starts a new task for calling a job, and responds with a JSON containing the unique ID of the task. Request \u00b6 POST /pub/async/new/job/{job_name}/{job_version}/{job_path} Path parameters: - job_name ( string ) - the name of the job, eg. adder - job_version ( string ) - the version of the job, eg. 1.0.0 or latest - job_path ( string ) - the path to the job, eg. api/v1/perform Body ( application/json ): - input parameters to the job (the same as for the classic synchronous call) Response \u00b6 201 Created - if task has been created successfully. Response is the JSON payload with the following fields: task_id ( string ) - a unique UUID identifier of the task 500 Internal Server Error - the task has failed. Details of an error will be included in the HTTP body. 503 Service Unavailable - Racetrack is currently in maintenance mode. Try again later. Request / Response Sample \u00b6 curl --request POST \\ --url http://127.0.0.1:7005/pub/async/new/job/adder/1.0.0/api/v1/perform \\ --header 'Content-Type: application/json' \\ --header 'X-Racetrack-Auth: eyJhbGciOiJIUz' \\ --data '{ \"numbers\": [40, 2] }' HTTP/1.1 201 Created Content-Type: application/json { \"task_id\": \"16b472ec-77c7-464a-932c-1cb2efc3e728\" } 2. Polling for the result \u00b6 This endpoint checks the status of the task and responds with a JSON containing the result of the task. Request \u00b6 GET /pub/async/task/{task_id}/poll Path parameters: - task_id ( string ) - the unique UUID identifier of the task, received from the previous endpoint Response \u00b6 200 OK - the task has completed successfully, the response payload contains the result of the job call in the JSON format (in the same format returned by the classic synchronous call) with the original HTTP status code and the headers. 202 Accepted , 408 Request Timeout or 504 Gateway Timeout - the task is still in progress, the client should repeat the request 404 Not Found - This error indicates that the task ID provided is invalid or the task is not found. 500 Internal Server Error - the task has failed. Details of an error will be included in the HTTP body in the JSON format with the following fields: error ( string ) - description of the error cause 503 Service Unavailable - Racetrack is currently in maintenance mode. Try again later. Request / Response Sample \u00b6 curl --request GET \\ --url http://127.0.0.1:7005/pub/async/task/16b472ec-77c7-464a-932c-1cb2efc3e728/poll \\ --header 'X-Racetrack-Auth: eyJhbGciOiJIUz' HTTP/1.1 200 OK Content-Type: application/json 42 3. Checking status of the task \u00b6 This is an optional endpoint for checking the current status of the task. This endpoint should return immediately, without waiting for the task to complete. Request \u00b6 GET /pub/async/task/{task_id}/status Path parameters: - task_id ( string ) - the unique UUID identifier of the task Response \u00b6 200 OK - if task exists. Response is the JSON payload with the following fields: task_id ( string ) - a unique UUID identifier of the task status ( string ) - current status of a task. Can be one of the following: ongoing - the task is still in progress completed - the task has successfully completed and it's ready to retrieve the result failed - the task has failed. Error details can be checked at the polling endpoint. 404 Not Found - This indicates that the task ID provided is invalid or the task is not found. Request / Response Sample \u00b6 curl --request GET \\ --url http://127.0.0.1:7005/pub/async/task/16b472ec-77c7-464a-932c-1cb2efc3e728/status \\ --header 'X-Racetrack-Auth: eyJhbGciOiJIUz' HTTP/1.1 200 OK Content-Type: application/json { \"task_id\": \"16b472ec-77c7-464a-932c-1cb2efc3e728\", \"status\": \"ongoing\" } Sample client implementation in Python \u00b6 pip install httpx import httpx pub_url = 'http://127.0.0.1:7005' job_name = 'windows12' job_version = 'latest' job_path = 'api/v1/perform' payload = { \"mode\" : \"timeout\" , \"t\" : 121 } auth_token = 'eyJhbGciOiJ...' # 1. Start async task response = httpx . post ( f ' { pub_url } /pub/async/new/job/ { job_name } / { job_version } / { job_path } ' , headers = { 'X-Racetrack-Auth' : auth_token , }, json = payload ) task_id : str = response . json ()[ 'task_id' ] # 2. Wait for the result while True : print ( 'Are we there yet?...' ) try : response = httpx . get ( f ' { pub_url } /pub/async/task/ { task_id } /poll' , timeout = httpx . Timeout ( 5 , read = 60 )) except httpx . ReadTimeout as e : print ( 'Read timeout' ) continue if response . status_code == 200 : break elif response . status_code in { 202 , 408 , 504 }: print ( f 'Response { response . status_code } { response . reason_phrase } ' ) continue else : raise RuntimeError ( f 'Response error: { response } ' ) result = response . json () Details \u00b6 To provide the result of the task to the client, Racetrack has to keep the result in memory for a certain period of time. Additionaly, it will store the records of the ongoing tasks in the database, to continue working on them after a server restart (e.g. due to the upgrade). The result are removed from the memory (and the database) right after the client retrieves the result. So that it can't be retrieved again . To accommodate any potential timeouts during the retrieval process, we allow a grace period of 10 seconds post-retrieval before the result is eventually removed. The task's result will be removed from the memory (and the database) after a certain period of time, even if the client didn't retrieve it. This timeout is set to 120 minutes . Racetrack doesn't keep the ongoing tasks that stuck forever on the job's end. There is a hard timeout for the task, set to 120 minutes . If the Racetrack servers are restarted (for instance, during an upgrade), the ongoing tasks will be resumed. Multiple replicas of the Pub component (Racetrack's gateway) communicate with each other to share the tasks and the results, so the result can be retrieved from any of them. The timeout for long polling is configured to 30 seconds . This implies that the server will terminate the polling endpoint after 30 seconds of no activity. Consequently, the client will renew the request every 30 seconds in a loop, until the task is completed. If a connection to a job is lost (eg. due to Out of Memory kill), the task will be retried automatically until reaching the maximum number of attempts (set to 2 by default).","title":"Asynchronous calls to jobs"},{"location":"docs/user/async-job-calls/#asynchronous-calls-to-jobs","text":"Classic calls to the jobs are made synchronously. While it's fine for short requests, it's not suitable for the very long requests, which may end with the timeout errors, imposed by the proxy in-between. Synchronous calls are still recommended for short requests (up to 30 seconds), especially for serving static resources or webview pages. For long requests, it might be better to use the asynchronous approach described below.","title":"Asynchronous calls to jobs"},{"location":"docs/user/async-job-calls/#asynchronous-call-to-a-job","text":"Racetrack exposes special endpoints for making asynchronous calls to the jobs. Instead of waiting for a long request to complete, a client can initiate a new task and immediately receive a task ID. The client then makes another request, this time to a different endpoint, to check the status of the task using the given ID. The task is processed in the background by Racetrack, utilizing the same classic synchronous call to a job under the hood. This means no modifications are needed on the job's end. This is an opt-in feature for clients, classic synchronous calls are still be available. The main idea is to split a long request into 2 parts: Start a new task - a user makes one request to start a task and get the immediate response with the ID of the task. The task is then scheduled and processed by a job. Check on the result - a user checks in a loop if a requested task is already done. If so, he gets a response. The latter endpoint uses the HTTP Long Polling technique, which makes client to wait until task is completed or timeout is reached. Client gets notified as soon as the task is completed, without the need to poll the server periodically. If a timeout occurs or the connection is lost for any reason, the client can simply resend the request without any loss, as the final result can still be retrieved using the task's ID. Using long polling instead of the periodic polling (eg. every 5 seconds) reduces the number of requests and the load on the server. Plus, client gets a real-time response, as soon as the task is completed.","title":"Asynchronous call to a job"},{"location":"docs/user/async-job-calls/#api-reference","text":"Pub component (Racetrack's gateway) will expose 2 new endpoints for making the asynchronous calls to the jobs (beside the classic synchronous endpoint):","title":"API reference"},{"location":"docs/user/async-job-calls/#1-starting-async-task","text":"This endpoint starts a new task for calling a job, and responds with a JSON containing the unique ID of the task.","title":"1. Starting async task"},{"location":"docs/user/async-job-calls/#request","text":"POST /pub/async/new/job/{job_name}/{job_version}/{job_path} Path parameters: - job_name ( string ) - the name of the job, eg. adder - job_version ( string ) - the version of the job, eg. 1.0.0 or latest - job_path ( string ) - the path to the job, eg. api/v1/perform Body ( application/json ): - input parameters to the job (the same as for the classic synchronous call)","title":"Request"},{"location":"docs/user/async-job-calls/#response","text":"201 Created - if task has been created successfully. Response is the JSON payload with the following fields: task_id ( string ) - a unique UUID identifier of the task 500 Internal Server Error - the task has failed. Details of an error will be included in the HTTP body. 503 Service Unavailable - Racetrack is currently in maintenance mode. Try again later.","title":"Response"},{"location":"docs/user/async-job-calls/#request-response-sample","text":"curl --request POST \\ --url http://127.0.0.1:7005/pub/async/new/job/adder/1.0.0/api/v1/perform \\ --header 'Content-Type: application/json' \\ --header 'X-Racetrack-Auth: eyJhbGciOiJIUz' \\ --data '{ \"numbers\": [40, 2] }' HTTP/1.1 201 Created Content-Type: application/json { \"task_id\": \"16b472ec-77c7-464a-932c-1cb2efc3e728\" }","title":"Request / Response Sample"},{"location":"docs/user/async-job-calls/#2-polling-for-the-result","text":"This endpoint checks the status of the task and responds with a JSON containing the result of the task.","title":"2. Polling for the result"},{"location":"docs/user/async-job-calls/#request_1","text":"GET /pub/async/task/{task_id}/poll Path parameters: - task_id ( string ) - the unique UUID identifier of the task, received from the previous endpoint","title":"Request"},{"location":"docs/user/async-job-calls/#response_1","text":"200 OK - the task has completed successfully, the response payload contains the result of the job call in the JSON format (in the same format returned by the classic synchronous call) with the original HTTP status code and the headers. 202 Accepted , 408 Request Timeout or 504 Gateway Timeout - the task is still in progress, the client should repeat the request 404 Not Found - This error indicates that the task ID provided is invalid or the task is not found. 500 Internal Server Error - the task has failed. Details of an error will be included in the HTTP body in the JSON format with the following fields: error ( string ) - description of the error cause 503 Service Unavailable - Racetrack is currently in maintenance mode. Try again later.","title":"Response"},{"location":"docs/user/async-job-calls/#request-response-sample_1","text":"curl --request GET \\ --url http://127.0.0.1:7005/pub/async/task/16b472ec-77c7-464a-932c-1cb2efc3e728/poll \\ --header 'X-Racetrack-Auth: eyJhbGciOiJIUz' HTTP/1.1 200 OK Content-Type: application/json 42","title":"Request / Response Sample"},{"location":"docs/user/async-job-calls/#3-checking-status-of-the-task","text":"This is an optional endpoint for checking the current status of the task. This endpoint should return immediately, without waiting for the task to complete.","title":"3. Checking status of the task"},{"location":"docs/user/async-job-calls/#request_2","text":"GET /pub/async/task/{task_id}/status Path parameters: - task_id ( string ) - the unique UUID identifier of the task","title":"Request"},{"location":"docs/user/async-job-calls/#response_2","text":"200 OK - if task exists. Response is the JSON payload with the following fields: task_id ( string ) - a unique UUID identifier of the task status ( string ) - current status of a task. Can be one of the following: ongoing - the task is still in progress completed - the task has successfully completed and it's ready to retrieve the result failed - the task has failed. Error details can be checked at the polling endpoint. 404 Not Found - This indicates that the task ID provided is invalid or the task is not found.","title":"Response"},{"location":"docs/user/async-job-calls/#request-response-sample_2","text":"curl --request GET \\ --url http://127.0.0.1:7005/pub/async/task/16b472ec-77c7-464a-932c-1cb2efc3e728/status \\ --header 'X-Racetrack-Auth: eyJhbGciOiJIUz' HTTP/1.1 200 OK Content-Type: application/json { \"task_id\": \"16b472ec-77c7-464a-932c-1cb2efc3e728\", \"status\": \"ongoing\" }","title":"Request / Response Sample"},{"location":"docs/user/async-job-calls/#sample-client-implementation-in-python","text":"pip install httpx import httpx pub_url = 'http://127.0.0.1:7005' job_name = 'windows12' job_version = 'latest' job_path = 'api/v1/perform' payload = { \"mode\" : \"timeout\" , \"t\" : 121 } auth_token = 'eyJhbGciOiJ...' # 1. Start async task response = httpx . post ( f ' { pub_url } /pub/async/new/job/ { job_name } / { job_version } / { job_path } ' , headers = { 'X-Racetrack-Auth' : auth_token , }, json = payload ) task_id : str = response . json ()[ 'task_id' ] # 2. Wait for the result while True : print ( 'Are we there yet?...' ) try : response = httpx . get ( f ' { pub_url } /pub/async/task/ { task_id } /poll' , timeout = httpx . Timeout ( 5 , read = 60 )) except httpx . ReadTimeout as e : print ( 'Read timeout' ) continue if response . status_code == 200 : break elif response . status_code in { 202 , 408 , 504 }: print ( f 'Response { response . status_code } { response . reason_phrase } ' ) continue else : raise RuntimeError ( f 'Response error: { response } ' ) result = response . json ()","title":"Sample client implementation in Python"},{"location":"docs/user/async-job-calls/#details","text":"To provide the result of the task to the client, Racetrack has to keep the result in memory for a certain period of time. Additionaly, it will store the records of the ongoing tasks in the database, to continue working on them after a server restart (e.g. due to the upgrade). The result are removed from the memory (and the database) right after the client retrieves the result. So that it can't be retrieved again . To accommodate any potential timeouts during the retrieval process, we allow a grace period of 10 seconds post-retrieval before the result is eventually removed. The task's result will be removed from the memory (and the database) after a certain period of time, even if the client didn't retrieve it. This timeout is set to 120 minutes . Racetrack doesn't keep the ongoing tasks that stuck forever on the job's end. There is a hard timeout for the task, set to 120 minutes . If the Racetrack servers are restarted (for instance, during an upgrade), the ongoing tasks will be resumed. Multiple replicas of the Pub component (Racetrack's gateway) communicate with each other to share the tasks and the results, so the result can be retrieved from any of them. The timeout for long polling is configured to 30 seconds . This implies that the server will terminate the polling endpoint after 30 seconds of no activity. Consequently, the client will renew the request every 30 seconds in a loop, until the task is completed. If a connection to a job is lost (eg. due to Out of Memory kill), the task will be retried automatically until reaching the maximum number of attempts (set to 2 by default).","title":"Details"},{"location":"docs/user/available-plugins/","text":"Available plugins \u00b6 Racetrack instance might be enriched by plugins that add customized, tailored functionality to a generic Racetrack. These are the known, public Racetrack plugins that are commonly available to be installed: Job types: Python 3 Job Type racetrack plugin install github.com/TheRacetrack/plugin-python-job-type Dockerfile-based Job Type (any language or app wrapped in a Dockerfile) racetrack plugin install github.com/TheRacetrack/plugin-dockerfile-job-type Dockerfile Proxy Job Type (app wrapped in a Dockerfile like Drupal or Sphinx with a proxy server that adheres to the Racetrack requirements) racetrack plugin install github.com/TheRacetrack/plugin-docker-proxy-job-type Golang (Go) Job Type racetrack plugin install github.com/TheRacetrack/plugin-go-job-type Rust Job Type racetrack plugin install github.com/TheRacetrack/plugin-rust-job-type HUGO Job Type racetrack plugin install github.com/TheRacetrack/plugin-hugo-job-type Infrastructure Targets: Docker infrastructure - deploys to a local (in-place) Docker racetrack plugin install github.com/TheRacetrack/plugin-docker-infrastructure Kubernetes infrastructure - deploys to local Kubernetes racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure Remote Docker - deploys to remote Docker Daemon racetrack plugin install github.com/TheRacetrack/plugin-remote-docker Remote Kubernetes - deploys to remote Kubernetes racetrack plugin install github.com/TheRacetrack/plugin-remote-kubernetes Others: Teams notifications - Sending notifications to Teams channel racetrack plugin install github.com/TheRacetrack/plugin-teams-notifications What's next? \u00b6 How to install a plugin","title":"Available plugins"},{"location":"docs/user/available-plugins/#available-plugins","text":"Racetrack instance might be enriched by plugins that add customized, tailored functionality to a generic Racetrack. These are the known, public Racetrack plugins that are commonly available to be installed: Job types: Python 3 Job Type racetrack plugin install github.com/TheRacetrack/plugin-python-job-type Dockerfile-based Job Type (any language or app wrapped in a Dockerfile) racetrack plugin install github.com/TheRacetrack/plugin-dockerfile-job-type Dockerfile Proxy Job Type (app wrapped in a Dockerfile like Drupal or Sphinx with a proxy server that adheres to the Racetrack requirements) racetrack plugin install github.com/TheRacetrack/plugin-docker-proxy-job-type Golang (Go) Job Type racetrack plugin install github.com/TheRacetrack/plugin-go-job-type Rust Job Type racetrack plugin install github.com/TheRacetrack/plugin-rust-job-type HUGO Job Type racetrack plugin install github.com/TheRacetrack/plugin-hugo-job-type Infrastructure Targets: Docker infrastructure - deploys to a local (in-place) Docker racetrack plugin install github.com/TheRacetrack/plugin-docker-infrastructure Kubernetes infrastructure - deploys to local Kubernetes racetrack plugin install github.com/TheRacetrack/plugin-kubernetes-infrastructure Remote Docker - deploys to remote Docker Daemon racetrack plugin install github.com/TheRacetrack/plugin-remote-docker Remote Kubernetes - deploys to remote Kubernetes racetrack plugin install github.com/TheRacetrack/plugin-remote-kubernetes Others: Teams notifications - Sending notifications to Teams channel racetrack plugin install github.com/TheRacetrack/plugin-teams-notifications","title":"Available plugins"},{"location":"docs/user/available-plugins/#whats-next","text":"How to install a plugin","title":"What's next?"},{"location":"docs/user/user-guide-1/","text":"User Manual \u00b6 What is Racetrack? \u00b6 Racetrack is a system which transforms your code to in-operation workloads, e.g. Kubernetes workloads. You write your code - say for a ML model or micro-service - in a specific style, you hand it over to Racetrack, and a minute later it is in production. Racetrack allows you to use several languages and frameworks (provided by installed plugins ), for example: Standard Python 3 Golang services Rust And for exotic edge cases, any Dockerfile-wrapped code HUGO framework Drupal Sphinx Quake III Racetrack can be extended to introduce new languages and frameworks . What distinguishes Racetrack? \u00b6 You only supply your function's logic . No need to write repetitive API code, setting up webservers, creating dockerfiles, kubernetes YAMLs, and so on. Racetrack takes care of that for you. Language agnostic . Deploy code written in Python 3, Go, Rust, or anything else encapsulated in a Dockerfile. Infrastructure independent . Deploy to either a Kubernetes cluster or a Docker environment in a single transparent step. Server-side building . Code is transformed into a microservice without your computer being involved. Customizable through plugins , Immutable jobs Reproducible jobs . Racetrack ensures that anyone else can deploy the same job effortlessly. Out-of-the-box tools : web endpoints, API documentation, metrics, monitoring, tracing, and more. Released under a permissive open-source license , Suitable for both on-premises and cloud environments. Architecture and Terminology \u00b6 The following terms recur through this documentation and describe the elements and actions involved in using Racetrack: Job code : your code, written in the style required by Racetrack Job Type : which of the languages and frameworks supported by Racetrack you choose to develop your job in Job submission : when you're happy with your code, and you push it to Racetrack to be deployed Job : when a job code is submitted to Racetrack, Racetrack converts it to a workload and deploys it. This workload is called a Job Convention : when you pick your Job Type, you will be asked to follow a specific style for that type which Racetrack understands; that style is a Convention Manifest : a YAML file in the root of your Job, which specifies the job type and provides configuration values for your Job according to that Job Type To tie all of these terms together: As a data scientist, I selected the Python 3 Job Type to develop my ML model Job in. I wrote my Job code according to the Job Type convention . I composed a Manifest in which I specified the Job Type I used, and in it I tweaked a few specific parameters for this Job Type. I submitted the job code to Racetrack, after which it was deployed as a Job . Conventions \u00b6 For Racetrack to convert your Job to a Job, you have to follow a specific style for your Job Type: a Convention. Broadly speaking, the purpose of this convention is to guide Racetrack: \"Look Racetrack! Right here is the method to query my model; and right here is the method for retraining.\" You write some code according to a set of conventions, submit it to Racetrack, and a short while after, your code is in operation. This convention is a key aspect of Racetrack: you can think of it as a \"way the code should look\" for Racetrack to understand it and deploy it to production. You can browse the documentation for the supported Job Types in their dedicated repositories. In there, the Convention for each Job Type is described. Conventions for any Job Type are simple, easy to follow, and very few. Some examples are: For the Python 3 Job Type, you must have a class JobEntrypoint with a method perform() for the Python 3 function which receives input and gives output (e.g. receiving a vector, and returning it normalized). Racetrack will then know to wrap this in an HTTP server and expose it. For the Go Job Type, your main function must have the following signature: func Perform(args []interface{}, kwargs map[string]interface{}) (interface{}, error) . As a concrete example, if you have written a clever Python 3 function which adds two integers and returns the result, your code pre-Racetrack would look like this: # file: model.py def add_em_up ( x , y ): z = x + y return z And refactored to meet the Racetrack Python 3 Job Type Convention: # file: job_entrypoint.py class JobEntrypoint : def perform ( self , x , y ): return add_em_up ( x , y ) def add_em_up ( x , y ): z = x + y return z The Manifest \u00b6 Having picked our Job Type and followed its Convention, the only thing we're missing is to inform Racetrack what Job Type we're submitting, and apply any special configuration supported by that Job Type. We do this by creating a file named job.yaml in the root directory of our Job source tree. This file is YAML formatted. The documentation for each Job Type describes how it should be formed. A typical job.yaml will look like this: name : my_fantabulous_skynet_AI owner_email : nobody@example.com jobtype : python3:latest # this would be your Job Type git : remote : https://github.com/racetrack/supersmart-model branch : master jobtype_extra : requirements_path : 'supersmart/requirements.txt' entrypoint_path : 'job_entrypoint.py' resources : memory_max : 2Gi So in a way, the Manifest is to Racetrack what the Dockerfile is to Docker. Some clever things that can be controlled using the Manifest is installing extra packages, or processing a Python requirements.txt file. It is recommended to declare maximum memory amount explicitly by defining resources : memory_max : 1Gi Please refer to the more comprehensive section The Job Manifest File for more detail. Submitting a Job \u00b6 Racetrack Jobs are deployed to operation; that means, they are sent off from your development computer to run on a server somewhere. This sending is in Racetrack terminology called Job \"submission\". As you will experience in the CLI client , Racetrack provides a command line client you can install, and which handles this Submission for you. When operating with Racetrack (either local instance or production server), the Racetrack command line client will need authentication. What's next? \u00b6 User Guide - Deploying a Job Quickstart - quickly setup Racetrack on local Docker engine. Local Kubernetes Setup - run Racetrack on KinD - longer, but more comprehensive guide. Available plugins Job Manifest File Schema Installation to standalone host","title":"User Manual"},{"location":"docs/user/user-guide-1/#user-manual","text":"","title":"User Manual"},{"location":"docs/user/user-guide-1/#what-is-racetrack","text":"Racetrack is a system which transforms your code to in-operation workloads, e.g. Kubernetes workloads. You write your code - say for a ML model or micro-service - in a specific style, you hand it over to Racetrack, and a minute later it is in production. Racetrack allows you to use several languages and frameworks (provided by installed plugins ), for example: Standard Python 3 Golang services Rust And for exotic edge cases, any Dockerfile-wrapped code HUGO framework Drupal Sphinx Quake III Racetrack can be extended to introduce new languages and frameworks .","title":"What is Racetrack?"},{"location":"docs/user/user-guide-1/#what-distinguishes-racetrack","text":"You only supply your function's logic . No need to write repetitive API code, setting up webservers, creating dockerfiles, kubernetes YAMLs, and so on. Racetrack takes care of that for you. Language agnostic . Deploy code written in Python 3, Go, Rust, or anything else encapsulated in a Dockerfile. Infrastructure independent . Deploy to either a Kubernetes cluster or a Docker environment in a single transparent step. Server-side building . Code is transformed into a microservice without your computer being involved. Customizable through plugins , Immutable jobs Reproducible jobs . Racetrack ensures that anyone else can deploy the same job effortlessly. Out-of-the-box tools : web endpoints, API documentation, metrics, monitoring, tracing, and more. Released under a permissive open-source license , Suitable for both on-premises and cloud environments.","title":"What distinguishes Racetrack?"},{"location":"docs/user/user-guide-1/#architecture-and-terminology","text":"The following terms recur through this documentation and describe the elements and actions involved in using Racetrack: Job code : your code, written in the style required by Racetrack Job Type : which of the languages and frameworks supported by Racetrack you choose to develop your job in Job submission : when you're happy with your code, and you push it to Racetrack to be deployed Job : when a job code is submitted to Racetrack, Racetrack converts it to a workload and deploys it. This workload is called a Job Convention : when you pick your Job Type, you will be asked to follow a specific style for that type which Racetrack understands; that style is a Convention Manifest : a YAML file in the root of your Job, which specifies the job type and provides configuration values for your Job according to that Job Type To tie all of these terms together: As a data scientist, I selected the Python 3 Job Type to develop my ML model Job in. I wrote my Job code according to the Job Type convention . I composed a Manifest in which I specified the Job Type I used, and in it I tweaked a few specific parameters for this Job Type. I submitted the job code to Racetrack, after which it was deployed as a Job .","title":"Architecture and Terminology"},{"location":"docs/user/user-guide-1/#conventions","text":"For Racetrack to convert your Job to a Job, you have to follow a specific style for your Job Type: a Convention. Broadly speaking, the purpose of this convention is to guide Racetrack: \"Look Racetrack! Right here is the method to query my model; and right here is the method for retraining.\" You write some code according to a set of conventions, submit it to Racetrack, and a short while after, your code is in operation. This convention is a key aspect of Racetrack: you can think of it as a \"way the code should look\" for Racetrack to understand it and deploy it to production. You can browse the documentation for the supported Job Types in their dedicated repositories. In there, the Convention for each Job Type is described. Conventions for any Job Type are simple, easy to follow, and very few. Some examples are: For the Python 3 Job Type, you must have a class JobEntrypoint with a method perform() for the Python 3 function which receives input and gives output (e.g. receiving a vector, and returning it normalized). Racetrack will then know to wrap this in an HTTP server and expose it. For the Go Job Type, your main function must have the following signature: func Perform(args []interface{}, kwargs map[string]interface{}) (interface{}, error) . As a concrete example, if you have written a clever Python 3 function which adds two integers and returns the result, your code pre-Racetrack would look like this: # file: model.py def add_em_up ( x , y ): z = x + y return z And refactored to meet the Racetrack Python 3 Job Type Convention: # file: job_entrypoint.py class JobEntrypoint : def perform ( self , x , y ): return add_em_up ( x , y ) def add_em_up ( x , y ): z = x + y return z","title":"Conventions"},{"location":"docs/user/user-guide-1/#the-manifest","text":"Having picked our Job Type and followed its Convention, the only thing we're missing is to inform Racetrack what Job Type we're submitting, and apply any special configuration supported by that Job Type. We do this by creating a file named job.yaml in the root directory of our Job source tree. This file is YAML formatted. The documentation for each Job Type describes how it should be formed. A typical job.yaml will look like this: name : my_fantabulous_skynet_AI owner_email : nobody@example.com jobtype : python3:latest # this would be your Job Type git : remote : https://github.com/racetrack/supersmart-model branch : master jobtype_extra : requirements_path : 'supersmart/requirements.txt' entrypoint_path : 'job_entrypoint.py' resources : memory_max : 2Gi So in a way, the Manifest is to Racetrack what the Dockerfile is to Docker. Some clever things that can be controlled using the Manifest is installing extra packages, or processing a Python requirements.txt file. It is recommended to declare maximum memory amount explicitly by defining resources : memory_max : 1Gi Please refer to the more comprehensive section The Job Manifest File for more detail.","title":"The Manifest"},{"location":"docs/user/user-guide-1/#submitting-a-job","text":"Racetrack Jobs are deployed to operation; that means, they are sent off from your development computer to run on a server somewhere. This sending is in Racetrack terminology called Job \"submission\". As you will experience in the CLI client , Racetrack provides a command line client you can install, and which handles this Submission for you. When operating with Racetrack (either local instance or production server), the Racetrack command line client will need authentication.","title":"Submitting a Job"},{"location":"docs/user/user-guide-1/#whats-next","text":"User Guide - Deploying a Job Quickstart - quickly setup Racetrack on local Docker engine. Local Kubernetes Setup - run Racetrack on KinD - longer, but more comprehensive guide. Available plugins Job Manifest File Schema Installation to standalone host","title":"What's next?"},{"location":"docs/user/user-guide-2/","text":"User Guide - Deploying a Job \u00b6 The Racetrack Workflow \u00b6 As a Racetrack user, to deploy a Job your workflow will typically look similar to this: (Do it once) Install Racetrack . (Do it once) Configure your racetrack client . Write a piece of code doing something useful Develop a Job by picking the Job Type and following the Convention. Compose a Job Manifest Push it to git Submit the Job code Call the Job These instructions will work against the local test version described in the Local Tutorial , but are also explained such that they make sense against a production instance of Racetrack on a real Kubernetes cluster . You will follow the same workflow in both cases. 1. Install Racetrack \u00b6 You have the following options: Install Racetrack locally - quickly setup Racetrack on local Docker engine. Install Racetrack in local Kubernetes - run Racetrack on Kind. Install to standalone host - e.g. Virtual Machine or EC2 Ask your friendly admin to install Racetrack for you and get you a link. In this tutorial, we chose first option and we'll assume Racetrack is running on: http://127.0.0.1:7102 - Lifecycle API service http://127.0.0.1:7103 - Racetrack Dashboard 2. Configure your racetrack client \u00b6 Get the Racetrack CLI client \u00b6 You need the racetrack-client CLI tool installed to manage Jobs. Something like this ought to work: python3 -m venv venv . venv/bin/activate python3 -m pip install --upgrade racetrack-client racetrack --help Set Remote \u00b6 You can set the current Racetrack server's URL (a \"remote\") with: racetrack set remote http://127.0.0.1:7102 This then affects all subsequent invocations to racetrack , if --remote parameter is not set explicitly. Using a Production Racetrack \u00b6 In the case of a production cluster, the only real change will be to the remote URL. You will need to obtain the Racetrack address instead of 127.0.0.1:7002 , so that: racetrack set remote http://127.0.0.1:7102 becomes racetrack set remote http://racetrack.platform.example.com:12345/lifecycle Other endpoints described in the tutorial will also change away from localhost . for example http://127.0.0.1:7103/ might become https://racetrack-lifecycle.platform.example.com/dashboard . You will need to check with your local Racetrack admin to get these endpoints. Or see how-to deploy Racetrack on your own . Setting aliases for Racetrack servers \u00b6 You can set up aliases for Racetrack server URL addresses by issuing command: racetrack set alias ALIAS RACETRACK_URL If you operate with many environments, setting short names may come in handy. For instance: racetrack set alias docker http://127.0.0.1:7102 racetrack set alias kind http://127.0.0.1:7002 racetrack set alias dev https://racetrack.dev.platform.example.com/lifecycle racetrack set alias test https://racetrack.test.platform.example.com/lifecycle racetrack set alias prod https://racetrack.prod.platform.example.com/lifecycle and then you can use your short names instead of full RACETRACK_URL address when calling racetrack deploy --remote docker . You can also use it in the current remote name. For instance, let's set it to \"docker\": racetrack set remote docker Create an account \u00b6 Before you can deploy a job to production Racetrack server or even view the list of Job on RT Dashboard, you need to create user there. Visit your Racetrack Dashboard at https://racetrack.platform.example.com/dashboard/ (or local http://127.0.0.1:7103/dashboard ), click link to Register . Type username (an email) and password. Password will be needed to log in, so manage it carefully. Then notify your admin that he should activate your user. Activate the account \u00b6 The administrator now has to visit Racetrack Dashboard, open Administration tab, open Lifecycle Admin panel , Browse the Users , and tick the Active checkbox near the new user. Authentication \u00b6 When he does that, you can log in, and click Profile tab. There will be your auth token for racetrack client CLI, along with ready command to log in. It will look like racetrack login <token> [--remote <remote>] . When you run this, then you can finally deploy your Job. Alternatively, use command racetrack login --username <username> to log in with your username and password (entered into the standard input) and to save the auth token without having to visit the Dashboard page. If you need, you can log out with racetrack logout [--remote <remote>] . To check your logged servers, there's racetrack get config command. You can aceess Job pages though the browser as long as you're logged to Racetrack Dashboard. Session is maintained through cookie. When viewing Job swagger page, you can run there the /perform method without specifying additional credentials, because auth data in cookie from Racetrack Dashboard is used as credential. However, if you copy the code to curl in CLI like this: curl -X 'POST' \\ 'https://racetrack.platform.example.com/pub/job/adder/0.0.1/api/v1/perform' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"numbers\": [ 40, 2 ] }' Then it won't work, because there's no auth data specified: Unauthenticated: no header X-Racetrack-Auth: not logged in You will need to include it in curl using -H 'X-Racetrack-Auth: $(racetrack get auth-token) . Jobs in private git repositories \u00b6 Racetrack requires in the job.yaml a git URL from which to fetch the Job source code. If this repo is private or protected, you will need to issue a token for the racetrack CLI tool to work. In GitLab, there are two kinds of tokens (either of them can be used): Personal Access Token - allows to operate on all user projects. Instructions to create it are here Project Access Token - needs to be issued per project. User has to be Maintainer in project to be able to create such token. Instructions to create it are here . In both cases it has to have read_repository privilege. Once you have this token, you need to register it with the racetrack CLI tool: racetrack set credentials REPO_URL USERNAME TOKEN where: REPO_URL : url to git repository, i.e. https://github.com/theracetrack/racetrack You can also set this to root domain i.e. https://github.com to make this token handle all projects. USERNAME : it's your GitHub/Gitlab account name, usually in the form of email TOKEN : A token giving the read-only access to repository. Keep it secret. Local Client Configuration \u00b6 The racetrack CLI tool maintains a configuration on your developer workstation. As you saw earlier in the section on Jobs in private git Repositories it can store Project/Personal Access Tokens. It is also possible to store the address of the Racetrack server: racetrack set remote http://127.0.0.1:7002 Local client configuration is stored at ~/.racetrack/config.yaml 3. Write a piece of code doing something useful \u00b6 Let's say we have a piece of Python code, checking if a given number is prime. import math def is_prime ( number : int ) -> bool : \"\"\"Check if a number is prime\"\"\" if number < 2 : return False for i in range ( 2 , int ( math . sqrt ( number )) + 1 ): if number % i == 0 : return False return True 4. Develop a Job \u00b6 You must pick the appropriate Racetrack Job Type and Refactor your code to follow its Convention. Job Type \u00b6 These links show how to use particular job types that are provided by the plugins : python3 - designed for Python projects golang - designed for Go projects docker-proxy - designed for any other Dockefile-based jobs We decide to use python3 job type to deploy our Python code. Refactor your code to follow the Convention \u00b6 Python3 job type requires to embed our code in a perform method inside a class. It also says that implementing method docs_input_example will show the example input values on the Swagger Documentation page. Let's create sample/python-primer/entrypoint.py file: import math class JobEntrypoint : def perform ( self , number : int ) -> bool : \"\"\"Check if a number is prime\"\"\" if number < 2 : return False for i in range ( 2 , int ( math . sqrt ( number )) + 1 ): if number % i == 0 : return False return True def docs_input_example ( self ) -> dict : \"\"\"Return example input values for this model\"\"\" return { 'number' : 7907 } If needed, we might want to specify additional Python packages in requirements.txt , but we skipped that step as we don't need any third-party libraries this time. General Job Guidelines \u00b6 This document uses the terms may, must, should, should not, and must not in accord with RFC 2119 . Must \u00b6 You must use one of the pre-defined (currently installed) job types. Should \u00b6 The call path should be kept shallow. We prefer a bit bigger Job over small that creates a deep call path. If part of the functionality of your Job becomes useful to a Service Consumer other than the current set of Service Consumers, consider if this part of its functionality should be split out into a separate Job. This is usually only a good idea of this part of the functionality is expensive in time or physical resources. Should not \u00b6 You are discouraged from creating code boundaries by splitting a job up into several, if they all serve the same request. While Racetrack supports chaining Jobs, it prefers tight coupling in Jobs serving single business purposes. The user should not use the Dockerfile job type. It's preferable to use one of the more specialised job types, or to coordinate with the RT developers to implement new job types. The Dockerfile job type exists as a fallback of last resort for tight deadlines and genuinely one-off runs which are demonstrably not accomplishable with current specialised job types, or which don't lend themselves via curation to improvements in specialised job types. May \u00b6 If you have a need which isn't covered by the currently implemented job types, you may raise the need with the Racetrack developers in the GitHub issue tracker. 5. Compose a Job Manifest \u00b6 To deploy a Job, the Developer should provide a build recipe called a Manifest, describing how to build, run, and deploy. Here's our sample/python-primer/job.yaml Manifest file: name : primer owner_email : sample@example.com jobtype : python3:latest git : remote : https://github.com/TheRacetrack/racetrack directory : sample/python-primer jobtype_extra : entrypoint_path : 'entrypoint.py' entrypoint_class : 'JobEntrypoint' which translates to: My job is called primer . It's been created by sample@example.com . It adheres to python3 job type, in latest version. The code of my job is stored in git at https://github.com/TheRacetrack/racetrack , in a sample/python-primer/ directory. There's an entrypoint.py file, in which there's JobEntrypoint class. According to this job type, the main logic is in the perform method. See Job Manifest File Schema . 6. Push it to git \u00b6 It's important to note that Racetrack builds the Job from the code found in git. That's why all the changes have to be pushed to git prior to deploying a Job. Push the job code to the same repository that is declared in job.yaml manifest file. If your repository is private, remember to also configure read access to your repository . 7. Deploy a Job \u00b6 Standing in the root directory of your Job code ( sample/python-primer/ ), submit the Job using the Racetrack command line client: racetrack deploy Wait briefly. Receive the message that it has been deployed. 8. Call the Job \u00b6 Through Browser \u00b6 A link to the Job will be provided to you in the racetrack deploy output. Open it directly or find it in the Racetrack Dashboard and click Open . You'll see the Swagger UI page, on which you can make an HTTP request at the /perform endpoint. You'll find the example input value there as well. Through Curl \u00b6 Alternatively, call the job by curl : curl -X POST \"http://127.0.0.1:7105/pub/job/primer/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"number\": 7907}' # Expect: true Through Racetrack client \u00b6 Or use racetrack CLI: racetrack call primer /api/v1/perform '{\"number\": 7907}' FAQ \u00b6 I've submitted a job, where can I see if it's ready? \u00b6 When you invoke racetrack deploy . --remote https://racetrack.platform.example.com/lifecycle , the client will block while the deployment operation is in progress. When the command terminates, unless you are given an error message, your job has been deployed into a Job. It will be added to the list of running Jobs in the Racetrack dashboard; you can see it there yourself, or if you don't have access, check with the local Racetrack admin. I've submitted a job, but it's not working. Where can I see my errors? \u00b6 If the error relates to the deployment action, the racetrack CLI tool will display an error for you. You can also see it on Racetrack Dashboard. My Job produces raw output, I need it in another format \u00b6 Racetrack supports chained Jobs; you can retain the original Job, and deploy a supplemental \"handler\" Job which calls the original and transforms its output to your desired format. Then you can simply call your handler. My Job takes config parameters, I don't want to pass them every call \u00b6 You have several options. You could use the handler pattern, placing a Job in front with the parameters baked in. This means every time you need to change the parameters, you need to update and redeploy the handler. This option is good for very slowly changing parameters. Another option is to build a Job with a web UI for tweaking the configuration parameters, and then placing this one in front of your original Job. This way, you change parameters at runtime rather than build-time. This option is more work, but suits parameters which change a little more frequently. As a third option, consider if you could \"bake in\" the parameters in your original model. Time to deployment in Racetrack is very quick, and you might be fine just redeploying the same Job with different config parameters when they change. I need to combine the results of multiple Job \u00b6 Develop a \"handler\" Job which calls the other Jobs whose results you want to combine. Racetrack supports chaining of Jobs for this purpose. I need to be able to receive my answers asynchronously \u00b6 The way to implement this is to create your own handler that can give you your answers asynchronously. A simple way of archiving this would be to provide a callback URL in the ESC query. So you would send the request to the Job from your ESC; as soon as the Job receives the request, the request terminates successfully; it does not wait to provide a response. Then your ESC listens on a defined webhook endpoint - the path which was for example provided in the request - and when the Job has finished processing the request and is ready with a response, it POSTs this to the endpoint on the ESC. Obviously, asynchronous calls require you to do some work on the ESC side as well. What's next? \u00b6 Quickstart - quickly setup Racetrack on local Docker engine. Local Kubernetes Setup - run Racetrack on KinD - longer, but more comprehensive guide. Available plugins Job Manifest File Schema Installation to standalone host","title":"User Guide - Deploying a Job"},{"location":"docs/user/user-guide-2/#user-guide-deploying-a-job","text":"","title":"User Guide - Deploying a Job"},{"location":"docs/user/user-guide-2/#the-racetrack-workflow","text":"As a Racetrack user, to deploy a Job your workflow will typically look similar to this: (Do it once) Install Racetrack . (Do it once) Configure your racetrack client . Write a piece of code doing something useful Develop a Job by picking the Job Type and following the Convention. Compose a Job Manifest Push it to git Submit the Job code Call the Job These instructions will work against the local test version described in the Local Tutorial , but are also explained such that they make sense against a production instance of Racetrack on a real Kubernetes cluster . You will follow the same workflow in both cases.","title":"The Racetrack Workflow"},{"location":"docs/user/user-guide-2/#1-install-racetrack","text":"You have the following options: Install Racetrack locally - quickly setup Racetrack on local Docker engine. Install Racetrack in local Kubernetes - run Racetrack on Kind. Install to standalone host - e.g. Virtual Machine or EC2 Ask your friendly admin to install Racetrack for you and get you a link. In this tutorial, we chose first option and we'll assume Racetrack is running on: http://127.0.0.1:7102 - Lifecycle API service http://127.0.0.1:7103 - Racetrack Dashboard","title":"1. Install Racetrack"},{"location":"docs/user/user-guide-2/#2-configure-your-racetrack-client","text":"","title":"2. Configure your racetrack client"},{"location":"docs/user/user-guide-2/#get-the-racetrack-cli-client","text":"You need the racetrack-client CLI tool installed to manage Jobs. Something like this ought to work: python3 -m venv venv . venv/bin/activate python3 -m pip install --upgrade racetrack-client racetrack --help","title":"Get the Racetrack CLI client"},{"location":"docs/user/user-guide-2/#set-remote","text":"You can set the current Racetrack server's URL (a \"remote\") with: racetrack set remote http://127.0.0.1:7102 This then affects all subsequent invocations to racetrack , if --remote parameter is not set explicitly.","title":"Set Remote"},{"location":"docs/user/user-guide-2/#using-a-production-racetrack","text":"In the case of a production cluster, the only real change will be to the remote URL. You will need to obtain the Racetrack address instead of 127.0.0.1:7002 , so that: racetrack set remote http://127.0.0.1:7102 becomes racetrack set remote http://racetrack.platform.example.com:12345/lifecycle Other endpoints described in the tutorial will also change away from localhost . for example http://127.0.0.1:7103/ might become https://racetrack-lifecycle.platform.example.com/dashboard . You will need to check with your local Racetrack admin to get these endpoints. Or see how-to deploy Racetrack on your own .","title":"Using a Production Racetrack"},{"location":"docs/user/user-guide-2/#setting-aliases-for-racetrack-servers","text":"You can set up aliases for Racetrack server URL addresses by issuing command: racetrack set alias ALIAS RACETRACK_URL If you operate with many environments, setting short names may come in handy. For instance: racetrack set alias docker http://127.0.0.1:7102 racetrack set alias kind http://127.0.0.1:7002 racetrack set alias dev https://racetrack.dev.platform.example.com/lifecycle racetrack set alias test https://racetrack.test.platform.example.com/lifecycle racetrack set alias prod https://racetrack.prod.platform.example.com/lifecycle and then you can use your short names instead of full RACETRACK_URL address when calling racetrack deploy --remote docker . You can also use it in the current remote name. For instance, let's set it to \"docker\": racetrack set remote docker","title":"Setting aliases for Racetrack servers"},{"location":"docs/user/user-guide-2/#create-an-account","text":"Before you can deploy a job to production Racetrack server or even view the list of Job on RT Dashboard, you need to create user there. Visit your Racetrack Dashboard at https://racetrack.platform.example.com/dashboard/ (or local http://127.0.0.1:7103/dashboard ), click link to Register . Type username (an email) and password. Password will be needed to log in, so manage it carefully. Then notify your admin that he should activate your user.","title":"Create an account"},{"location":"docs/user/user-guide-2/#activate-the-account","text":"The administrator now has to visit Racetrack Dashboard, open Administration tab, open Lifecycle Admin panel , Browse the Users , and tick the Active checkbox near the new user.","title":"Activate the account"},{"location":"docs/user/user-guide-2/#authentication","text":"When he does that, you can log in, and click Profile tab. There will be your auth token for racetrack client CLI, along with ready command to log in. It will look like racetrack login <token> [--remote <remote>] . When you run this, then you can finally deploy your Job. Alternatively, use command racetrack login --username <username> to log in with your username and password (entered into the standard input) and to save the auth token without having to visit the Dashboard page. If you need, you can log out with racetrack logout [--remote <remote>] . To check your logged servers, there's racetrack get config command. You can aceess Job pages though the browser as long as you're logged to Racetrack Dashboard. Session is maintained through cookie. When viewing Job swagger page, you can run there the /perform method without specifying additional credentials, because auth data in cookie from Racetrack Dashboard is used as credential. However, if you copy the code to curl in CLI like this: curl -X 'POST' \\ 'https://racetrack.platform.example.com/pub/job/adder/0.0.1/api/v1/perform' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"numbers\": [ 40, 2 ] }' Then it won't work, because there's no auth data specified: Unauthenticated: no header X-Racetrack-Auth: not logged in You will need to include it in curl using -H 'X-Racetrack-Auth: $(racetrack get auth-token) .","title":"Authentication"},{"location":"docs/user/user-guide-2/#jobs-in-private-git-repositories","text":"Racetrack requires in the job.yaml a git URL from which to fetch the Job source code. If this repo is private or protected, you will need to issue a token for the racetrack CLI tool to work. In GitLab, there are two kinds of tokens (either of them can be used): Personal Access Token - allows to operate on all user projects. Instructions to create it are here Project Access Token - needs to be issued per project. User has to be Maintainer in project to be able to create such token. Instructions to create it are here . In both cases it has to have read_repository privilege. Once you have this token, you need to register it with the racetrack CLI tool: racetrack set credentials REPO_URL USERNAME TOKEN where: REPO_URL : url to git repository, i.e. https://github.com/theracetrack/racetrack You can also set this to root domain i.e. https://github.com to make this token handle all projects. USERNAME : it's your GitHub/Gitlab account name, usually in the form of email TOKEN : A token giving the read-only access to repository. Keep it secret.","title":"Jobs in private git repositories"},{"location":"docs/user/user-guide-2/#local-client-configuration","text":"The racetrack CLI tool maintains a configuration on your developer workstation. As you saw earlier in the section on Jobs in private git Repositories it can store Project/Personal Access Tokens. It is also possible to store the address of the Racetrack server: racetrack set remote http://127.0.0.1:7002 Local client configuration is stored at ~/.racetrack/config.yaml","title":"Local Client Configuration"},{"location":"docs/user/user-guide-2/#3-write-a-piece-of-code-doing-something-useful","text":"Let's say we have a piece of Python code, checking if a given number is prime. import math def is_prime ( number : int ) -> bool : \"\"\"Check if a number is prime\"\"\" if number < 2 : return False for i in range ( 2 , int ( math . sqrt ( number )) + 1 ): if number % i == 0 : return False return True","title":"3. Write a piece of code doing something useful"},{"location":"docs/user/user-guide-2/#4-develop-a-job","text":"You must pick the appropriate Racetrack Job Type and Refactor your code to follow its Convention.","title":"4. Develop a Job"},{"location":"docs/user/user-guide-2/#job-type","text":"These links show how to use particular job types that are provided by the plugins : python3 - designed for Python projects golang - designed for Go projects docker-proxy - designed for any other Dockefile-based jobs We decide to use python3 job type to deploy our Python code.","title":"Job Type"},{"location":"docs/user/user-guide-2/#refactor-your-code-to-follow-the-convention","text":"Python3 job type requires to embed our code in a perform method inside a class. It also says that implementing method docs_input_example will show the example input values on the Swagger Documentation page. Let's create sample/python-primer/entrypoint.py file: import math class JobEntrypoint : def perform ( self , number : int ) -> bool : \"\"\"Check if a number is prime\"\"\" if number < 2 : return False for i in range ( 2 , int ( math . sqrt ( number )) + 1 ): if number % i == 0 : return False return True def docs_input_example ( self ) -> dict : \"\"\"Return example input values for this model\"\"\" return { 'number' : 7907 } If needed, we might want to specify additional Python packages in requirements.txt , but we skipped that step as we don't need any third-party libraries this time.","title":"Refactor your code to follow the Convention"},{"location":"docs/user/user-guide-2/#general-job-guidelines","text":"This document uses the terms may, must, should, should not, and must not in accord with RFC 2119 .","title":"General Job Guidelines"},{"location":"docs/user/user-guide-2/#must","text":"You must use one of the pre-defined (currently installed) job types.","title":"Must"},{"location":"docs/user/user-guide-2/#should","text":"The call path should be kept shallow. We prefer a bit bigger Job over small that creates a deep call path. If part of the functionality of your Job becomes useful to a Service Consumer other than the current set of Service Consumers, consider if this part of its functionality should be split out into a separate Job. This is usually only a good idea of this part of the functionality is expensive in time or physical resources.","title":"Should"},{"location":"docs/user/user-guide-2/#should-not","text":"You are discouraged from creating code boundaries by splitting a job up into several, if they all serve the same request. While Racetrack supports chaining Jobs, it prefers tight coupling in Jobs serving single business purposes. The user should not use the Dockerfile job type. It's preferable to use one of the more specialised job types, or to coordinate with the RT developers to implement new job types. The Dockerfile job type exists as a fallback of last resort for tight deadlines and genuinely one-off runs which are demonstrably not accomplishable with current specialised job types, or which don't lend themselves via curation to improvements in specialised job types.","title":"Should not"},{"location":"docs/user/user-guide-2/#may","text":"If you have a need which isn't covered by the currently implemented job types, you may raise the need with the Racetrack developers in the GitHub issue tracker.","title":"May"},{"location":"docs/user/user-guide-2/#5-compose-a-job-manifest","text":"To deploy a Job, the Developer should provide a build recipe called a Manifest, describing how to build, run, and deploy. Here's our sample/python-primer/job.yaml Manifest file: name : primer owner_email : sample@example.com jobtype : python3:latest git : remote : https://github.com/TheRacetrack/racetrack directory : sample/python-primer jobtype_extra : entrypoint_path : 'entrypoint.py' entrypoint_class : 'JobEntrypoint' which translates to: My job is called primer . It's been created by sample@example.com . It adheres to python3 job type, in latest version. The code of my job is stored in git at https://github.com/TheRacetrack/racetrack , in a sample/python-primer/ directory. There's an entrypoint.py file, in which there's JobEntrypoint class. According to this job type, the main logic is in the perform method. See Job Manifest File Schema .","title":"5. Compose a Job Manifest"},{"location":"docs/user/user-guide-2/#6-push-it-to-git","text":"It's important to note that Racetrack builds the Job from the code found in git. That's why all the changes have to be pushed to git prior to deploying a Job. Push the job code to the same repository that is declared in job.yaml manifest file. If your repository is private, remember to also configure read access to your repository .","title":"6. Push it to git"},{"location":"docs/user/user-guide-2/#7-deploy-a-job","text":"Standing in the root directory of your Job code ( sample/python-primer/ ), submit the Job using the Racetrack command line client: racetrack deploy Wait briefly. Receive the message that it has been deployed.","title":"7. Deploy a Job"},{"location":"docs/user/user-guide-2/#8-call-the-job","text":"","title":"8. Call the Job"},{"location":"docs/user/user-guide-2/#through-browser","text":"A link to the Job will be provided to you in the racetrack deploy output. Open it directly or find it in the Racetrack Dashboard and click Open . You'll see the Swagger UI page, on which you can make an HTTP request at the /perform endpoint. You'll find the example input value there as well.","title":"Through Browser"},{"location":"docs/user/user-guide-2/#through-curl","text":"Alternatively, call the job by curl : curl -X POST \"http://127.0.0.1:7105/pub/job/primer/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: $( racetrack get auth-token ) \" \\ -d '{\"number\": 7907}' # Expect: true","title":"Through Curl"},{"location":"docs/user/user-guide-2/#through-racetrack-client","text":"Or use racetrack CLI: racetrack call primer /api/v1/perform '{\"number\": 7907}'","title":"Through Racetrack client"},{"location":"docs/user/user-guide-2/#faq","text":"","title":"FAQ"},{"location":"docs/user/user-guide-2/#ive-submitted-a-job-where-can-i-see-if-its-ready","text":"When you invoke racetrack deploy . --remote https://racetrack.platform.example.com/lifecycle , the client will block while the deployment operation is in progress. When the command terminates, unless you are given an error message, your job has been deployed into a Job. It will be added to the list of running Jobs in the Racetrack dashboard; you can see it there yourself, or if you don't have access, check with the local Racetrack admin.","title":"I've submitted a job, where can I see if it's ready?"},{"location":"docs/user/user-guide-2/#ive-submitted-a-job-but-its-not-working-where-can-i-see-my-errors","text":"If the error relates to the deployment action, the racetrack CLI tool will display an error for you. You can also see it on Racetrack Dashboard.","title":"I've submitted a job, but it's not working. Where can I see my errors?"},{"location":"docs/user/user-guide-2/#my-job-produces-raw-output-i-need-it-in-another-format","text":"Racetrack supports chained Jobs; you can retain the original Job, and deploy a supplemental \"handler\" Job which calls the original and transforms its output to your desired format. Then you can simply call your handler.","title":"My Job produces raw output, I need it in another format"},{"location":"docs/user/user-guide-2/#my-job-takes-config-parameters-i-dont-want-to-pass-them-every-call","text":"You have several options. You could use the handler pattern, placing a Job in front with the parameters baked in. This means every time you need to change the parameters, you need to update and redeploy the handler. This option is good for very slowly changing parameters. Another option is to build a Job with a web UI for tweaking the configuration parameters, and then placing this one in front of your original Job. This way, you change parameters at runtime rather than build-time. This option is more work, but suits parameters which change a little more frequently. As a third option, consider if you could \"bake in\" the parameters in your original model. Time to deployment in Racetrack is very quick, and you might be fine just redeploying the same Job with different config parameters when they change.","title":"My Job takes config parameters, I don't want to pass them every call"},{"location":"docs/user/user-guide-2/#i-need-to-combine-the-results-of-multiple-job","text":"Develop a \"handler\" Job which calls the other Jobs whose results you want to combine. Racetrack supports chaining of Jobs for this purpose.","title":"I need to combine the results of multiple Job"},{"location":"docs/user/user-guide-2/#i-need-to-be-able-to-receive-my-answers-asynchronously","text":"The way to implement this is to create your own handler that can give you your answers asynchronously. A simple way of archiving this would be to provide a callback URL in the ESC query. So you would send the request to the Job from your ESC; as soon as the Job receives the request, the request terminates successfully; it does not wait to provide a response. Then your ESC listens on a defined webhook endpoint - the path which was for example provided in the request - and when the Job has finished processing the request and is ready with a response, it POSTs this to the endpoint on the ESC. Obviously, asynchronous calls require you to do some work on the ESC side as well.","title":"I need to be able to receive my answers asynchronously"},{"location":"docs/user/user-guide-2/#whats-next","text":"Quickstart - quickly setup Racetrack on local Docker engine. Local Kubernetes Setup - run Racetrack on KinD - longer, but more comprehensive guide. Available plugins Job Manifest File Schema Installation to standalone host","title":"What's next?"},{"location":"docs/user/using-plugins/","text":"Using plugins \u00b6 Racetrack instance might be enriched by plugins that add customized, tailored functionality to a generic Racetrack. Installing / Uninstalling plugin \u00b6 From a dashboard \u00b6 To activate the plugin in Racetrack, you need the ZIP plugin file. Go to the Dashboard Administration page (you need to be privileged, staff user to see this tab) and upload the zipped plugin there. To disable a plugin, click \"Delete\" button next to a plugin. From a racetrack-client \u00b6 Plugins can be installed by means of racetrack-client command racetrack plugin install <plugin_uri> [--remote <racetrack_url>] , where plugin_uri can be either: local file path (eg. python3-job-type-2.4.0.zip ), URL to a remote HTTP file (eg. https://github.com/TheRacetrack/plugin/releases/download/2.4.0/python3-job-type-2.4.0.zip ), GitHub repository name (eg. github.com/TheRacetrack/plugin-python-job-type ) - it takes the ZIP file from the latest release. Pay attention to omit https:// part. GitHub repository name with version (eg. github.com/TheRacetrack/plugin-python-job-type==2.4.0 ) - it takes the ZIP file from the specific release. Pay attention to omit https:// part. In first place, choose the current remote address, so you can omit --remote parameter later on: racetrack set remote http://127.0.0.1:7002 # use local kind setup For instance, use the following command to activate the latest python3 plugin: racetrack plugin install github.com/TheRacetrack/plugin-python-job-type Plugins can be uninstalled with a command: racetrack plugin uninstall <plugin_name> <plugin_version> List of currently installed plugins can be checked with: racetrack plugin list What's next? \u00b6 Available plugins","title":"Installing plugins"},{"location":"docs/user/using-plugins/#using-plugins","text":"Racetrack instance might be enriched by plugins that add customized, tailored functionality to a generic Racetrack.","title":"Using plugins"},{"location":"docs/user/using-plugins/#installing-uninstalling-plugin","text":"","title":"Installing / Uninstalling plugin"},{"location":"docs/user/using-plugins/#from-a-dashboard","text":"To activate the plugin in Racetrack, you need the ZIP plugin file. Go to the Dashboard Administration page (you need to be privileged, staff user to see this tab) and upload the zipped plugin there. To disable a plugin, click \"Delete\" button next to a plugin.","title":"From a dashboard"},{"location":"docs/user/using-plugins/#from-a-racetrack-client","text":"Plugins can be installed by means of racetrack-client command racetrack plugin install <plugin_uri> [--remote <racetrack_url>] , where plugin_uri can be either: local file path (eg. python3-job-type-2.4.0.zip ), URL to a remote HTTP file (eg. https://github.com/TheRacetrack/plugin/releases/download/2.4.0/python3-job-type-2.4.0.zip ), GitHub repository name (eg. github.com/TheRacetrack/plugin-python-job-type ) - it takes the ZIP file from the latest release. Pay attention to omit https:// part. GitHub repository name with version (eg. github.com/TheRacetrack/plugin-python-job-type==2.4.0 ) - it takes the ZIP file from the specific release. Pay attention to omit https:// part. In first place, choose the current remote address, so you can omit --remote parameter later on: racetrack set remote http://127.0.0.1:7002 # use local kind setup For instance, use the following command to activate the latest python3 plugin: racetrack plugin install github.com/TheRacetrack/plugin-python-job-type Plugins can be uninstalled with a command: racetrack plugin uninstall <plugin_name> <plugin_version> List of currently installed plugins can be checked with: racetrack plugin list","title":"From a racetrack-client"},{"location":"docs/user/using-plugins/#whats-next","text":"Available plugins","title":"What's next?"},{"location":"image_builder/","text":"Image Builder \u00b6 Image Builder is a Python package building deployable Job Docker images from source code workspace (job blueprints). Build sample Job image \u00b6 Build sample Job docker Image: cd ../sample/python-class && image_builder build Run \u00b6 Run Image builder and serve API for building Job images: make run","title":"Image Builder"},{"location":"image_builder/#image-builder","text":"Image Builder is a Python package building deployable Job Docker images from source code workspace (job blueprints).","title":"Image Builder"},{"location":"image_builder/#build-sample-job-image","text":"Build sample Job docker Image: cd ../sample/python-class && image_builder build","title":"Build sample Job image"},{"location":"image_builder/#run","text":"Run Image builder and serve API for building Job images: make run","title":"Run"},{"location":"lifecycle/","text":"LC API Server \u00b6 LC server is an API Server for managing deployed Job workloads. In particular, it's the entypoint for a Developer when deploying a new Job Workload. Testing \u00b6 Run the following command to perform unit tests: make test Run \u00b6 Run local LC-API server: make run By default localhost LC-API will use sqlite db. In docker-compose and kind setups it will use Postgres.","title":"LC API Server"},{"location":"lifecycle/#lc-api-server","text":"LC server is an API Server for managing deployed Job workloads. In particular, it's the entypoint for a Developer when deploying a new Job Workload.","title":"LC API Server"},{"location":"lifecycle/#testing","text":"Run the following command to perform unit tests: make test","title":"Testing"},{"location":"lifecycle/#run","text":"Run local LC-API server: make run By default localhost LC-API will use sqlite db. In docker-compose and kind setups it will use Postgres.","title":"Run"},{"location":"racetrack_client/","text":"Racetrack CLI client \u00b6 racetrack-client is a CLI client tool for deploying and managing workloads in Racetrack. Racetrack system allows to deploy jobs in a one step. It transforms your code to in-operation workloads, e.g. Kubernetes workloads. You write some code according to a set of coventions, you include the manifest file which explains the code, and you submit it to Racetrack. A short while after, the service calling your code is in operation. Quickstart \u00b6 # Install racetrack-client python3 -m pip install --upgrade racetrack-client # Set current remote racetrack set remote https://racetrack.platform.example.com # Log in with token racetrack login T0k3n.g0es.H3r3 # Or log in with username racetrack login --username admin # Deploy a Job racetrack deploy Installation \u00b6 Install racetrack-client using pip: python3 -m pip install --upgrade racetrack-client Python 3.8 (or higher) is required. This will install racetrack CLI tool. Verify installation by running racetrack command. Usage \u00b6 Run racetrack --help to see usage. Adding a remote \u00b6 Assuming your Racetrack server is running on https://racetrack.platform.example.com/lifecycle, you can add this remote as an alias: racetrack set alias my-dev https://racetrack.platform.example.com/lifecycle Alias is a short, friendly name for the URL of your Racetrack server, which is also known as \"remote\". From now on, you can refer to your remote with an alias. Switching remotes \u00b6 Set your current remote with: racetrack set remote my-dev This will set up a \"remote\" context for later use. Checking current remote \u00b6 You can check your current remote with racetrack get remote . A command racetrack get remote -q (with flag -q or --quiet ) prints only the current address of Lifecycle (without other logs), which makes it usable for scripts. Likewise, a command racetrack get pub -q (with flag -q or --quiet ) prints the current address of Pub service. \"Quiet\" mode is automatically applied when not in a TTY. Logging in \u00b6 Log in to Racetrack with your user account (you can get your token from the Dashboard's profile page): racetrack login T0k3n.g0es.H3r3 Alternatively, command racetrack login --username <username> allows you to log in with your username and password (entered into the standard input) and saves the auth token without having to visit the Dashboard page. In case you're going to use a private repository, provide your git credentials so the job can be built from your code: racetrack set credentials https://github.com/YourUser/YourRepository USERNAME TOKEN Installing plugins \u00b6 Extend Racetrack's possibilities by installing a bunch of plugins: # This plugin allows you to deploy jobs written in Python racetrack plugin install github.com/TheRacetrack/plugin-python-job-type # This plugin allows you to deploy jobs to a local Docker infrastructure racetrack plugin install github.com/TheRacetrack/plugin-docker-infrastructure Plugins can only be installed by admin users. Deploying a job \u00b6 When your code is ready and you pushed your changes to a repository, it's time to deploy it; that means, upload it to Racetrack so it can become a proper running Job. To deploy a job, just run it in the place where job.yaml is located: cd MuffinDestroyer racetrack deploy You will see the URL of your deployed job in the output. Listing jobs \u00b6 You can see the list of all deployed jobs with a command: racetrack list Checking runtime logs \u00b6 Check the logs of a running job by means of: racetrack logs MuffinDestroyer Deleting a job \u00b6 Delete your running job with: racetrack delete MuffinDestroyer Extra vars \u00b6 Manifest values can be overriden with key-value pairs coming from a command line. It doesn't modify actual file, but its one-time, in-memory version before submitting it. Racetrack client has --extra-vars KEY=VALUE parameter (or -e in short) that overwrites values found in YAML manifest. KEY is the name of field and it can contain dots to refer to a nested field, for example git.branch=master VALUE can be any YAML or JSON object. Extra vars parameters can be used multiple times in one command. Example: racetrack deploy -e secret_runtime_env_file = .env.local -e git.branch = $( git rev-parse --abbrev-ref HEAD ) It makes CLI commands more script-friendly, so you can overwrite manifest without tracking changes in job.yaml file. Tip: Use racetrack validate command beforehand to make sure your final manifest is what you expected. Getting auth token \u00b6 Command racetrack get auth-token prints out current auth token. It can be used in CLI scripts: curl -H \"X-Racetrack-Auth: $(racetrack get auth-token)\"","title":"CLI client"},{"location":"racetrack_client/#racetrack-cli-client","text":"racetrack-client is a CLI client tool for deploying and managing workloads in Racetrack. Racetrack system allows to deploy jobs in a one step. It transforms your code to in-operation workloads, e.g. Kubernetes workloads. You write some code according to a set of coventions, you include the manifest file which explains the code, and you submit it to Racetrack. A short while after, the service calling your code is in operation.","title":"Racetrack CLI client"},{"location":"racetrack_client/#quickstart","text":"# Install racetrack-client python3 -m pip install --upgrade racetrack-client # Set current remote racetrack set remote https://racetrack.platform.example.com # Log in with token racetrack login T0k3n.g0es.H3r3 # Or log in with username racetrack login --username admin # Deploy a Job racetrack deploy","title":"Quickstart"},{"location":"racetrack_client/#installation","text":"Install racetrack-client using pip: python3 -m pip install --upgrade racetrack-client Python 3.8 (or higher) is required. This will install racetrack CLI tool. Verify installation by running racetrack command.","title":"Installation"},{"location":"racetrack_client/#usage","text":"Run racetrack --help to see usage.","title":"Usage"},{"location":"racetrack_client/#adding-a-remote","text":"Assuming your Racetrack server is running on https://racetrack.platform.example.com/lifecycle, you can add this remote as an alias: racetrack set alias my-dev https://racetrack.platform.example.com/lifecycle Alias is a short, friendly name for the URL of your Racetrack server, which is also known as \"remote\". From now on, you can refer to your remote with an alias.","title":"Adding a remote"},{"location":"racetrack_client/#switching-remotes","text":"Set your current remote with: racetrack set remote my-dev This will set up a \"remote\" context for later use.","title":"Switching remotes"},{"location":"racetrack_client/#checking-current-remote","text":"You can check your current remote with racetrack get remote . A command racetrack get remote -q (with flag -q or --quiet ) prints only the current address of Lifecycle (without other logs), which makes it usable for scripts. Likewise, a command racetrack get pub -q (with flag -q or --quiet ) prints the current address of Pub service. \"Quiet\" mode is automatically applied when not in a TTY.","title":"Checking current remote"},{"location":"racetrack_client/#logging-in","text":"Log in to Racetrack with your user account (you can get your token from the Dashboard's profile page): racetrack login T0k3n.g0es.H3r3 Alternatively, command racetrack login --username <username> allows you to log in with your username and password (entered into the standard input) and saves the auth token without having to visit the Dashboard page. In case you're going to use a private repository, provide your git credentials so the job can be built from your code: racetrack set credentials https://github.com/YourUser/YourRepository USERNAME TOKEN","title":"Logging in"},{"location":"racetrack_client/#installing-plugins","text":"Extend Racetrack's possibilities by installing a bunch of plugins: # This plugin allows you to deploy jobs written in Python racetrack plugin install github.com/TheRacetrack/plugin-python-job-type # This plugin allows you to deploy jobs to a local Docker infrastructure racetrack plugin install github.com/TheRacetrack/plugin-docker-infrastructure Plugins can only be installed by admin users.","title":"Installing plugins"},{"location":"racetrack_client/#deploying-a-job","text":"When your code is ready and you pushed your changes to a repository, it's time to deploy it; that means, upload it to Racetrack so it can become a proper running Job. To deploy a job, just run it in the place where job.yaml is located: cd MuffinDestroyer racetrack deploy You will see the URL of your deployed job in the output.","title":"Deploying a job"},{"location":"racetrack_client/#listing-jobs","text":"You can see the list of all deployed jobs with a command: racetrack list","title":"Listing jobs"},{"location":"racetrack_client/#checking-runtime-logs","text":"Check the logs of a running job by means of: racetrack logs MuffinDestroyer","title":"Checking runtime logs"},{"location":"racetrack_client/#deleting-a-job","text":"Delete your running job with: racetrack delete MuffinDestroyer","title":"Deleting a job"},{"location":"racetrack_client/#extra-vars","text":"Manifest values can be overriden with key-value pairs coming from a command line. It doesn't modify actual file, but its one-time, in-memory version before submitting it. Racetrack client has --extra-vars KEY=VALUE parameter (or -e in short) that overwrites values found in YAML manifest. KEY is the name of field and it can contain dots to refer to a nested field, for example git.branch=master VALUE can be any YAML or JSON object. Extra vars parameters can be used multiple times in one command. Example: racetrack deploy -e secret_runtime_env_file = .env.local -e git.branch = $( git rev-parse --abbrev-ref HEAD ) It makes CLI commands more script-friendly, so you can overwrite manifest without tracking changes in job.yaml file. Tip: Use racetrack validate command beforehand to make sure your final manifest is what you expected.","title":"Extra vars"},{"location":"racetrack_client/#getting-auth-token","text":"Command racetrack get auth-token prints out current auth token. It can be used in CLI scripts: curl -H \"X-Racetrack-Auth: $(racetrack get auth-token)\"","title":"Getting auth token"},{"location":"sample/docker-golang-http/","text":"Deploying \u00b6 Run racetrack deploy in this directory. Calling a Job \u00b6 The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42 Health \u00b6 The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"} API docs \u00b6 Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"Deploying"},{"location":"sample/docker-golang-http/#deploying","text":"Run racetrack deploy in this directory.","title":"Deploying"},{"location":"sample/docker-golang-http/#calling-a-job","text":"The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/docker-golang-http/#health","text":"The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"}","title":"Health"},{"location":"sample/docker-golang-http/#api-docs","text":"Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"sample/docker-python-flask/","text":"Deploying \u00b6 Run racetrack deploy in this directory. Calling a Job \u00b6 The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42 Health \u00b6 The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"} API docs \u00b6 Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"Deploying"},{"location":"sample/docker-python-flask/#deploying","text":"Run racetrack deploy in this directory.","title":"Deploying"},{"location":"sample/docker-python-flask/#calling-a-job","text":"The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/docker-python-flask/#health","text":"The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"}","title":"Health"},{"location":"sample/docker-python-flask/#api-docs","text":"Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"sample/docs-formatter/","text":"Docs Formatter \u00b6 This is the exemplary Job using ASGI webview feature. Go to /api/v1/webview endpoint to see the frontend application formatting Markdown document as an input and rendering it on a webview. It keeps all data encoded in the URL so it's easy to share the rendered document.","title":"Docs Formatter"},{"location":"sample/docs-formatter/#docs-formatter","text":"This is the exemplary Job using ASGI webview feature. Go to /api/v1/webview endpoint to see the frontend application formatting Markdown document as an input and rendering it on a webview. It keeps all data encoded in the URL so it's easy to share the rendered document.","title":"Docs Formatter"},{"location":"sample/python-auxiliary-endpoints/","text":"Calling a Job \u00b6 The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-auxiliary-endpoints/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x\": 40, \"y\": 2}' # Expect: # 42 Calling auxiliary endpoints \u00b6 The model has one auxiliary endpoint configured: /explain . It works similarly to /perform endpoint, but returns different result based on custom method implementation: curl -X POST \"http://127.0.0.1:7005/pub/job/python-auxiliary-endpoints/latest/api/v1/explain\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x\": 40, \"y\": 2}' # Expect: # {\"x_importance\": 0.9523809523809523, \"y_importance\": 0.047619047619047616}","title":"Calling a Job"},{"location":"sample/python-auxiliary-endpoints/#calling-a-job","text":"The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-auxiliary-endpoints/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x\": 40, \"y\": 2}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/python-auxiliary-endpoints/#calling-auxiliary-endpoints","text":"The model has one auxiliary endpoint configured: /explain . It works similarly to /perform endpoint, but returns different result based on custom method implementation: curl -X POST \"http://127.0.0.1:7005/pub/job/python-auxiliary-endpoints/latest/api/v1/explain\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x\": 40, \"y\": 2}' # Expect: # {\"x_importance\": 0.9523809523809523, \"y_importance\": 0.047619047619047616}","title":"Calling auxiliary endpoints"},{"location":"sample/python-chain/","text":"Setup \u00b6 Deploy \"python-class\" job (see python-class ) Deploy \"python-chain\" job by running racetrack deploy in this directory. Create ESC and assign \"python-chain\" job to it in Racetrack admin panel: http://127.0.0.1:7103/lifecycle/admin In Racetrack admin panel open \"adder\" job and add \"python-chain\" job to its \"Allowed jobs\". Calling a Job \u00b6 This job calls another model suming up given numbers (good old \"adder\" model). After getting partial result, it returns rounded integer. The following request performs its functionality by calling it through PUB. Replace auth header with your ESC caller token. curl -X POST \"http://127.0.0.1:7105/pub/job/python-chain/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Esc-Auth: <insert your token here>\" \\ -d '{\"numbers\": [40, 2.7]}' # Expect: # 43 Health \u00b6 The following request checks service's healthiness: curl \"http://127.0.0.1:7105/pub/job/python-chain/latest/health\" # Expect: # {\"service\": \"job\", \"job_name\": \"python-chain\", \"status\": \"pass\"} API docs \u00b6 Check out root endpoint http://127.0.0.1:7005/pub/job/python-chain/latest/ with Swagger UI page containing interactive list of all endpoints.","title":"Setup"},{"location":"sample/python-chain/#setup","text":"Deploy \"python-class\" job (see python-class ) Deploy \"python-chain\" job by running racetrack deploy in this directory. Create ESC and assign \"python-chain\" job to it in Racetrack admin panel: http://127.0.0.1:7103/lifecycle/admin In Racetrack admin panel open \"adder\" job and add \"python-chain\" job to its \"Allowed jobs\".","title":"Setup"},{"location":"sample/python-chain/#calling-a-job","text":"This job calls another model suming up given numbers (good old \"adder\" model). After getting partial result, it returns rounded integer. The following request performs its functionality by calling it through PUB. Replace auth header with your ESC caller token. curl -X POST \"http://127.0.0.1:7105/pub/job/python-chain/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Esc-Auth: <insert your token here>\" \\ -d '{\"numbers\": [40, 2.7]}' # Expect: # 43","title":"Calling a Job"},{"location":"sample/python-chain/#health","text":"The following request checks service's healthiness: curl \"http://127.0.0.1:7105/pub/job/python-chain/latest/health\" # Expect: # {\"service\": \"job\", \"job_name\": \"python-chain\", \"status\": \"pass\"}","title":"Health"},{"location":"sample/python-chain/#api-docs","text":"Check out root endpoint http://127.0.0.1:7005/pub/job/python-chain/latest/ with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"sample/python-class/","text":"Deploying \u00b6 Run racetrack deploy in this directory. Calling a Job \u00b6 The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7105/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42 API docs \u00b6 Check out root endpoint http://127.0.0.1:7105/pub/job/adder/latest with Swagger UI page containing interactive list of all endpoints.","title":"Deploying"},{"location":"sample/python-class/#deploying","text":"Run racetrack deploy in this directory.","title":"Deploying"},{"location":"sample/python-class/#calling-a-job","text":"The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7105/pub/job/adder/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -H \"X-Racetrack-Auth: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzZWVkIjoiY2UwODFiMDUtYTRhMC00MTRhLThmNmEtODRjMDIzMTkxNmE2Iiwic3ViamVjdCI6ImFkbWluIiwic3ViamVjdF90eXBlIjoidXNlciIsInNjb3BlcyI6bnVsbH0.xDUcEmR7USck5RId0nwDo_xtZZBD6pUvB2vL6i39DQI\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/python-class/#api-docs","text":"Check out root endpoint http://127.0.0.1:7105/pub/job/adder/latest with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"sample/python-env-secret/","text":"Deploying a Job \u00b6 This Job makes use of secret vars, it should be filled out before attempting to deploy. 1. Make secret files from the templates: cp build.env.template build.env cp runtime.env.template runtime.env 2. Edit build.env and runtime.env files and fill out your secrets. 3. Ensure the secret files are ignored by git. 4. Deploy the sample. Calling a Job \u00b6 The Job generates random number and prints env vars configured in job.yaml and loaded from secret files. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-env-secret/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{}' # Expect: # { # \"model\": \"zoo\", # \"result\": 0.844725159639815, # \"passwd\": \"itworks!\" # }","title":"Deploying a Job"},{"location":"sample/python-env-secret/#deploying-a-job","text":"This Job makes use of secret vars, it should be filled out before attempting to deploy. 1. Make secret files from the templates: cp build.env.template build.env cp runtime.env.template runtime.env 2. Edit build.env and runtime.env files and fill out your secrets. 3. Ensure the secret files are ignored by git. 4. Deploy the sample.","title":"Deploying a Job"},{"location":"sample/python-env-secret/#calling-a-job","text":"The Job generates random number and prints env vars configured in job.yaml and loaded from secret files. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-env-secret/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{}' # Expect: # { # \"model\": \"zoo\", # \"result\": 0.844725159639815, # \"passwd\": \"itworks!\" # }","title":"Calling a Job"},{"location":"sample/python-metrics/","text":"Calling a Job \u00b6 The model generates random number. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-metrics/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{}' # Expect: # 0.5 Metrics \u00b6 Fetch /metrics endpoint to see Prometheus metrics values (generic metrics as well as the custom ones). curl \"http://127.0.0.1:7005/pub/job/python-metrics/latest/metrics\" # Expect: # perform_requests_total 17.0 # ... # # HELP job_wasted_seconds Seconds you have wasted here # job_wasted_seconds 1.2 # # HELP job_positives Number of positive results # job_positives{color=\"blue\"} 5.0","title":"Calling a Job"},{"location":"sample/python-metrics/#calling-a-job","text":"The model generates random number. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-metrics/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{}' # Expect: # 0.5","title":"Calling a Job"},{"location":"sample/python-metrics/#metrics","text":"Fetch /metrics endpoint to see Prometheus metrics values (generic metrics as well as the custom ones). curl \"http://127.0.0.1:7005/pub/job/python-metrics/latest/metrics\" # Expect: # perform_requests_total 17.0 # ... # # HELP job_wasted_seconds Seconds you have wasted here # job_wasted_seconds 1.2 # # HELP job_positives Number of positive results # job_positives{color=\"blue\"} 5.0","title":"Metrics"},{"location":"sample/python-recordkeeper/","text":"Description \u00b6 This is typical \"adder\" model, that will send RecordKeeper PEM every time it is called. This is just for example, it's not the best RK practice (PEMs should be sent only for relevant business events). Deploying \u00b6 Run racetrack deploy in this directory. Calling a Job \u00b6 The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42 Health \u00b6 The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"} API docs \u00b6 Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"Description"},{"location":"sample/python-recordkeeper/#description","text":"This is typical \"adder\" model, that will send RecordKeeper PEM every time it is called. This is just for example, it's not the best RK practice (PEMs should be sent only for relevant business events).","title":"Description"},{"location":"sample/python-recordkeeper/#deploying","text":"Run racetrack deploy in this directory.","title":"Deploying"},{"location":"sample/python-recordkeeper/#calling-a-job","text":"The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/python-recordkeeper/#health","text":"The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"}","title":"Health"},{"location":"sample/python-recordkeeper/#api-docs","text":"Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"sample/python-static-endpoints/","text":"Calling a Job \u00b6 The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-auxiliary-endpoints/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x\": 40, \"y\": 2}' # Expect: # 42 Calling static endpoints \u00b6 The model has a few static endpoints configured. GET request simply retrieves the file: curl \"http://127.0.0.1:7005/pub/job/python-static-endpoints/latest/api/v1/xrai\" # Expect xrai.yaml content curl \"http://127.0.0.1:7005/pub/job/python-static-endpoints/latest/api/v1/manifest\" -v # Expect job.yaml content with Content-Type \"application/x-yaml\" curl \"http://127.0.0.1:7005/pub/job/python-static-endpoints/latest/api/v1/docs/readme\" -v # Expect README.md content with Content-Type \"text/plain\"","title":"Calling a Job"},{"location":"sample/python-static-endpoints/#calling-a-job","text":"The model sums up given numbers. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7005/pub/job/python-auxiliary-endpoints/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x\": 40, \"y\": 2}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/python-static-endpoints/#calling-static-endpoints","text":"The model has a few static endpoints configured. GET request simply retrieves the file: curl \"http://127.0.0.1:7005/pub/job/python-static-endpoints/latest/api/v1/xrai\" # Expect xrai.yaml content curl \"http://127.0.0.1:7005/pub/job/python-static-endpoints/latest/api/v1/manifest\" -v # Expect job.yaml content with Content-Type \"application/x-yaml\" curl \"http://127.0.0.1:7005/pub/job/python-static-endpoints/latest/api/v1/docs/readme\" -v # Expect README.md content with Content-Type \"text/plain\"","title":"Calling static endpoints"},{"location":"sample/python-ui-flask/","text":"Deploying \u00b6 Run racetrack deploy in this directory. Calling a Job \u00b6 This job sums up given numbers. The following request performs its functionality by calling it through PUB: curl -X POST \"http://127.0.0.1:7005/pub/job/python-ui-flask/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42 Health \u00b6 The following request checks service's healthiness: curl \"http://127.0.0.1:7005/pub/job/python-ui-flask/latest/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"} API docs \u00b6 Every endpoint is served with PUB prefix path: /pub/job/python-ui-flask/latest . Check out root endpoint http://127.0.0.1:7005/pub/job/python-ui-flask/latest/ with Swagger UI page containing interactive list of all endpoints. Webview UI \u00b6 The job exposes custom UI pages available through PUB at: http://127.0.0.1:7005/pub/job/python-ui-flask/latest/api/v1/webview It also exposes a sample POST endpoint for some internal webview communication: curl -X POST \"http://127.0.0.1:7005/pub/job/python-ui-flask/latest/api/v1/webview/postme\" \\ -H \"Content-Type: application/json\" \\ -d '\"World\"' # Expect: # {\"hello\": \"World\"}","title":"Deploying"},{"location":"sample/python-ui-flask/#deploying","text":"Run racetrack deploy in this directory.","title":"Deploying"},{"location":"sample/python-ui-flask/#calling-a-job","text":"This job sums up given numbers. The following request performs its functionality by calling it through PUB: curl -X POST \"http://127.0.0.1:7005/pub/job/python-ui-flask/latest/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"numbers\": [40, 2]}' # Expect: # 42","title":"Calling a Job"},{"location":"sample/python-ui-flask/#health","text":"The following request checks service's healthiness: curl \"http://127.0.0.1:7005/pub/job/python-ui-flask/latest/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"}","title":"Health"},{"location":"sample/python-ui-flask/#api-docs","text":"Every endpoint is served with PUB prefix path: /pub/job/python-ui-flask/latest . Check out root endpoint http://127.0.0.1:7005/pub/job/python-ui-flask/latest/ with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"sample/python-ui-flask/#webview-ui","text":"The job exposes custom UI pages available through PUB at: http://127.0.0.1:7005/pub/job/python-ui-flask/latest/api/v1/webview It also exposes a sample POST endpoint for some internal webview communication: curl -X POST \"http://127.0.0.1:7005/pub/job/python-ui-flask/latest/api/v1/webview/postme\" \\ -H \"Content-Type: application/json\" \\ -d '\"World\"' # Expect: # {\"hello\": \"World\"}","title":"Webview UI"},{"location":"sample/scikit-model/","text":"Deploying \u00b6 Run racetrack deploy in this directory. Calling a Job \u00b6 The model uses linear regression to predict Y value for given data point. X input data has 2 numerical features. Model accepts data point (composed of list of 2 floats) and returns list of predicted values for them. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x_new\": [0.5, 2]}' # Expect: # [201.96842858259438] Health \u00b6 The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"} API docs \u00b6 Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"Deploying"},{"location":"sample/scikit-model/#deploying","text":"Run racetrack deploy in this directory.","title":"Deploying"},{"location":"sample/scikit-model/#calling-a-job","text":"The model uses linear regression to predict Y value for given data point. X input data has 2 numerical features. Model accepts data point (composed of list of 2 floats) and returns list of predicted values for them. The following request performs its functionality: curl -X POST \"http://127.0.0.1:7000/api/v1/perform\" \\ -H \"Content-Type: application/json\" \\ -d '{\"x_new\": [0.5, 2]}' # Expect: # [201.96842858259438]","title":"Calling a Job"},{"location":"sample/scikit-model/#health","text":"The following request checks service's healthiness: curl \"http://127.0.0.1:7000/health\" # Expect: # {\"service\": \"job\", \"status\": \"pass\"}","title":"Health"},{"location":"sample/scikit-model/#api-docs","text":"Check out root endpoint http://127.0.0.1:7000/ with Swagger UI page containing interactive list of all endpoints.","title":"API docs"},{"location":"tests/performance/","text":"Test response times of Racetrack's jobs \u00b6 Install pip install -r requirements.txt Copy .env.template to *.env and fillout missing values: cp .env.template test.env Run performance test: python response_time_test.py test.env","title":"Test response times of Racetrack's jobs"},{"location":"tests/performance/#test-response-times-of-racetracks-jobs","text":"Install pip install -r requirements.txt Copy .env.template to *.env and fillout missing values: cp .env.template test.env Run performance test: python response_time_test.py test.env","title":"Test response times of Racetrack's jobs"},{"location":"tests/stress/","text":"Stress tests with Locust \u00b6 Testing production Racetrack \u00b6 Copy .env.template to .env Fillout missing values - base_url , auth_token to desired cluster Deploy a Job Create ESC, make adder allowed Run locust (or locust --tags perform if you want to run single test case) Open http:0.0.0.0:8089 , set parameters, click Start swarming Watch results Alternatively, run locally: ENV_FILE = .env locust --tags perform","title":"Stress tests with Locust"},{"location":"tests/stress/#stress-tests-with-locust","text":"","title":"Stress tests with Locust"},{"location":"tests/stress/#testing-production-racetrack","text":"Copy .env.template to .env Fillout missing values - base_url , auth_token to desired cluster Deploy a Job Create ESC, make adder allowed Run locust (or locust --tags perform if you want to run single test case) Open http:0.0.0.0:8089 , set parameters, click Start swarming Watch results Alternatively, run locally: ENV_FILE = .env locust --tags perform","title":"Testing production Racetrack"},{"location":"utils/ci/","text":"Continuous Integration \u00b6 Usage \u00b6 Unit tests are run everytime commit is pushed to branch. But tests aren't run when commit message contains WIP: . Development \u00b6 Upon making changes to ci-test.Dockerfile, publish it: DOCKER_IMAGE_NAME=\"\" ./push.sh <VERSION> Where <VERSION> is ie. 2021-07-22. Then update .gitlab-ci.yml .","title":"Continuous Integration"},{"location":"utils/ci/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"utils/ci/#usage","text":"Unit tests are run everytime commit is pushed to branch. But tests aren't run when commit message contains WIP: .","title":"Usage"},{"location":"utils/ci/#development","text":"Upon making changes to ci-test.Dockerfile, publish it: DOCKER_IMAGE_NAME=\"\" ./push.sh <VERSION> Where <VERSION> is ie. 2021-07-22. Then update .gitlab-ci.yml .","title":"Development"},{"location":"utils/registry_cleaner/","text":"Registry Cleaner \u00b6 This is the Container Registry utility for cleaning obsolete images from the registry (collecting garbage). It checks what's deployed on Racetrack environments, then it compares it with the images in the registry. If there are images in the registry that are not needed anymore, it will list them as candidates for removal. Configure auth tokens for the registry, Racetrack tokens and update the list of active deployment environments. Copy settings.dist.py file to settings.py and update the values there. Activate project's virtual env: . venv/bin/activate Run the script: ./main.py list to list candidate images for removal. Review candidates and ensure the list is fine. Do the cleansing: ./main.py list --delete","title":"Registry Cleaner"},{"location":"utils/registry_cleaner/#registry-cleaner","text":"This is the Container Registry utility for cleaning obsolete images from the registry (collecting garbage). It checks what's deployed on Racetrack environments, then it compares it with the images in the registry. If there are images in the registry that are not needed anymore, it will list them as candidates for removal. Configure auth tokens for the registry, Racetrack tokens and update the list of active deployment environments. Copy settings.dist.py file to settings.py and update the values there. Activate project's virtual env: . venv/bin/activate Run the script: ./main.py list to list candidate images for removal. Review candidates and ensure the list is fine. Do the cleansing: ./main.py list --delete","title":"Registry Cleaner"}]}